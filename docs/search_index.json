[["index.html", "マーケティングリサーチ講義ノート Rを使ったリサーチ基礎固め Chapter 1 はじめに（本資料について）", " マーケティングリサーチ講義ノート Rを使ったリサーチ基礎固め 田頭拓己 神戸大学大学院経営学研究科 2024-06-18 Chapter 1 はじめに（本資料について） 本資料は神戸大学経営学部で開講されている「経営データ分析（マーケティング）」および、一橋大学商学部で開講されている「マーケティングリサーチ」の講義ノートである。本講義では、定量的な分析手法のマーケティングリサーチ文脈への応用と、R (R studio) を用いた分析の実行法を紹介する。これにより、読者が本書を通じてリサーチに従事するための基礎を固めることを期待する。なお、本資料は基礎的な統計的・計量経済学的分析手法をマーケティング領域に応用することに焦点を合わせているため、統計学、計量経済学の理論そのものや、より発展的な分析手法については他の講義や著書で学習することを求める。 本資料は、経営学・商学分野の大学学部上級および大学院修士レベルの学生を主な読者層として想定し作成している。特に、修士論文や卒業論文執筆のために、マーケティング分野にて初めて学術的な論文を書く学生に向けた内容を意識して構成されている。そのため、マーケティング領域で頻繁に用いられる手法の紹介だけではなく、学術的な研究を進めるための考え方についても言及している。また、本講義の履修者はこれまで講義で基礎的な統計学・計量経済学の講義を履修していることが想定されている。このような特徴を持つ本資料の目的は以下のようにまとめることができる。 マーケティング領域における研究の全体像を理解する。 基本的な分析手法についての結果を適切に解釈できるようになる。 基礎的な分析手法を実行する能力を習得する。 筆者は、これらの目的が研究者として定量的な実証分析を実行し論文を書くことに加え、実務的な観点からも重要になると期待している。上記の目標を達成することで、リサーチプロセスをブラックボックスとして捉え、「調査会社がこう言ってたからこれが正しい」という態度で調査・分析を依頼、理解することを避けられると考えている。これを読んでいる読者の中には、現在または将来管理者としてマーケティングリサーチプロジェクトに関わる人もいるだろう。その場合リサーチの全体像を理解し、各プロセスについても理解する必要がある。また、リサーチそのものは他社にアウトソースすることもあるだろう。その場合であっても、調査会社が提出した結果をきちんと理解する必要がある。したがって、統計的な分析手法についての理解が必要になるのである。加えて、もしあなたが小規模な組織に所属しているのであれば、分析作業も自分でできたほうが良いかもしれない。その場合実際にソフトウェア上でデータを扱って分析作業をこなす能力も必要になる。上記の目的は、これらの実務的重要性も有していると考える。 本資料の構成は以下の通りである。まず2章 では、Rおよび R studioの概要及び基本的な操作方法について説明する。本資料ではフリーソフトウェアである R (R studio環境)を用いるため、実際の分析作業に移る前に各自操作に慣れてほしい。続いて前半部（3章と4章）では、研究プロセスや論文の構成について学ぶ。特に3章は本資料の特色を表しており、分析方法やRの操作方法だけでなく「なんのために分析を実行するのか？」という問いに着目する。前半部では、リサーチ過程の全体像と論文の構成を始点とし、研究課題の設定や理論・仮説の役割に加えて、アンケート調査票設計について学ぶ。一方後半部分（5章以降）ではR studioを用いて、データ処理（5章）や統計的仮説検定（7章）、回帰分析（8章）について学ぶ。その後、マーケティングで広く用いられている手法として、クラスター分析（10章）、因子分析（11章）、価格感度測定（12章）を学ぶ。 "],["rusage.html", "Chapter 2 R と R studioに慣れる", " Chapter 2 R と R studioに慣れる R（あーる）は統計、データ解析、統計グラフ作成のためのオープンソースソフトウェアである。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし、インストールが可能である。ソフトウェアには、Windows用、Mac用、Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。Rを用いる際には、多くの場合) R studio、Jupyter notebookや、Rコマンダーのようなユーザーインターフェイスが利用される。そしてRを使用する際には (Rコマンダーを使わないかぎりは) 基本的にソースコードを入力し計算、分析を行う。しかし、本講義においては後述する Cloud環境を利用するため、個人の意思でデスクトップ版をインストールする場合を除き、Rのインストールについては気にしなくて良い。 Rを用いる際に最もよく使われる環境（アプリケーション）のひとつがR studioである。そのため、本講義においても基本的にはR studioを用いることを前提とするが、R studioをデスクトップにインストールし利用する場合には、Rそのものもインストールしておく必要があることに注意が必要である。R studioは現在、Positとも呼ばれており、以下のサイトからアクセスが可能である（https://posit.co/）。 本講義では、Posit Cloudという、アカウント登録を行うことでブラウザ上でR studioを利用できる環境を勧める。R studio Desktop版 の利用においては、ディレクトリ設定などによってエラーが生じることが多々あり、個別のPC環境に合わせて対応、設定を行う必要がある。そのため、まとまった人数に対応する必要がある本講義においてはクラウド版を利用する。本講義を通じてRおよびR studioの使い方に慣れ、自身の研究や仕事等でデータ処理や分析を行う場合にはR studio デスクトップ版（通常はR studio IDEのフリーバージョンで十分）をインストールし、利用してほしい。もちろんはじめからデスクトップ版を利用してもらっても構わないが、その際には環境設定について色々と注意してほしい。本資料内では、第 2.2 節にて、デスクトップ版の利用についての説明をしているため、関心のある読者は参照してみて欲しい。 "],["posit-cloudを始める.html", "2.1 Posit cloudを始める", " 2.1 Posit cloudを始める R studioは、Rを利用するためのアプリケーションである。R単体で使うよりも便利な機能が搭載されており、R studioを使うことでプログラミング作業を容易にすることが可能になる。最も大きな特徴としては、Rでの操作、分析を実行するための「コンソール画面」と、実行したい操作、分析のコードを記述しておく「Rスクリプト」と呼ばれるテキストファイルを一つの画面内に同時に表示できることである。そのため、Rに実行してほしいコマンドをテキストデータのように記述、修正し書き溜めておける一方で、その実行もスムーズに行え、結果も同画面内で確認することができる。 R studioをより手軽に利用できるサービスがPosit Cloudである。Posit Cloudはブラウザを通じてR studio環境を利用できるサービスであり、アカウント登録をするだけでよく、コンピュータへのRおよびR studioのダウンロードとインストールが不要である。 Posit Cloudの利用方法はとても簡単である。大まかな利用までの流れは以下のとおりである。 以下のリンク（https://posit.co/）からサイトへアクセスし、ProductsタブからPosit Cloudを選択する。 Posit Cloud 画面 その後、進んだ画面で “Get Started” \\(\\rightarrow\\) （特別な理由がなければ）Free planを選択し “Sign up” \\(\\rightarrow\\) 好きな方法でアカウントを作成する。 Sign up 画面 登録が完了すると、自身のアカウントのホーム画面へ移動する。新しいR studio セッションを開始するためには、画面右上の New projectボタンを押し、“New Rstudio Project” を選択する。 New project画面 New projectのセットアップが完了すると、Studio環境画面が表示される。 新しいRstudio 画面 Rstudioは、上記の図のような画面構成をしている。Rstudioの画面を構成する主なウィンドウはペインと呼ばれ、(1) RスクリプトでRコードの入力・編集に用いる”Source”、(2) Rの命令を直接入力し結果も表示される”Console”がなどが主な要素としてある。また、その他利用しているデータ情報、パッケージ、履歴など様々なタブが存在する。Rstudioの初回起動時にはSourceのペインは収納されているため、 Rスクリプトファイルを作成する必要がある。Rstudioは基本的に4分割画面で表示され、各ペインの配置については、Tools \\(\\rightarrow\\) Global option \\(\\rightarrow\\) Pane Layoutより変更が可能になる。Rstudioを操作する上で、基本的に重要となる情報は、(1) Source、(2) Console、(3) データやプロットに関する環境情報の3点であるので、以下のような配置がおすすめである。 左上 or 下: Source 左下 or 上: History (ただし、さほど重要ではないので畳んだ状態にしておく) 右上 or 下: Console 右下 or 上: 複数タブをまとめ 配置の目的はあくまで、必要な情報を同一画面上に表示することであるため、自身のやりやすい配置を考えてアレンジしてほしい。なお、本講義ノート内に掲載している R studio 操作画面のキャプション画像では、Posit cloudではなくデスクトップ版の画面を用いている場合もあるが、ご容赦いただきたい。 "],["desktop.html", "2.2 （補足）R studio デスクトップ版の利用", " 2.2 （補足）R studio デスクトップ版の利用 Posit cloud のフリーアカウントには、利用可能な時間やデータ容量に制限が存在する。自身の利用スタイルを鑑みて、Posit cloudのフリープランでは不十分である場合には、有料版へのアップグレードやDesktop版のインストールによって対応する必要がある。ここでは、 R studio Desktop の利用について紹介する。 R studioをオフライン環境で使う場合には、R と R studioの両方をインストールする必要がある。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし,インストールが可能である。ソフトウェアには、Windows用，Mac用，Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。 R studio のインストールは、以下のリンクから “Download” ボタンをクリックすることで始まる（https://posit.co/downloads/）。なお、特別な事情がない限り、無料版で十分分析が可能である。 無料版のダウンロードが完了したら、指示に従いインストールを実施する。その際の設定はすべてデフォルトで構わない。 Rstudio Desktop インストール ただし、WindowsでのRおよびR studioのインストールには注意が必要である。特に、Rを用いる講義を受け持っていると、新たなパッケージのインストールができないなどのトラブルが頻発する。これらの問題点に調べると、(1) 文字コードによる文字化けの問題、(2) ユーザーアカウントのホームディレクトリ名に日本語（全角）が利用されていること、(3) Rのライブラリが(勝手に) One drive 上に作成されることが原因であることが多かった。これに対して、 R の version 4.20以降からは、UTF-8の文字コードに対応したり、デフォルトでのRのインストール場所の変更（One drive上でない）が行われたりと、問題の改善が図られている。自身のホームディレクトリの名前が全角文字であるときは、ホームディレクトリ以外のローカルディレクトリを設定したほうが良い。この点に関する対応には、三重大学の奥村先生によって以下のウェブサイトに説明が記載されている（https://okumuralab.org/~okumura/stat/R-win.html）。 "],["補足デスクトップ版の利用とプロジェクト機能.html", "2.3 （補足）デスクトップ版の利用とプロジェクト機能", " 2.3 （補足）デスクトップ版の利用とプロジェクト機能 R および R studio のインストールが完了したら、アプリケーションを起動する。R studio の利用方法については基本的に Posit cloudの説明と同様である。ただし、デスクトップ版で R studio を利用する際には、「プロジェクト」機能を使うことを勧める。プロジェクトは、互いに関連し合ったファイルの集まりを指す。Rを通じた分析では、たくさんのファイルを扱うことになる。例えば、複数のRスクリプトやデータセット、加工したデータセットの保存、分析結果、出力された図表などがある。これらのファイルを手作業で一括管理することは困難である。むしろそのような管理作業に認知的な負担を費やしたくないというのが分析者の本音である。プロジェクト機能を使うことにより、作業ディレクトリとファイルの保存先をひとまとまりに指定できるため、ファイル管理の手間がなくなる。 新しいプロジェクトを作成するシンプルな方法が、Fileから作成する方法である。具体的には、File -&gt; New Project -&gt; New Directory -&gt; Create New Project -&gt;Directory nameの指定 -&gt; プロジェクトの設置場所（ディレクト）の指定、という手順で作成する。 プロジェクト作成手順1 プロジェクト作成手順2 R をデスクトップ上で利用する際には、基本的には自身のPC内にある（もしくはディレクトリにアクセス可能である）データの所在地（ディレクトリ）を特定することでデータの操作や分析を行う。これに対してプロジェクト機能を利用することでそのプロジェクトを実行している際に参照するワーキングディレクトリを固定することが可能になる。この機能によってR studioを通じたデータ処理や分析作業が容易になり、不要なトラブルを避けることが可能になるため、デスクトップでR studioを使う場合には可能な限りプロジェクト機能を利用してほしい。 "],["rの基本操作.html", "2.4 Rの基本操作", " 2.4 Rの基本操作 ここでは、Rを使用する上での基本的な操作方法を紹介する。Rはコマンド（命令）をconsoleを通じて実行することで動かすことができる。例えば四則演算であれば、以下のように命令し、計算が実行できる。 1 + 2 ## [1] 3 5 - 10 ## [1] -5 3 * 8 ## [1] 24 1/2 ## [1] 0.5 基本的に一つのコマンドは1行に書き、数字、演算記号、スペースは半角で入力する。以下は、べき乗、平方根、自然対数を計算するためのコマンドで計算できる。 2^3 ## [1] 8 sqrt(2) ## [1] 1.414214 log(2) ## [1] 0.6931472 Rは、ベクトルや行列の計算も可能である。c() という関数を用いると、ベクトルを作成できる。例えば、c(1, 3, 5) というコマンドによって(1, 3, 5)というベクトルが作成できる。作成したベクトルを使って以下のような計算も可能である。 c(1, 3, 5) + 1 ## [1] 2 4 6 ベクトルは、連続した数字の列を生成するための演算子である : を用いても作成することができる。例えば、1から100の整数を要素とするベクトルは以下のように作成することが可能である。 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 また、ベクトルの要素は文字列でも構わない。 cities &lt;- c(&quot;Tokyo&quot;,&quot;Osaka&quot;,&quot;Kobe&quot;) cities ## [1] &quot;Tokyo&quot; &quot;Osaka&quot; &quot;Kobe&quot; 上記の計算方法に加え、Rが持つ重要な特徴に、オブジェクトの定義がある。Rでは、任意の行列、ベクトル、数値などに名前をつけ定義したうえで、それを用いた計算を行うことができる。なお、Console上で以下のように定義（実行）したオブジェクトはenvironmentタブ内に表示されるため、各自確認をしてほしい。なお、定義したオブジェクトの確認・出力も簡単に行えるが、大文字と小文字は区別されるため、注意が必要である。 a &lt;- 1 b &lt;- 2 a ## [1] 1 A ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found また、定義したオブジェクトを用いた計算も実行できるため、各自以下の計算を実行し、結果を確認してほしい。 a + b a / b a ^ b なお、先程のベクトル操作と組み合わせ、ベクトル名 [i] とすることで、ベクトルの i 番目の要素にアクセスすることができる。例えば、以下のaとbというベクトルから特定の要素を取り出すことを考える。なおこの場合、同時に複数の要素を取り出すこともできる。 a &lt;- seq(10, 100, length = 10) b &lt;- 10:1 aの2番目の要素 a[2] ## [1] 20 bの2番目の要素 b[2] ## [1] 9 aの3-5番目の要素 a[3:5] ## [1] 30 40 50 aの1,3,5番目の要素 a[c(1,3,5)] ## [1] 10 30 50 分析で繰り返し必要になる機能がRで使えないときは、function()関数を使って、新たな関数を作成できる。例えば、最大値と最小値を並べて表示したい場合を考える。そのために、ここでは ‘mm’ という新たな(オブジェクトxの最小値と最大値で構成されるベクトルを返す)関数を作ってみる。 mm &lt;- function(x){ c(min(x), max(x)) } そして上記の関数を利用して、以下のオブジェクト a, b の最小値と最大値を出力する。 a &lt;- c(1, 5, 100, 2, -8, 7) b &lt;- c(1, 6, 8, 0, 120) mm(a) ## [1] -8 100 mm(b) ## [1] 0 120 しかし、すべての関数を自作するのは難しい。Rでは様々な計算を実行するための関数が用意されており、多くのマーケティング研究においては既存の関数を用いることで対応が可能である。実は上記のmm関数の中で使っている “min”や”max”も、それぞれ最小値と最大値を返す関数である。他にも例えば、meanや median があり、これらはそれぞれ平均値と中央値を計算するための関数である。 関数の利用においては例えば、’f()’のように関数名’f’のあとにカッコをつけて表記する。()の中には、引数（arguments）を用い、計算に必要な情報を指定することが必要となる。例えば、seq()という関数を用いて、2以上20以下の偶数の数列(sequence)を作ることが可能である。以下では、二通りの表記を提示するが、どちらも同じ結果を返す。 seq(from = 2, to = 20, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 特定の関数に対する引数を確認したい場合は ‘?関数名’とconsoleに命令することで確認が可能になる。例えば、seq関数について知りたければ、’?seq’ で確認できる。 Rに元から含まれている関数以外にも他者が開発してくれた関数も存在する。そしていくつかの関数をまとめたpackagesが多数存在する。これまで世界中の開発者たちが作成したパッケージを公開してくれている。パッケージは何らかの目的や課題を達成することを目的に構成されたコードライブラリであり、それらをインストールし、各セッションごとに起動することで利用できる。 CRANで公開されているパッケージは、install.packages()でインストール可能である。また、Rstudio の場合、ペインからpackagesタブ\\(\\rightarrow\\)Install\\(\\rightarrow\\)パッケージ名の入力という手順でもインストールが可能である。そしてインストール済パッケージは、library() によって起動することで、活用可能にある。ここで注意しておきたいのは、library()によるパッケージの起動はセッションごとに実行しないといけないという点である。まずは以下の通り、本講義で用いる “tidyverse” パッケージをインストールし起動してみる。“tidyverse”は、複数のパッケージから構成されているパッケージであり、データの整形・分析を行うために役立つ複数のパッケージをまとめてインストール・起動できる。 install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) "],["r-スクリプトのすゝめ.html", "2.5 R スクリプトのすゝめ", " 2.5 R スクリプトのすゝめ Rstudio環境で作業を行う際には、Consoleに直接コマンドを入力するのではなく、‘.R’ という拡張子のファイルを使って、「Rスクリプト」を作成することを勧める。RスクリプトはRでの分析に対応しRコマンドの集まりとして記されるファイルであり、SourceエディタからRスクリプトに記載されたコマンドを、Consoleを通じて実行する。 Rスクリプトを用いることの重要性は、コマンドの修正可能性と、分析の再現性という二点から理解できる。まず修正可能性として、そもそもコマンドを書くうえでは大小様々な誤りが付き物である。このような間違いに対応し適宜修正を加えていくためには、実行するコマンドを一つ一つ console に直接記載するのではなく、Rスクリプトとして分析過程を記録し、その内容に基づき記載、修正を加えることが好ましい。 第二に再現性においては、Rスクリプトとしてデータ整形・分析のプロセスを文書ファイルとして保存しておくことで研究者自身もしくは第三者が分析を再現することが可能になる。これは研究プロセスの客観性を高め、分析結果の信頼性を高めるために非常に重要な要素である。 これらに加え、Rスクリプトの利用は研究者個人の研究遂行上の利点もある。自身が行った研究であっても分析の細部に関しては時間経過とともに忘れてしまうものである。その際に、Rスクリプトによる分析プロセスの追跡可能性が役に立つ。また、同様の分析を再度別データで実施する場合も、既存のRスクリプトを応用することで効率的に分析が可能になる。これらに加え、共同研究において他の研究者と分析プロセスを共有する場合にもRスクリプトが役に立つ。 2.5.1 Rスクリプトを書く 先述のRスクリプトの利点を活かすために、Rスクリプトの作成においては、いつ作成されたなんのためのファイルなのか、そしてファイル内に記載されているコマンドがどのような意図によるものなのかがわかるように書くべきである。そのための重要になるのがコメント機能である。一つの行の中で#記号よりも後ろの部分はコメントとして処理（コメントアウト）される。この機能を使い、コメントで自然言語による説明を加えることで、コマンドの説明や意図等、自分や他人がスクリプトを見返して内容を理解できるようにする。例えば、Xという変数の平均値を求める場合、以下のようにRスクリプトを書くようにする。 #Xの平均値を求める。 mean(X) Rスクリプトは、Rstudioの左上にある+ボタンから新規作成可能である。ここでは試しに、新規Rスクリプトを作成し、“mktg01.R” という名前で保存してほしい。保存したRスクリプトはファイルから開くことができる。“mktg01.R”ファイルを作成したら、試しに以下のコマンドを書き込み、実行してほしい。Rスクリプトからコマンドを実行する際には、コマンド記入後、Rスクリプト上で実行したい行にカーソルを合わせた状態でcommand (control) + Return (Enter)を入力する。もう一度同じキーを押すと、2行目のコマンドが実行される。これらを実行することで、Rコンソール上に、下記と同じ結果が出ていることを確認してほしい。なお、コマンドを記入の際には、こまめに command (control) + s により保存することを心がけるようにしてほしい。 a &lt;- 9 sqrt(a) ## [1] 3 また、Rスクリプトを作成する際には、ファイルの冒頭に以下の説明を書き込む習慣をつけると後々見返すときに便利である。 ファイル名 目的 作成者 作成日 最新更新日 例えば、上記の内容とコマンドを含めた “mktg01.R” ファイルは、以下のようになる。 Rスクリプト例 "],["本章のまとめ.html", "2.6 本章のまとめ", " 2.6 本章のまとめ Rはデータの管理、分析、図表の作成を行うことができる統計分析プログラミング言語である。 Rを動かすには、コマンドと呼ばれる命令をコンソールを通じて実行する。 コマンドは基本的にRスクリプトに書き込んでからcommand (control) + Return (Enter)で実行する。 Rスクリプトにはコマンドだけでなくコメントを使った説明も追加する。 分析には既存の関数やパッケージを使うことが多い。 "],["参考文献.html", "2.7 参考文献", " 2.7 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. ランダージャレド（2015）「みんなのR 第2版」，高柳新市・牧山幸史・蓑田高志訳，マイナビ. "],["design.html", "Chapter 3 リサーチデザイン", " Chapter 3 リサーチデザイン 本講義は定量的マーケティングリサーチ過程の全体像を理解し、それを監督・実行できるようになることを目的とする。そのためには、基本的な分析方法およびその結果を適切に解釈できるようになることに加え、実証的なリサーチプロジェクトを管理・実行する前提知識が必要になる。ただ思いつきのままにデータを取り、それを羅列しても適切な定量的研究にはならない。特に、マーケティング・リサーチは、企業活動における顧客や市場のリサーチと、学術研究としてのリサーチが存在する。企業活動におけるリサーチにおいても、学術研究で培われるスキルや考え方は応用できる。そこで本章では、（1）マーケティング実務とリサーチの関係、（2）研究の全体像、（3）研究課題や仮説の導出に加え、（4）学術的研究に求められる科学哲学や理論に関する基礎的な考え方を提示する。本章の内容は研究者志望の大学院生にとって重要であることはもちろんだが、企業においてマーケティングリサーチに従事する人たちにとっても重要であると期待する。特に、本章では「マネジメントとリサーチの異なる視点」、「実務的課題と研究課題」といった実務的意思決定とリサーチ業務の対比によるマーケティングにおけるリサーチの考え方についての議論も提供している。筆者は、これらの内容は企業内でリサーチ業務を実施する際にも有用だと考える。 "],["マーケティング実務とリサーチ.html", "3.1 マーケティング実務とリサーチ", " 3.1 マーケティング実務とリサーチ 3.1.1 実務とリサーチの異なる視点 経営学部や商学部などに在籍している学生であれば、「マーケティング」に関する様々な講義や教科書に触れているだろう。しかしながら、その際にそれらの講義や教科書がどのような視点でマーケティングを捉え、議論しているのかに注意することが重要になる。マーケティングを議論する際には、（1）マーケティング意思決定者の視点と、（2）マーケティングリサーチャーの視点が存在する。マーケティング意思決定者の視点からは、マーケティングに関する意思決定を行い、実行することを目的に議論が行われる。「マーケティング・マネジメント」はこの視点の典型的な領域である。ここでは、意思決定のサポートするための既存のフレームワークを紹介することが多い。例えば、マーケティングで紹介される4Ps（Product, Price, Promotion, Place）や、STP（Segmentation, Targeting, Positioning）などは、意思決定や実行を手助けするための指針となることを目的としている。そのために、提示される議論や枠組みの精緻さよりも実務的有用性が優先されることも多い。 第二に、マーケティングリサーチャーの視点では、マーケティングや消費者、顧客に関連する信頼度の高い情報や知識を得ることを目的に議論が行われる。「マーケティングリサーチ」や「マーケティングサイエンス」はこの視点の典型的な領域である。マーケティングに関する研究では、既存の知識を疑ったり、不足している知識を発見し補う事を求める。そのうえで、学術的研究の場合には発見物による一般化可能性を、企業での研究では自社顧客への深い理解を求めることが多い。そのため、この視点では議論されている内容の精緻さや文書内の論理的一貫性が優先される。 企業内におけるマーケティング実務においては当然マーケティング意思決定者の視点が優先されるわけだが、マーケティングリサーチもこれと無関係ではない。企業内におけるマーケティングリサーチは、実務的な意思決定を助けるために実施される（Malhotra, 2019）。マーケティング意思決定者は通常、マーケティングに関する実務的課題（3.3節参照）を抱えておりそれを解決したいと考えている。しかしながら、意思決定者は闇雲にマーケティング戦略や戦術を決定すれば良いわけではなく、市場や消費者、既存顧客などに関する情報をもとにより効果的な方策を模索している。企業としてのマーケティングリサーチでは、（1）実務的課題から必要な情報（研究課題）の特定化、（2）情報（調査結果）の提供、を通じてマーケティング意思決定者をサポートすることが期待される。 マーケティング実務においては、「どうするべきか」という行動に即した問いや規範的な議論が注視されるが、「どうするべきか」という規範的な問いに直接的に答えるようにリサーチを設計することは避けたほうが良い。そのためにも、「リサーチの結果に基づく含意として実務的な指針が導かれる」ということを意識すると実務的にも関連性の高いリサーチを設計しやすい。 例えば、「どのような広告内容を採用するのが良いか」という問いに対して直接的に答えるリサーチを計画するのは好ましくない。この問いが問題である理由はいくつか存在する。第一に、これは分析手法においても説明するが、リサーチにより正確に未来を予測することは不可能である。そのため、「こうすべき」という規範的な回答は基本的に避けたほうが良い。規範的な問いに対しては、研究の結果得た知識をもとに議論を行うことで得る「実務的含意」に基づき迫ることが好ましい。なお、企業内での研究ではもちろんだが、学術的な研究においても結果から得る実務的含意はとても重要である。マーケティングに関する研究は社会科学の応用領域である。そのため、我々が行っている研究およびその結果がどのように社会や実務につながるのかについて、研究者は意識的になる必要がある。 第二に、問題設定が不明確である点についても問題を指摘することができる。何を良しとするかの基準が明確ではなく、消費者が商品に対して覚えてくれる（想起集合に入る）ことや、好意を抱く（態度形成）、買いたいと思う（購買意図の形成）に加え、実際に買う（選択）など、何を目的とするのかが明確でなく、何をもって「良い広告」とするのかが漠然としている。同様に、広告設計において企業が調整できる内容も様々ある。広告内で用いるメッセージや、有名人利用の有無から、広告内で用いる文字のフォントやサイズまで、色々なものを選択しなければならない。このように、「何と何の関係を捉えようとしているのか」が曖昧であることも上記の問いの問題点である。学生の中には、「網羅的かつ包括的にこの問題を捉えたい」という志のもと、上記のような曖昧な問いを採用するケースも散見される。しかしながら基本的には、曖昧な問いからなにか明確な回答を得ることは困難である。つまり、包括性の名のもとにこのような問いを設計してしまうと、結局は実務的にも何も言えない結果しか得ることができないことに注意が必要である。 本節のこれまでの内容をまとめると、「実務的課題と研究課題は別物だが、研究によって得る実務的含意は重要である。」と言える。企業内でのマーケティングリサーチは意思決定者をサポートするために実行される（Malhotra, 2019）。そのため、リサーチ結果が意思決定者に指示を与えるわけではないということに注意が必要である。以下の図（a）のように、実務家が直面している課題に対していきなり解決策を求めようとすると、個人や組織の勘や経験、思いつきに頼ることになる。このような意思決定を避けようとするのが、基本的なマーケティング・マネジメントの考え方である。そこで、直面している問題の背後に存在する理由について推察し、どのような情報を得る（問いに答える）ことができればその理由が正しいか否かを理解できるのかを考えることがまず必要になる。つまり、問題の背後にある何らかのメカニズムについて仮説を構築し、その仮説自体を検証するための問いを考えることが重要になる。その後、どうやったらその問いに答えるための方法を考察・実行し、得た結果を知見として整理する。ここでもう一つ重要になるのが、研究結果として得る情報自体は必ずしも直接的な解決策にはならないという点である。研究上の問いは、あくまで問題の背後にあるなんらかのメカニズムを捉えていた。そのため、研究の結果得られるのは、メカニズム対する何らかの知識である。研究者は得た結果に基づき、「このようなメカニズムがなりたっているという前提で考えるならば、実務としてはこのような方法が好ましいのではないか」という提案を考察することが好ましい。下記図（b）は、実務的含意と問題設定の関係について示したものである。 実務課題と実務的含意 上記の問題設定について、ソーシャルメディアにおける炎上（以下、炎上）を例に取り考えてみる。例えば、「炎上に直面した企業はどのように対応すれば良いか？」問いは曖昧な実務的問いであるといえる。上述の原則から考えると、この問いに直接的に答えるような研究を試みるのは避けるべきである。炎上と言っても様々な論点が存在するが、ここではマーケティングらしく、炎上における消費者の行動についてもう少し考えてみる。 企業の立場から炎上を捉えると、自社への批判が拡大することで自社の価値や成果を損ねることは避けたいのは当然だが、炎上には企業にとって正の影響がある可能性を示した研究も存在する。これは、炎上した企業に対する批判が集まる中でその企業を擁護する人々も存在し、批判者と擁護者による議論の拡大が広告機会につながるというものである（Scholz and Smith, 2019）。しかしながら、消費者によるソーシャルメディア上での発言と、（購買などの）オフラインかつプライベートな行動との間には一貫性があるのだろうか？政治的な文脈では、2016年のアメリカ大統領選挙におけるトランプ前大統領に対する隠れ支持者の存在が話題になった。ソーシャルメディア上ではトランプ前大統領を批判するが、実際には彼に投票した有権者が多く存在したとされた（Enns et al., 2017）。このようなオンラインとオフラインでの一貫しない行動を炎上の文脈に置き換えれば、「ソーシャルメディア上では文句を言うのに、その企業の製品・サービスを買う個人」 も存在するのではないかと考えられる。 そこで、（a）炎上の文脈における隠れ支持者（ソーシャルメディア上で企業に対して批判を行うが、実際にはその企業の製品を買う人）はいるのだろうか？そして、（b）もしいるのであればその行動はどのようなメカニズムで説明できるのだろうか？という問いを考える。これら２つの問いは、（a）同じ情報（炎上）に対して、異なる文脈（ソーシャルメディアと購買）において異質な反応が見られるのか、（b）その異質な行動の先行要因は何か、という問いは探索的であるが、実際の研究では先行研究のレビューを通じて重要になる特定し、より具体的かつ検証可能な問いを立てることになる。ちなみに、異なる文脈における実際の消費者行動を追跡するのは非常に難しく、現実的な調査手段として消費者へのアンケート調査が有効になるかもしれない。 Tagashira et al. (2023) による消費者へのアンケート調査を利用した未刊行論文内では、功利主義（utilitarianism）的倫理観の高い個人においては、炎上を起こした企業を批判するツイートを拡散したり、自ら投稿する意図が高い一方で、その企業の製品を購入する意図も高いことが示された。そのため、功利主義的（物事の善悪を出来事の結果生じる自身や社会にとっての利益や福祉に基づき判断する）傾向の高い人は、「炎上に対して文句を言うが買う」という傾向が強いことがわかった。一方で、形式主義（formalism）傾向の強い個人は、ソーシャルメディア上での拡散や投稿意図は低いものの、当該企業への購買意図も低いことがわかった。つまり形式主義的（物事の善悪を、出来事の結果に関わらず行動そのものが原理原則から考えて良いか悪いかに基づき判断する）人々は、ソーシャルメディア上では活発に発言しないが、その企業の製品・サービスを買う可能性が下がると言える。 これらの結果は、「どのような消費者がどんな行動を取りやすいか」という情報を記述的に示しているのみであり、結果自体が実務的な提案そのものではないことを理解してほしい。しかしながら、この情報から実務的に有効そうな施策を推測することは可能である。例えば、企業がソーシャルメディア上の批判を沈静化させたい場合、本結果に基づけば、功利主義的な人が騒いでいる可能性が高いために、消費者ないしは社会的な便益を改善するようなメッセージの提示が有効になるかもしれない。一方で、批判せずに購買から遠ざかってしまう消費者を食い止めたいと企業が考えるのであれば、倫理的な原則に基づく謝罪や説明、対応を示すほうが良いかもしれない。このような、研究結果から推察・議論できる何らかの提案を「実務的含意」と呼ぶ。 ただし、実務的含意として述べられている方策の有効性自体は検証していないことには注意が必要である。例えば、「企業は消費者や社会への便益を向上させるような対応やメッセージを発信すべきか？」という実務的課題に着目するならば、「消費者や社会への便益を向上させるような対策やメッセージが功利主義的な傾向の強い個人による批判的な投稿や拡散を阻害するのか？」という研究課題を扱うことが考えられる。研究は基本的に、ある問いを立ててそれに答えるという構造を有しており、ある研究を達成したことによって生じた新たな問いはまた別の研究によって回答することが重要になる。なお、学術的な論文であってもマーケティング領域においてはこの実務的含意を議論することが求められるため、研究者志望の大学院生もこの点を十分に理解することが求められる。 3.1.2 実務とリサーチの分離 マーケティング研究を通じた実務的意思決定への知見の提供では、同一企業内で調査・分析を実施する場合もあれば、調査会社等にそれらをアウトソースする場合もある。いずれの場合においても、企業活動のためにマーケティング研究を実施する際には、「マネジメントとリサーチの分離」が重要になる。マーケティング実務に関する調査分析を行う場合、研究を発注する主体でありマネジメントを主な仕事とする「クライアント」と、それを受け研究に従事する「リサーチャー」が存在する。リサーチャーは、クライアントが直面している実務的問題に基づき研究課題を設計し、調査や分析を実行する。そしてその研究成果をレポートにまとめ、クライアントに報告する必要がある。 リサーチャーは、レポート執筆において「ありのままを書く」必要がある。具体的には研究方法、結果、結論について正確に記述し情報を提供しなければならない。データや結果の改ざんは重大な研究不正である。企業におけるリサーチであっても読み手であるクライアントが期待する、または望んでいる結果にレポートの内容を書き換えてはいけない。しかしこの問題は、個人の倫理観だけではなく、リサーチャーとクライアントを取り巻く環境にも関係して論じられるべき問題である。例えば、クライアントにとっては特定のマーケティング戦略（例、モバイル広告）を実行し成果をあげることに自身の出世や昇給がかかっており、モバイル広告がその企業にとって有益であるというエビデンスを「欲しがっている」かもしれない。その上で、リサーチャーによる調査と分析の結果、モバイル広告による成果の向上が確認されなかった場合を仮定する。このとき、クライアントとリサーチャーの間に利害関係がなければ、「ありのまま」を伝えるのは難しくないかもしれない。しかしながら、もしこのクライアントがリサーチャーの直属の上司であるならばどうだろうか。ひどい場合であれば、クライアントの不利になる結果を提示したとしてリサーチャーが悪い評価を受けるかもしれないし、客観的な評価に影響がなくても、個人的な反感を買うかもしれない。もしくは、リサーチャー自身がこのような可能性を危惧し、「ありのまま」を伝えることに躊躇してしまうこともあるだろう。大前提として、研究結果に基づき（クライアントとリサーチャーどちらも）従業員の評価を行うことは避けるべき問題であり、研究結果は研究結果として受け入れることが重要である。しかしながら、人間である以上このような正論では割り切れない部分も出てくるかもしれない。そのため、クライアントとは利害関係のないリサーチャーに研究を発注することが何より重要であり、クライアント側の組織においても研究結果に基づき人事的評価を行わないという前提やルール作りが必要になる。これらが満たされていないと、クライアントにとって都合の良い結果を求めることで適切な研究が設計されなかったり、不利な研究結果受け入れないといった行動を取る誘因がクライアント側に存在することになる。 上記の例は、企業内における利害関係を捉えたものであるが、学術的研究においても注意が必要である。自身の経験や知識に基づいて、「このマーケティング戦略は効果があるはずだ」や「この戦略の有用性を示したい」と考えている場合、意識的もしくは無意識的に自身の都合の良い方法や結果を選別してしまうかもしれない。また、研究者が企業との共同研究に従事している場合には、先述のクライアントとリサーチャーの関係と同様の問題に直面するかもしれない。そのため、あなたが企業、学術どちらの研究に従事するとしても、マネジメントとリサーチとを分離し、研究結果をありのままに記述、報告できる状況を整えることが何より重要である。 "],["リサーチデザインと論文の構成.html", "3.2 リサーチデザインと論文の構成", " 3.2 リサーチデザインと論文の構成 良い研究を遂行するためには、自身の立てた研究上の問い、議論される仮説、調査・分析手法や結論といった、研究を構成する要素間の一貫性を保った形でリサーチデザインを決定する必要がある。そのためには、研究に取り組む前に、研究の全体像を把握しておく必要がある。そこで、本章では、研究の全体像を把握・理解するために学術論文の構成を整理しつつ、それぞれの構成要素の役割について把握する。なお、ここで紹介される論文の構成は修士・卒業論文の執筆においても応用できる。また、本節の後半では実務的なマーケティングリサーチレポートにおける注意点も追記する。 マーケティングに関する学術論文は（様々な形態があるが、一般的には）以下の構成として整理できる。なお、以下の内容はマーケティングに関する定量的な実証研究についての形式であるため、アプローチが異なれば構成は変わると考えられる。例えば、定性的なアプローチを用いたテキストとしてはベルクほか（2016）による著書が挙げられる。また、項目の順番も論文の特性によって前後することもある。 イントロダクション 現実的（substantive）トピックの文献レビュー 理論的（theoretical）基盤の文献レビュー 仮説の提示 調査分析手法 分析結果 まとめと議論 イントロダクションの役割は論文としての「問い」と、その問いにどのように回答するかを簡潔かつ明示的に説明することである。そのうえで、その問いについて答えることがなぜ重要なのか、どのような価値があるのかについて、論文の読み手に理解させる必要がある。そのためには、（1） 論文として着目する問題（および問題の背景）の明示、（2） 研究課題の提示、（3） どのような研究群にどんな貢献を与えるのかについての説明、（4） 論文の概要（実際にどんな結果を得たかなど）の説明、が必要になる。 第二に、現実的（substantive）なトピックの文献レビューについてだが、ここでは着目する現象や実務的課題に対応した先行研究のまとめを記述する。マーケティング研究における文献レビューは主に、（1） substantive なトピックベースで先行研究の潮流をまとめ、課題を見出すレビューと、（2） theoretical （理論的）な議論を整理したり、理論的課題を見出すレビューとに分けることができる。例えばあなたが、「ソーシャルメディアでの炎上における消費者行動を、Fluency theory （Schwarz, 2004; Schwarz et al., 2021） を用いて研究をする」という研究目的を持っていたと考える。このとき、「ソーシャルメディアでの炎上」に関する研究群を網羅的にレビューし、批判的検討を行うことは、substantive な文献レビューであると考えられる。多くの場合、substantiveな文献レビューによって、「そのトピックにおいて今までなにがわかっていて、何がわかっていないのか」を明確にし、研究課題を導出することが多い。一方で、Fluency theory とはどういうものであり、これまでどのような研究蓄積があるのかを整理し議論するような文献レビューは、theoretical な文献レビューであるといえる。 マーケティング領域の研究では、substantiveな文献レビューによって研究課題の明確化、提示している論文が多いものの、理論の精緻化の必要性から課題を明確化することももちろん可能である。つまり、文献レビューと言ってもその役割は大きく分けて二つあり（substantive vs. theoretical）、自身がどのような目的を持ってレビューを行っているのかについてその都度自覚的になる必要がある。その上でこのパートでは、「今までなにがわかっていて、何がわかっていないのか」を明示的に伝える必要がある。このパートではイントロダクションでは説明しきれないより詳細な既存研究の整理を行う。そのためには、著者自身が着目している研究領域や潮流を明確化し、提示した研究課題およびそれへの回答が、どのような貢献をその研究領域に与えるのかについて説明することが求められる。 第三の理論的基盤の文献レビューは先述の theoretical な文献レビューに相当する。このパートにおいては、著者の研究課題を解明するために依拠する理論やメカニズムを提示することが求められる。ここでは主に、研究の文脈に依存しない抽象度の高い理論的枠組みを定義、整理し、議論する。そのため、理論的基盤の参照のためにはマーケティングや経営領域に限らない分野の研究を参照することになる。例えば、先述のFluency theoryとは、人々の情報処理や思考過程を捉えたメタ認知理論であり、主観的な情報処理の容易さ（流暢性）により、態度形成や意思決定に影響を与えることが知られている（Schwarz, 2004）。Fluency theoryはソーシャルメディアにおける炎上固有の理論ではなく、様々な異なる文脈において議論、分析されてきた（Schwarz et al. 2021）。理論的基盤の議論ではこのような抽象度の高い議論について、説明的な記述をすることで、どのような見方で研究対象を論じるのかを明確にすることが求められる。また、理論の定義や潮流の整理に加えて、着目している理論の精緻化や拡張可能性などを見出した場合には、それも議論し、精緻化されたモデルを提示することが好ましい。 第四の仮説の提示においては、先述の理論的枠組みに基づき具体的な変数1に対応する仮説を提示する。なお、「理論」と「仮説」は3.5節で説明するように、根本的には同じものだと考えられる。しかしここでは理解のしやすさを優先し、便宜上研究文脈に依存しない抽象的な議論を理論、それよりも具体的で文脈依存的な予想を仮説と仮定する。仮説は通常、「H1: 〇〇が高まると、〜〜が高まる。」というような説明として提示される事が多い。なお、この仮説で用いられる〇〇などは、分析で扱う具体的な変数と一致していることが求められる。 これに加えて、このパートでは仮説のみではなく、「なぜ、もしくはどのようにして仮説のような予想が可能なのか」という、仮説導出に関わる論拠や理屈を説明することが求められる。論文内で複数の仮説を提示する場合には、各仮説ごとにその論拠も含めて明示的に提示するこが求められる。また、例外や対抗仮説（別の有力な理論）がある場合にはそれも提示し、どのような結果になれば、対抗仮説ではなく著者が依拠する理論仮説が支持されることになるかについても論じるとよい。 第五の調査分析手法の説明において研究者は、自身の立てた仮説を検証するために使う変数をどのように測定、観察（データとして収集）するのかについて、具体的に説明する必要がある。例えば、データ収集方法として、仮説検証のための証拠となるデータを、誰が、どこで、どのような方法で集めた、どんなデータなのかを説明することが求められる。マーケティングにおけるデータ収集では、二次データ（企業のIR情報、政府統計など）、質問紙調査（郵送、オンライン）などを活用することが多い。データの種類についての詳細は「データの種類とアンケートデザイン」節で説明する。これに加えてこのパートでは、どのような分析手法を用いるのかについても説明する必要がある。統計的な分析を実施する場合には、その分析モデルや手法についても明示することが求められる。 第六の分析結果パートでは、分析の結果をありのまま提示し、それが論文内で主張している仮説と整合的な（仮説を支持する）結果なのかを評価し、説明する。統計的な分析を行う場合には、統計学的な原則と自身が提示する結果の解釈が整合的であるかどうかに注意が必要となる。また、ここでは必要な情報を網羅しながらも、表などを用いながら簡潔に読みやすく構成することが求められる。 最後に議論パートでは、この研究がどのような理論的貢献と、実務的含意を有しているのかについて説明する。 理論的貢献では、本研究が既存の知識体系にどのような新たな知見を提供したのかを明示的かつ簡潔に説明する。実務的含意では、研究結果に基づく解釈として、マーケティング実務者へ具体的にどのような行動指針を提示できるかを述べる必要がある。また、このパートでは、研究の限界（手法上の限界など）についても説明する必要がある。なお、限界を説明する場合にはその限界に基づき、どのような将来的な研究機会を見いだせるのかについても説明することが好ましい。 企業におけるリサーチレポートでは、学術論文と異なるいくつかの工夫が必要になる。第一に、レポートの読者層を特定しその読者に向けて書く必要がある。例えば、企業におけるリサーチレポートを書くのであれば、主な読者はマーケティング意思決定者（管理職従事者）である。彼/彼女らは基本的に忙しく、かつ必ずしも統計的分析手法の専門家とは限らないため、レポートは簡潔かつ平易な言葉で書く必要がある。そのため、一般的に不必要な専門用語は避け、直感的かつ説明的に書くことが好ましい。どうしても技術的説明が必要になる場合には、脚注に記す等の工夫が必要になる。また、図表のような視覚的コンテンツで内容を補強することも有効である。第二に、企業におけるリサーチレポートでは、学術論文とは重視する点が異なる。特に、企業におけるレポートでは、既存の文献レビューや理論的議論については多くの場合不要であり、最低限に留める必要がある。このようなレポートでは、文献レビューや理論的議論のかわりに、実務的課題、研究課題、知見、実務的含意（助言）が簡潔かつ明示的に提示されることが好ましい。第三に、レポートの構成も学術論文とは異なることが多い。最もフォーマルな形式のリサーチレポートは、（1）序文、（2）本文、（3）付録という構成で作成される （Malhotra, 2019）。序文パートでは、タイトルページ、カバーレター、目次、エグゼクティブサマリー（要約や概要）、主な発見物や含意（助言等）、などを記載する。本文パートでは、問題背景と研究課題から始まり、調査・分析手法、結果、限界と留意点、結論と含意を説明する。最後に付録では、調査で用いたアンケート項目や詳細な分析結果表など、読み飛ばすことも可能だが、研究内容を詳細に理解するためには不可欠な情報を追加する。企業でのマーケティングリサーチレポートの場合、これらの工夫が必要になるが、リサーチを実行する際に経るプロセスについては学術的研究と共通する部分も多い。そのため、実際にリサーチを実行する前に、本節で紹介したリサーチの全体像を把握することが必要になる。 上記で紹介した構成は、研究を行い、それを文章化する上で必要な情報を捉えている。そのため、研究者は、現実的なトピック、理論、手法の三つの側面について知識を蓄積し研究に望むことが求められる。 変数の定義については色々とあるが、ここではデータにおける観測対象となっている情報とする（倉田・星野、2011）。↩︎ "],["question.html", "3.3 研究課題（問い）の設定", " 3.3 研究課題（問い）の設定 研究は、「問いを立てる」ことから始まる。「問いを立てる」というと、自身の疑問を提示することであり「そんなのは簡単だ」と感じる人もいるかもしれない。しかし、本資料は研究課題（研究上の問い）として、特に実証的に検証可能な問いに着目し、「問いを立てるのはなかなか難しい」という立場を取る。研究者は問いをただ闇雲に思いつくままに述べればいいわけではない。なぜならば、研究課題はその後のリサーチデザイン設計にも深く関わることになるためである。言い換えると、リサーチデザインは、自身の立てた研究課題にきちんと回答できるように設計すべきである。自身の立てた研究課題とその後のリサーチデザインや議論との一貫性を保つことは存外難しく、多くの人にとっては何度か失敗を繰り返しながら学ぶものになる。また、本節で説明する「実務的課題」と「研究課題」との関係は、（意思決定者のサポートとしての）企業におけるマーケティングリサーチにおいても重要になる。クライアントが抱える実務的課題に対して効果的な助言（含意）を提供するためには、直面する実務的課題に則した研究課題の設定が必要になる。そのため、本節の内容は企業におけるマーケティングリサーチ課題の設計方法としても役立つと考えられる。 マーケティング研究における問いを適切に立てるためには、「実務的課題」と「研究課題」という二つの異なる課題のタイプが存在することを理解すべきである。この2つの課題弁別は、前章で説明したマーケティングに関わる二つの視点に準拠するものである。実務的課題とは、マーケティングに関する意思決定についての課題であり、主に意思決定者が何をすべきなのかを捉えている。例えば、ある企業における製品の市場シェアが減少していたとする。ここで、「どうればよいのか？」という問いは典型的な実務的課題だと考える。また、たとえ具体的な方策に着目した問いを立てたとしても、例えば、「モバイル広告を実施すべきか？」や「どうすればオムニチャネル化を推進できるか？」といった問いも実務的課題だといえる。マーケティングに興味を持つ学生の場合、このような実務的課題に関連する、問題を解決するための手段やアイデアを扱うことに慣れているかもしれない。しかし、これら問いに直接的に答えることが必ずしも研究にはならないということを理解する必要がある。 一方で研究課題（ここでは特に実証的な研究課題）とは、実証的に検証可能なものであり、現実社会で何が起きているのかについて、定量/定性的調査を通じて得た情報を用いて結論を提示できる問いを指す。例えば、「ファストリテイリングは何年から有明倉庫を稼働させたか？」という問いは、事実を調べることで回答できる。また、「新しいパッケージデザインは以前のものよりも消費者の購買意図を高めるか？」という問いも研究課題の例であり、消費者を対象とした実験調査によって回答可能である。多くの場合前者のような問いは単純すぎて研究課題として利用されない。問いの価値は、3.2節で説明したように、その問いに答えることでどのような含意が得られるか、という視点から substantive と theoreticalの両側面から評価される。 ここで本資料が強調したいのは、研究の実行においては研究課題の提示が絶対に必要なのだが、我々が経営学という応用学問領域に属している以上、実務的課題も重要だという点である。マーケティングにおいては多くの場合、社会や実務で起こっている問題に対し何らかの示唆を与える研究を行うことが求められる。そのため、研究のための研究ではなく、社会や実務への含意を見いだせるような研究が重視される傾向にある。つまり我々にとっては実務的課題も重要になるものの、先述の通り実務的課題のままでは研究課題として機能しない。そこで、実務的課題を実証的研究課題に変換することが求められる。問いの変換方法として、ここでは浅野・矢内（2018）で提示されている二つの方法について取り上げる2。 第一の方法が、「参照枠組みを変える」という方法である。具体的には、実務的課題から議論の対象となる主体を特定し、彼/彼女らの評価について情報を収集する形に問いを変換するような方法だと言える。例えば、「企業は環境負荷に配慮されたチョコ製品を販売すべきか？」という実務的課題を考える。このとき例えば、企業が販売したチョコ製品を購入する主体である既存顧客を、問いの中心となる主体として設定することで、「既存顧客のチョコ製品選択に対し、企業による環境対応の有無は影響を与えるか？」という問いに変換することができる。この変換後の問いであれば、既存顧客へのアンケート等でデータを集め実証可能であるため、研究課題として機能すると考えられる。同様の問いを特定の企業活動に限定しない形で提示するならば、例えば「企業の環境、社会・ガバナンス（ESG）活動はチョコレート菓子市場における消費者の購買意図を向上させるのか？」という問いも設定可能である。 第二の方法は、「背後に想定されている暗黙の前提を問う」というものである。これは、実務的問いの背後に暗黙的に仮定されている理屈やメカニズムに自覚的になり、それ自体を問うものである。例えば、「どうすればオムニチャネル化を推進できるか（or すべきか）？」という実務的課題があったと考える。この問いの背後には、「オムニチャネル化が企業成果に好ましい影響を与えるはずだ」という暗黙の前提が置かれているかもしれない。オムニチャネルに限らず、多くのビジネス書などで話題になる戦略では、このような成果に対する暗黙の前提が置かれ、その用語だけが独り歩きして流行ることも散見される。では「そもそもオムニチャネル化は本当に企業成果に影響があるのかだろうか」、もしあるのだとすれば「どのような成果に対して影響があるのか」という問いは、暗黙の前提を問うものであり、非常に素朴だが重要な研究課題である。これに関連する研究として、「オムニチャネル化（チャネル間統合）は小売企業の売上成長率に影響するのか？」（Cao and Li, 2015）や「オムニチャネル化（チャネル間統合）は小売企業の費用効率性に影響するのか？」（Tagashira and Minami, 2019）といった研究課題が実際に扱われ、論文化されている。 暗黙の前提を捉えた別の具体的な研究例として、Lim et al. （2020） による、顧客満足度が成果へ与える影響について捉えた論文が挙げられる。顧客満足度はマーケティングにおいて非常に重要視される概念である。顧客満足度の企業にとっての重要性は、顧客満足度が高まることによって顧客による再購買が増え、企業のマーケティング費用が効率化される、というロジックによって説明されてきた。しかしながらそれは本当だろうか、というのが Lim et al. （2020） の研究課題である。Lim et al.（2020）の結果が気になる場合はぜひ論文を読んでみて欲しい。このように暗黙の前提を問うような研究課題の設定は、非常に素朴な問いになるがそれだけに、もしそれが既存研究で未解決である場合には大きな理論的貢献につながる可能性を持つ。 本節では、研究課題の設定について説明した。本講義で扱う研究課題は、検証可能であり実務的課題とは異なるものであるという点を理解してほしい。また、次章（4.1節）ではリサーチデザインと研究課題との関連について説明しているため、そちらも合わせて研究課題設計について理解してほしい。次節では自ら立てた問いと整合的な議論を提示するための理論や仮説構築について説明する。 浅野・矢内（2018）では、規範的議論と実証的議論との対比で以下の内容を提示している。規範的議論に関心がある場合は、浅野・矢内（2018）を参照してほしい。↩︎ "],["問題と分析をつなげる仮説の提示.html", "3.4 問題と分析をつなげる仮説の提示", " 3.4 問題と分析をつなげる仮説の提示 マーケティングに関する実証分析は、依拠した理論に基づく仮説を提示し、データを用いた分析によって仮説を検証するという形式が取る事が多い。言い換えると、事象を観察し、論理的な説明としての仮説を提示ししたうえで、それを客観的・科学的と考えられる手順で検証するというプロセスを通じて証明を行う。そのため、マーケティング研究における仮説は、検証可能かつ研究課題や研究が依拠する理論と整合的である必要がある。 本資料では、理論を事象の原因と結果に関する一般的（抽象度の高い）理屈であると考える（浅野・矢内, 2018）。詳しくは後述するが、理論について詳細に考察すると、理論は現在広く受け入れられている仮説だと考えられる。そういった意味で、「理論」と「仮説」の間に本質的な違いはないといえる。しかしながら、マーケティング領域では、様々な具体的変数に応用できる「抽象度の高い構成概念同士の関係」を理論と呼び、「より具体的な変数間の関係」を仮説と呼ぶことが多い。ここでいう構成概念とは心理学的研究アプローチにおいて用いられる、直接観測することはできないがその存在を仮定することで測定や観測を可能にするために構成された抽象的概念のことである。例えばマーケティング分野では、「顧客満足」や「顧客エンゲージメント」といった構成概念が用いられる。そのため、本資料では説明の容易さのために、理論と仮説という言葉を区別して用いる。具体的には、理論とは抽象度の高い概念同士の関係を表し、仮説（作業仮説）は、理論を検証するために引き出された特定の変数間の関係に関する記述を指す（浅野・矢内, 2018）。そのうえで仮説は、データに基づく検証のベースとなる記述であるため、その内容は入手可能な変数間の関係として記述することが大切になる。 ここで、「お金がある人ほど衝動買いをする」という理論があったと仮定して、作業仮説化について考えてみる。作業仮説化においては、この理論と整合的かつ測定（検証）可能な変数を捉えた記述であることが重要だと述べた。それを踏まえ、以下の二つの作業仮説例を考える。 「年収」の高い人ほど「衝動買い性向の程度」が高い 「買い物時の予算」が多い人ほどその買い物における「非計画購買購入額」が高い （1）の例は、年収という個人属性と衝動買い性向という心理尺度を捉えており、個人の特性を表す二変数間の関係を示した仮説である。一方で、（2）はある購買客の入店時予算とその買い物時に発生した非計画購買額を捉えている。この二つの例は、作業仮説化において重要な要素である「分析単位の一貫性」を満たしている。基本的に作業仮説化で捉える変数は同一の分析単位である必要がある。どちらの例も、特定の消費者に関する（1）属性と心理尺度と、（2）買い物時の予算と購買額、という形で測定単位が一致している。これがもし、消費者個人の特徴と店舗での売上との関係を記述した仮説である場合、分析単位が異なるため、データによる分析と仮説検証が困難になる。したがって、特別な場合を除き分析単位の一貫性を守ることは重要となる。 先述の二つの仮説は分析単位の一貫性は守っていた一方で、「理論を正確に参照する」という点においては注意が必要である。「お金がある人ほど衝動買いをする」という理論では、「お金」と「衝動買い」という二つの概念間の正の関係が示唆されている。それに対して一つめの仮説では、「年収」という個人属性を示す変数で「お金」という概念を捉えており、個人の心理的傾向としての「衝動買いのしやすさ」を「衝動買い」の変数として扱っている。他方で二つめの仮説では、「特定の購買時点での予算」と、その買い物での「非計画購買の額」を捉えている。「お金がある人ほど衝動買いをする」という理論から導出された仮説としては、どちらもある程度の一貫性がありそうだが、両者は全く違う変数を捉えている。この場合、どちらか一方もしくはどちらも不正確に理論を参照している可能性がある。もしかしたらその理論を提唱している最初の論文を正確に参照すれば、消費者の所得と心理的性向を捉えたものであることがわかり、一つめの仮説化が適切であることが判明するかもしれない。しかしながら、「お金」や「衝動買い」という曖昧で自分にとって理解しやすい別の言葉に置き換えて理論を参照している状態では、その判断もつかない。そのため、抽象度の高い理論に関する論文が難解であったとしても、できる限り正確に、論文が述べていることをそのまま理解する必要がある。また、無意識的にしろ意識的にしろ、研究者にとって都合の良い（測定しやすい）文脈に理論を読み違えて仮説化してしまう場合も散見される。このような問題を避けるためにも、その理論が具体的にどのような視点に基づき議論を展開しているのかについてできるだけ正確に内容を理解することが必要になる。 多くのマーケティング研究では、抽象度の高い理論に基づき仮説を提示し、データを用いて仮説を検証する。仮説の提示においては、着目する研究課題、理論、分析単位の一貫性を保つことが重要になる。本節の後半では特に、理論を正確に参照することの重要性について強調した。本章の以降の節では、学術的研究を実行するために必要となる、マーケティング研究に関する知識や考え方を紹介する。具体的には、学術的研究を実施するうえで特に重要になる科学哲学や、理論的貢献の提示について説明する。 "],["学術的研究のための留意点.html", "3.5 学術的研究のための留意点", " 3.5 学術的研究のための留意点 3.5.1 マーケティング研究と科学哲学 本資料では、主にマーケティングに関する定量的な調査・分析手法を紹介している。それでは、これらの手法によって明らかにされた知見はどのような意味を持つのだろうか。人によっては、データに基づき定量的に示された結果は、揺るぎないこの世の真実であると考えるかもしれない。しかしながら、そのような捉え方は科学的とは言い難い。そのために本節では、研究を行うことそのものの目的について学術的視点から考えることで、社会科学における科学的姿勢とはどのようなものかを概観する。 「科学的であるということはどういうことか。」この問題について考える領域は科学哲学と呼ばれており、本節では社会科学における科学哲学（吉田, 2021）とマーケティング研究におけるパラダイム議論（宇野ほか, 2022）の議論を整理した内容を紹介する。そのため、この議論に関心のある場合には、これらの文献およびそこで提示されている関連図書を参照してほしい。なお科学哲学に関する議論は、学術的研究を実施する際に特に重要な内容である。そのため、本節は大学院修士・博士課程に在籍、または今後進学を考えている読者に特に理解してほしい。科学哲学の潮流においては、論理実証主義、反証主義、解釈主義、などいくつかの主要な考え方が存在する。宇野ほか（2022）によると、マーケティング領域においては実証主義と解釈主義という二つの主義が主流のパラダイムとして存在する。実証主義では研究者は現象から独立した存在という立場のもと、普遍的な法則見出すことで知識を形成すること目指す。一方で解釈主義では、研究者が現実から独立し事象を観察することは困難であるという立場をとる。 その上で本資料では、より広範な社会科学における科学哲学との関連で議論が可能になるように、吉田（2021）の議論に従い、主に自然主義と解釈主義という二つの考え方に大別し議論を整理する。第一に、自然主義は現在の社会科学研究における有力な立場であり、社会現象は自然現象と同じように研究できると考える。これは、特に数学や物理学を中心に自然科学として蓄積されてきた方法論や考え方を採用することで社会現象に関する客観的かつ信頼性の高い知識の蓄積を試みる考え方である。上述の実証主義はこの自然主義的潮流に含まれる、19世紀に論じられた哲学である。その後様々な批判や社会変化を受け修正されることで、20世紀前半には論理実証主義という形で論じられるようになった。論理実証主義では、論理的に推測されたものが客観的な経験的事実に基づいて証明されたとき、それを科学的に正しいと捉える。そのうえで、このパラダイムでは再現可能性が重視された。しかし現実的には、科学的に正しいとされた検証結果が後に覆ることもある。その際、元々正しいとされた結論をどのように受け止めるべきなのかという問題が残る。また、論理実証主義的立場からは、観察・検証できないものは科学でないと切り捨てざるを得ない。 これらの問題を克服すべく議論されたのが、ポパーによる反証主義である（ポパー, 1980; 吉田, 2021）。反証主義においては、科学的であることの条件として「反証可能性」を重視する。これは、ある主張が科学的であるためにはその主張が間違っていることを証明される可能性を有している必要があるとする考え方である。反証可能性の詳しい説明と具体例については後述するが、このように自然科学的科学観に基づき社会現象を研究しようとする考え方を総じて自然主義と呼び、現代の社会科学研究の主流となっている（吉田,2021）。なお、宇野ほか（2022）によると、2016年から2021年に出版されたマーケティング領域のトップジャーナル8誌3に掲載された論文のうち、96%以上が実証主義的アプローチに基づくものであったとされている。 第二の解釈主義は、社会現象の研究には独自の手法が必要であるとする立場である。本資料では解釈主義に関する詳細な説明は避けるが、この考え方では特に社会現象においては因果的説明だけでなく、社会現象を構成する人々への内的な理解も必要であると考えられている。また、解釈主義的立場は反実証主義的立場を取り、社会科学研究においては研究者や研究対象者の意図を排除することはできないので、自然科学のように客観的かつ中立的な研究を実施することはできないと主張する。社会科学は自然科学とは異なり、社会現象を研究対象としている。この社会現象の特殊性が、解釈主義者が自然主義を批判する根拠となる。特に社会現象においては、「意図せざる結果」の影響が自然科学に比べて強いということが言われている。この意図せざる結果は主に「自己成就的予言」と「自己破壊的予言」に分けて説明することができる。自己成就的予言とは、たとえ事実に基づかない予言であっても、何らかの発言（予言）を行うことでそれが実現してしまうことである。例えば社会的不安を煽るデマ情報が流布され、その情報を目にした民衆がその不安を信じ込んでしまうことで、本来は根拠ないデマであった予言が実現してしまうことである。一方で自己破壊的予言は、予言されたことによってそれが実現されなくなることである。例えば、根拠があるものの楽観的な予想が提示されたことによって、当事者が油断しまいその予言が実現しなくなることが典型的な例として挙げられる。 解釈主義者が主張するように社会現象と自然現象には違いがある。違いがあること自体は事実であるため、社会科学がただ暗黙的に自然科学と全く同じ手法を踏襲するということが好ましくない場合もあるかもしれない。しかしながら、だからといって自然科学で蓄積されてきた客観的かつ信頼性の高い手法を放棄すべきだということはあまりに極端な主張であるとも考えられる。これらの主義については唯一絶対の正解が存在するわけではなく、継続的な議論や批判による発展が必要になる。しかしながら吉田（2021）は、現時点において社会科学分野で科学的研究を行うためには、研究プロセスは最低限、推測と反駁の方法として「反証可能性」を有している必要があると主張する。この反証可能性は、先述の通り反証主義的考え方であり、「漸進的な発展を想定した科学観」を反映している。反証可能性はテスト可能性などとも呼ばれ、科学的理論それ自体が正しいのか誤っているのかを確認することができる可能性を表す。反証可能性はポパーによる反証主義において科学的基本として捉えられており、科学的な理論や主張はそれを反証する余地を有する必要があると議論されている。つまり、科学とは何らかの真理に至るための方法論であり、何らかの絶対的な真理を前提とすることは科学的ではないと考える。そしてこの反証可能性が科学的か非科学的かを分ける決定的な違いであると捉えられている。 反証可能性を有していない議論の例としては、フロイトによる精神分析が有名である。精神分析は常に正しい理論であるため、科学的ではないと言われている。この点について伊勢田（2003）は、以下のように説明している。精神分析の枠組みでは、人間の心は自我（意識的欲求）、超自我（社会的・道徳的行動統制）と、イド（無意識の欲求）によって構成されている。この理論において、研究者が潜在的な無意識の欲求が本当に存在するのかという問いに関心をもったと想定する。研究者による調査・分析によって、その潜在的な欲求を示唆する行動が観察されれば潜在的欲求仮説は支持される。しかし、そのような行動が観察できない場合、どのように結論付けられるのか？フロイトの理論では、「無意識の欲求は存在するが、超自我によって統制されて顕在化しない」と説明される。すなわち、無意識の欲求の存在についてこの理論は反証可能性を有していないことになる。このように反証されるリスクを背負っていない主張は、反証可能性を軸とした科学哲学に基づくと科学的ではないということになる。 例えば、「AはBを高める」という予想があったとしよう。この予想が科学的である場合、特定の手続きを経て調査・分析を行い、ある結果が出ればこの予想を受けいれ、それ以外の結果であればこの予想が誤っていると結論づけることができる。このように、ある理論が科学的であるためには、絶えず反証による理論の修正や、新たな理論の提案を行うことが可能であることが重要となる（ポパー, 1980; 吉田, 2021）。その上で本資料では自然主義的な立場を取りつつも、社会現象が持つ独自性をもつことも認める。そのうえで吉田（2021）の議論の通り反証可能性をもつことが科学的知識の条件として考える。そのため、調査・分析の結果も唯一絶対の真理ではなく常に反証される可能性を有している、という理解のもと本資料の内容が構成されていることを理解して欲しい。本節では、我々が調査によって明らかにする知見・理論が持つ特性や目的について、主に反証可能性という概念の重要性を論じてきた。次節では、次節では、学術的研究において重要になる理論および理論的貢献について説明・紹介する。 Journal of Consumer Psychology，Journal of Consumer Research，Journal of Marketing，Journal of Marketing Research，Journal of the Academy of Marketing Science，Marketing Science，International Journal of Research in Marketing，Journal of Retailing↩︎ "],["研究における理論の利用と精緻化.html", "3.6 研究における理論の利用と精緻化", " 3.6 研究における理論の利用と精緻化 実証的なマーケティング研究では、結果や議論との間の論理的整合性に寄与する「理論」が重要になる。本講義では理論を、何らかの客観的真理を探求するための主張だとする「実在論」的な立場に基づき捉える。しかしながらここで強調したいのは、科学的な理論が「絶対的かつ不変の真理を意味しない」ということである。言い換えると、理論はあくまで暫定的な仮説に過ぎず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくことが求められる。なお、実在論と相反する立場として「道具主義」という考え方も存在する（吉田, 2021）。道具主義では、科学理論を予想や説明のための便利な道具として捉えるため、用いている理論の経験的正しさは問題視しない。これらの異なる立場に対する優劣を決めるのは困難である。しかしいずれの立場においても、可謬性という、「知識についての主張は原理的に誤りうる」という性質を受け入れ、自己批判と相互批判に基づきより良い理論を目指すことが重要ではないかと言われている（吉田, 2021）。 本資料では、先述の通り、理論を事象の原因と結果に関する一般的（抽象度の高い）理屈であると考える（浅野・矢内, 2018）。マーケティング領域では様々な分野から理論を応用することになるのだが、学術的研究では理論的貢献として、着目する理論的議論や理解を前進させることが求められる。広義の経営学領域における理論的貢献に関して Fisher and Aguinis （2017） は、theory generation（理論生成）、theory testing（理論検証）、theory elaboration（理論精緻化）という3種類のアプローチを紹介している。Theory generationは、ゼロから新たに検証可能な理論や概念を提示することを目的とする。これは、まだ既存の理論がなく説明されていない現象について、新たな構成概念の導出や概念間の関係について論理的かつ十分に納得の行く議論を展開することで新たな知見を提示するアプローチである。一方で theory testing は、既存の理論を検証することを目的とするアプローチである。既存の理論から具体的な仮説を導出し、データを収集し分析することで、提示した仮説的関係が支持されるかを検証するアプローチである。 Fisher and Aguinis （2017） では、theory generation（理論の生成） と theory testing（理論の検証） よりも特に、既存の理論を洗練させ、拡張することに重点を置いたtheory elaboration（理論の精緻化）の重要性が強調された。Theory elaboration は、theory generation や theory testing よりも、漸進的な理論のアップデートについて捉えた考え方であり、既存の理論からの積み重ねによって理論を改善していくプロセスとして期待される。Theory elaboration の主な実施アプローチとして Fisher and Aguinis （2017） は、「対比（Contrasting）」「構成要素の特定（Construct specification）」「構造化（Structuring）」の3つを提示した。 「対比」アプローチでは、異なる理論や構成要素を比較し、類似点や相違点を明らかにする。このアプローチに関連する具体的なプロセスとして、水平的対比と垂直的対比がある。水平的対比は、既存の理論を異なる文脈で検討することで、その理論がどのように適合するかを調べるプロセスである。これにより、異なる文脈での理論の有用性や限界が明らかになる。垂直的対比は、同じ文脈内で異なるレベルの抽象度を持つ理論や構成要素を比較することで、新しい洞察を得るプロセスである。これにより、より具体的または抽象的なレベルで理論を分析し、新しい仮説や洞察を導き出すことが期待される。 「構成要素の特定」アプローチでは、構成要素定義の洗練化や明確化によって構成要素の妥当性を向上させることを目的とする。このアプローチに関連する具体的なプロセスとして、新しい構成要素の指定や構成要素の分割が考えられる。新しい構成要素の指定は、既存の理論で考慮されていなかった新しい概念を特定し、それを定義することである。これにより、既存の理論がカバーしていなかった現実世界の側面を捉えることができる。一方で構成要素の分割は、既存の理論内の概念をより明確に区別し分けて捉えることである。これにより、類似した概念間の区別が明確化され、それらが異なる現象を説明するために使用できることを示すことで、理論の妥当性や範囲を向上させる。 「構造化」アプローチでは、構成要素間の関係をより体系的に整理することを目指す。このアプローチに関連する具体的なプロセスとして、「特定の関係の構造化」、「連続的関係の構造化」、「循環的関係の構造化」などがある。「特定の関係の構造化」では、二つの概念間の特定の関係を明確に定義する。これにより、異なる概念間の関係性がより明確になり、理論が現実世界にどのように適合するかが明らかになる。「連続的関係の構造化」は、概念やイベントに関する一連や順序的な共起関係を説明することである。これにより、現象やイベントがどのように発生し、その結果として次にどのように進行していくかを説明することができる。 最後に、「循環的関係の構造化」は、二つ（以上）の概念間での相互作用（双方向の因果）を説明する。これにより、理論が現実世界でどのように適合し、相互作用がどのように進行するかを説明することができる。 本節では、理論を何らかの真理を探求するためのものだと捉えた。しかしながら、理論は唯一絶対の真理を意味せず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくものである。その意味において理論と仮説は本質的には同質的であるものの、マーケティング領域では、抽象的で様々な分野へ応用可能な概念間の関係を理論、より具体的な変数間の関係を仮説と言う事が多い。また本節では、先行研究から理論をただ援用するだけではなく、既存研究へ理論的貢献を与えるための theory elaboration について説明した。Theory generation や theory testing だけでなく、より漸進的 な理論のアップデートについてその具体的なアプローチも重ねて理解することで、より発展的な論文執筆に役立つことを期待している。 "],["参考文献-1.html", "3.7 参考文献", " 3.7 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. 伊勢田哲治 (2003) 「疑似科学と科学の哲学」, 名古屋大学出版会. 宇野舞・グェン フォン バオ チャウ・趙雅欣・六嶋俊太・福川恭子（2022）『マーケティング研究領域におけるパラダイムの現状と課題 : より活発な理論発展へ向けて』,「一橋商学論叢」, 17(2), 14-30. 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. ポパーカール (1980) 「推測と反駁-科学的知識の発展」, 藤本隆志・石垣壽郎・森博訳, 法政大学出版局. ベルクラッセル・フィッシャーアイリーン・コジネッツロバート（2016）「消費者理解のための定性的マーケティング・リサーチ」，松井剛訳，碩学社. 吉田敬 (2021) 「社会科学の哲学入門」, 勁草書房. Cao, L., &amp; Li, L. (2015). The Impact of Cross-Channel Integration on Retailers’ Sales Growth. Journal of Retailing, 91(2), 198-216. Enns, P. K., Lagodny, J., &amp; Schuldt, J. P. (2017). Understanding the 2016 US presidential polls: The importance of hidden Trump supporters. Statistics, Politics and Policy, 8(1), 41–63. Fisher, G., &amp; Aguinis, H. (2017). Using Theory Elaboration to Make Theoretical Advancements. Organizational Research Methods, 20(3), 438-464. Hui, S. K., Inman, J. J., Huang, Y., &amp; Suher, J. (2013). The Effect of In-Store Travel Distance on Unplanned Spending: Applications to Mobile Promotion Strategies. Journal of Marketing, 77(2), 1-16. Lim, L. G., Tuli, K. R., &amp; Grewal, R. (2020). Customer Satisfaction and Its Impact on the Future Costs of Selling. Journal of Marketing, 84(4), 23-44. Malhotra, N. (2019) Marketing Research: An Applied Orientation, Pearson Education Limited. Schwarz, N. (2004). Metacognitive Experiences in Consumer Judgment and Decision Making. Journal of Consumer Psychology, 14(4), 332-348. Schwarz, N., Jalbert, M., Noah, T., &amp; Zhang, L. (2021). Metacognitive experiences as information: Processing fluency in consumer judgment and decision making. Consumer Psychology Review, 4(1), 4-25. Scholz, J., &amp; Smith, A. N. (2019). Branding in the age of social media firestorms: How to create brand value by fighting back online. Journal of Marketing Management, 35(11-12), 1100–1134. Tagashira, T., &amp; Minami, C. (2019). The Effect of Cross-Channel Integration on Cost Efficiency. Journal of Interactive Marketing, 47, 68-83. Tagashira, T., Fukukawa, K., &amp; Matsui, T. (2023). Hidden Supporters in Online Firestorms: Investigating the Effects of Ethical Problem Recognition and Ethical Predispositions in Public and Private Spheres. Working Paper, Available upon request. "],["survey.html", "Chapter 4 データの種類とアンケートデザイン", " Chapter 4 データの種類とアンケートデザイン 研究においては、変数間の関係について分析、記述することが多い。仮説を提示する際の注意点においても、分析に用いる変数と整合的であることが重要であると述べた。そこで重要なのは、自身が用いる変数がどのような特徴を持つ尺度であり、どのように事象を測定しているかを理解することである。以下の表は、主な尺度のタイプとその特徴をまとめたものである。 Table 4.1: 尺度表 尺度 特徴 例 名義 対象の識別と分類 性別・職種 順序 対象の相対的ポジション 好み順位・ランキング 間隔 対象間の大小関係比較（原点は定まっていない） 態度・指数・気温 比率 連続的関係（原点が定まっており、比率計算も可能） 所得・売上 上表における間隔尺度や比率尺度は一般的に量的尺度と分類される。売上高はマーケティング研究における最も典型的な「比率尺度」の例である。この尺度は大小にも間隔にも意味があり、かつ比率にも意味があるため、四則演算に対応する尺度である。一方で「間隔尺度」は正の整数で表される尺度であり、その値の大小関係と間隔にも意味があるものの、原点が定まっておらず、比率計算に耐えない尺度である。マーケティング研究においては、アンケート調査における質問項目や、質問項目の合計値（およびそれを項目数で割ったもの）である合成変数が間隔尺度の典型的な例である。 一方でマーケティング研究においては、必ずしも量的ではない情報に着目して分析を行うことも多い。そのような場合には、質的尺度を用いて観察対象のカテゴリを分類することで分析可能にする。例えば、消費者の購買行動に関する東京、大阪、北海道という地域（都道府）間の差を分析する場合を考える。このとき、「地域の違い」に数量的な違いは存在しないものの、地域の違いを表すために東京 = 1, 大阪 = 2, 北海道 = 3 のような地域コードを用いる事が多い。しかしながら、この変数がとる数値そのものに本来的な意味はなく、東京が大阪と北海道よりも低い値を取っているという解釈は適切でない。この変数はあくまで異なる地域に分類されることを示しているのみである。このような属性の分類や有無を表すための尺度を「名義尺度」と呼ぶ。また、観測対象が特定の属性に対応する場合（例、男性）には 1 を、そうでない場合には 0 を取るような、1 と 0 で分類された名義尺度のことを特に「ダミー変数」と呼ぶ。ダミー変数は分析結果の解釈が容易になる利点もあるが、詳しくは後述する。質的尺度のもうひとつの例が「順序尺度」である。順序尺度は、観察対象の序列や大小関係を表す尺度であるが、その数値の間隔に意味はない。例えば 1 が最低であり、4 が最高となるような金融商品の等級において、商品 A は ランク 4、商品 B はランク 1 だとする。このとき、 B は A よりも高い評価を受けているということは言えるが、A は B の 4倍優れているという議論は不適切である。このように、各対象間の推移性を表現するときに用いるのが、順序尺度である。 次に、データの収集や取得という観点に基づくデータ分類基準を提示する。研究の遂行においては、研究者の関心や研究課題のために収集されたデータを用いることもあれば、別の目的で収集されたデータを用いることもある。前者のようなデータを「一次データ」、後者を「二次データ」と呼ぶ。一次データは研究上の問いに回答するために実施された調査、実験や観察に基づき収集形成されたデータある。一方で二次データは、業務上蓄積されたデータ、民間リサーチ会社の統計データや、政府統計などに代表される他の目的で収集された、ないし継続して収集されているデータを指す。現在は、様々な二次データがアクセス可能であり、二次データを利用することで研究上の問いに回答できる可能性も十分にある。例えば、企業の視点にたてば、組織内部の二次データ（業務活動で得たデータ: Point of sales (POS) データ、webサイトへのアクセス記録など）と外部の二次データ（民間リサーチ会社の統計データや政府統計など）が存在する。その他にも、オープンソース化されているデータも様々存在する。そのため、本書では一時データを収集する前に、関連する二次データとしてどのようなものが存在しアクセス可能なのか検討することを勧める。しかしながら、二次データではすでに集計や加工をされたデータしか入手できず、raw データ（収集されたまま加工されていないデータ）にアクセスできない場合もある。そのため、入手可能な二次データが本当に自身の研究課題や仮説で議論されている内容および集計レベルと整合的なのか、という点については慎重に検討する必要がある。 データのタイプは、集計レベル（分析単位）に基づいて分類することもできる。この分類では、個人の行動や回答を捉えた「非集計データ」と、非集計データをある単位でまとめ、計算や整理を施した「集計データ」とを捉えることができる。非集計データの例としては、ID-POSデータ（ロイヤルティカード（アプリ）などによる顧客の個人IDと、購買製品、価格、数量などの情報が含まれたPOSデータを結合したもの）や、消費者個人を対象としたアンケートデータなどが挙げられる。一方で企業成果・業績などの財務データは、企業レベルで集計されたデータだと言える。自身の研究課題や仮説がどのような集計レベルのデータに対応するものなのかを考え、研究内容と一貫したデータを用いることが必要になる。 "],["記述的リサーチデザイン.html", "4.1 記述的リサーチデザイン", " 4.1 記述的リサーチデザイン コンピュータやソフトウェアが発展した現代においては、データを収集し、ある変数についての情報を要約、分析すること自体はさほど難しいことではない。しかしながら、実務的ないし学術的に意義のある研究を行うのは容易ではない。そのためには、研究課題について慎重に吟味しつつ、適切なリサーチデザインを決定する必要がある。リサーチデザインとは、研究プロジェクトを実行するための枠組みや計画案のことである（Malhotra, 2019）。経営学分野におけるリサーチデザインは、田村(2006) にて検討されている。ここでは、Malhotra (2019) を参照に記述的リサーチデザインを中心に説明する。 Malhotra (2019) は、マーケティング・リサーチのタイプとして、大きく分けて (1) 探索的リサーチ、(2) 決定的 (conclusive) リサーチがあるとし、決定的リサーチの中にさらに (3) 記述的リサーチと (4) 因果リサーチがあるとした。探索的リサーチでは、その研究プロセスが非構造（または半構造）的かつ柔軟であり、一般的に定性的方法が用いられることが多い。研究者は、研究や実務に関するアイディアやインサイトの発見、仮説の構築、未開拓領域における研究課題の定義、等を目的にこのタイプの研究を採用することが多い。一方で決定的リサーチを採用する研究者は、特定の仮説を検証したり、変数同士の関係についての分析を目的とすることが多い。このアプローチでは、リサーチプロセスはより構造化され、定量的な分析手法が用いられることが多い。 リサーチデザイン分類 決定的リサーチの中でも、記述的リサーチは、特定の変数や複数の変数間の関係を明らかにすることで、実証的な問いに回答しようと試みる。記述的リサーチを用いた研究目的の例としては、(1) 関連するグループ（顧客や商圏）の特徴を説明する、 (2) 顧客全体のうち特定の行動を取る顧客の比率はどの程度なのか明らかにする (e.g. ヘビーユーザーの比率)、(3) 企業の操作するマーケティング変数と顧客の購買意図との関係を明らかにする、などが挙げられる。これらはあくまで例であるため、あえて抽象的な表現を用いているが、実際に記述的リサーチ課題を提示する際には、検証可能なレベルにまで焦点を絞ることが必要になる。そして、本書ではおもにこの記述的リサーチに焦点を合わせて、手法を紹介する。 一方で因果リサーチでは、相関関係と因果関係を区別し、先行要因が結果変数へ与える効果について因果関係を想定した形で分析することを指す。相関関係とは、2つの変数XとYの間に、比例や反比例といった共変関係があることを指す。一方で、要因Xを変化させることで要因Yが変化する時、Xを原因でYを結果とする因果関係がある（X\\(\\rightarrow\\)Yと示す）という。つまり相関関係とは異なり、因果関係ではどちらが先行要因であるかという前後関係がはっきりしている。このような因果関係を明らかにする分析手法は一般的に因果推論と言われ、記述的リサーチよりも高度な調査設計・統計的手法が必要となる。本講義は、マーケティングにおける因果推論の応用は扱わないため、別資料を参照してほしい。 リサーチデザインは、研究上の問いと整合的である必要がある。言い換えると、記述的リサーチにより自身の立てた問いにきちんと答えるためには、記述的リサーチで回答可能な問いを考えなければならない。例えば、「Eコマースにおける製品品質補償の提示は新規顧客の獲得につながるのか？」という問いは補償サービスと新規顧客によるEコマースサイトの選択率という2つの変数間の関係にまで論点を落とし込んだ問いとなっており調査・分析が可能である。しかしながら、学生のレポートでは、「Eコマースにおいて顧客獲得に重要な要素はなにか？」のような問いが散見される。このような（何か、なぜか、どのような等）広範な視座を持つ問いは探索的に検討すべきものであり、記述的リサーチによって検証可能な形に具体化がされていない。そのため、このような問いに整合的な回答を提示するためには、探索的リサーチデザインが必要になる。高度な研究では、混合メソッド（Mixed method）アプローチとして、探索的リサーチ（例、インタビューや理論的整理）を行ったあとに記述的リサーチ（例、質問紙調査）を実施するような方法も存在する。しかしながら、本書では単一のリサーチメソッドの採用に焦点を合わせ、記述的リサーチと整合的な問いを立てることを推奨する。 「何か、なぜか、どのような」を捉えた問いは研究者の素朴な疑問が表出しており、研究者の現実的・理論的情報のインプットが不足していることにより提示されることが多い。そのため、先行研究や実務的情報をインプットしたあとに、より具体的で重要性の高い問題を捉えることが、より意義のある検証可能な研究課題につながる。優れたリサーチデザインは、自身の立てた問いに対してどのように調査・分析を行えば整合的な回答を提示できるかを具体化したものであり、研究全体の以下の要素間の一貫性や整合性を保つことができる。 着目する事象（実務的課題） 研究課題 （理論と）仮説 採用する具体的な変数 調査・分析手法 基本的には、リサーチデザインそのものに優劣はなく、「ある方法を採用しているから別の方法よりも優れている」ということはない。ただし、方法そのものがアップデートされたり、ある手法が誤っていると研究によって明らかになることがある。そのような場合を除きどのようなリサーチデザインを採用しているから優れているないし劣っている研究であるということは言えない。そのため、リサーチデザインの決定においては、「自身の立てた問いにきちんと回答できるか」という問いとの整合性が重要になる。この点に注意をして研究課題とリサーチデザインの設計をセットで考えるようにクセづけてほしい。 "],["アンケート設計.html", "4.2 アンケート設計", " 4.2 アンケート設計 マーケティングにおける記述的リサーチアプローチとして広く用いられれているのが、質問紙（アンケート）調査である。マーケティングにおける調査や研究では行動データ等では捉えきれない消費者の態度や知覚について知ることを目的とする場合も多い。このような目的を達成するため、これまでの研究では伝統的に質問紙調査によって情報を得てきた。質問紙調査とは、データ収集のための構造化された手法で、回答者が回答する一連の質問（書面または口頭）から構成される。質問紙調査は、研究者と回答者との間のコミュニケーションとして捉えられる。 オンラインでの通販サイト（例、Amazon）やプラットフォームサービス（例、Google）が大規模化しデータ収集・分析能力が向上したことによって、現代のマーケティングでは（購買、クリック、検索などの）行動を捉えたビッグデータ分析が盛んになっている。一方でGoogle社が運営するYoutubeでの動画視聴中にアンケートの回答を求められた経験のある方も多いのではないだろうか。データ収集・分析能力に長けるGoogleがなぜわざわざアンケートという古典的な方法を用いるのだろうか。この点について本資料では、アンケート調査には得意な領域があるのだろうと考える。企業のマーケティング施策とその成果のプロセスについて、以下の図のように簡易的に示す。 マーケティングと成果との関係、その過程 企業は自社の戦略・目標に基づいて具体的なマーケティング・ミックスを策定し顧客へ価値を提供する。客観的データとしてはその提供物（品質・価格、広告など）に対する反応（購買、買い物かごへの選択など）として観察できる。しかしながら、どのように/なぜその行動に至ったのかという消費者の内的な心理的プロセスについては観察できない。たとえばYoutubeにおいて提示される広告動画をどのアカウントが何秒視聴し、その後リンクをクリックしたか否かは行動として確認できる。しかし、その広告によってどの程度の視聴者がその企業を認知または記憶したのか、もしくは好意的な態度を形成したのかという心理的過程における行動への先行要因については観察できない。そのため、Youtube内において、「以下のリストのうち知っている企業は…」というようなアンケートが行われるのだと考えられる。言い換えるとアンケート調査は上図内顧客の過程における点線部分に関する調査を得意とするのだと考えられる。一方で、「企業による施策が消費者の行動的反応や企業成果へ与える直接的な効果を判別したい」というような目的の研究では、アンケート調査は適切ではないと考えられる。自身の着目する研究課題に適したリサーチ方法を採用する必要がある。 なお、観察可能な行動に基づく調査・分析方法も点線で示されているプロセスを無視しているわけではない。経済学的なアプローチでは、個人の意思決定に関する首尾一貫性についての仮定を満たすことで個人の選択行動が自身の効用を最大化するような合理的な意思決定として分析・再現・予測できるという理論を用いる。つまり、観察できない人間の内的プロセスにおいては理論的なモデル化を行うことで対応し、観察可能な刺激（提供物の属性など）と選択行動との関係についての分析を分析を行う。このようなアプローチにより消費者の効用関数や市場の需要関数を推定し、「仮に価格がxx円からyy円に変化したら個人の選択確率や市場での需要量はどのように変化するか」を予測できることも可能になる。このことからも、アンケートが一方的に優れているもしくは劣っているということではなく、研究者自身が持つ理論的な視点や研究課題に応じて適切な調査・分析アプローチを採用することが求められる。 質問紙調査では、調査者が求める情報を、回答者が回答できる一連の具体的な質問に変換することを目的とする。調査の実施において研究者は、回答者が相互コミュニケーションに参加し、協力し、面接を完了するように意欲を高め、動機を与え、奨励すると同時に、回答誤差を最小化する必要がある。研究者はこの双方向的なコミュニケーションにおいて、下の図のようなノイズが生じることを理解し、それを削減する努力を講じることが求められる。 アンケートノイズ アンケートによるデータ収集は、社会科学における「人を対象にした研究」に該当するため、人を対象にした研究としての研究倫理規則を守る必要がある。例えば、一橋大学における研究倫理規則は以下のリンクから確認できる（https://www.hit-u.ac.jp/academic/research_ethics/index.html）。ここでは、主に調査協力者の身体的・精神的負担を最小化し、彼らに不利益が生じないよう留意する必要がある。また、データを収集する際には、研究対象者から同意（インフォームド・コンセント）を得る必要がある。インフォームド・コンセントにおいては、、調査を行う主体、目的、調査により得た情報の管理方法、公開有無や方法について明示し、この条件について回答者から許諾を得る必要がある。ただし、大規模な無記名アンケート調査など、研究者が、回答者から個人名を記載した形の同意書を得ることが困難な場合がある。その場合は慣習として、調査開始前に上記の点について説明し、調査回答へ進むことで同意を得たことと代替する事が多い。ただし、このような対応方法に関する適切性の議論も、時代の経過とともに変化することが考えられるため、研究倫理については研究者自身が継続的に学習し、適切な手法を採用していくことが求められる。以下は、実際に田頭が使用したアンケートにおけるカバーレターであり、インフォームド・コンセント文の例である。 インフォームド・コンセント例 アンケートのデザインは主に以下のプロセスを経る。 質問トピックを決める 質問と回答方式を決める ワーディングを決める 質問の順番を決める アンケートのレイアウトや装丁を決める プレ調査の実施 アンケート（本調査）の実施 上記のステップに対し、ここではまず質問と回答方式について説明する。なお質問トピックは、研究課題に深く関わるため、本資料にてすでに議論されている内容を参照してほしい。アンケートでは、主に自由回答式質問、選択肢型質問、尺度型質問、といった方式の質問が用いられる。自由回答式質問は、回答者が自身の言葉で自由に回答できる質問であり、回答に関する事前の選択肢は設定されていないタイプの質問である。回答者は、このような質問に対して一言で簡潔に回答するか、詳細に長く回答するかなど、回答方法に裁量を持つことになる。例えば、「出身はどちらですか？」という問いに対して回答者は、「東京都」と都道府県レベルで答えることも、「東京都町田市」と市区町村レベルで答えることも可能である。また、典型的な自由回答式質問の例として、理由や動機を問う設問がある。例えば「なぜ、この航空会社を選びましたか？」といった質問に対して回答者は「機内食」のように一言で回答することも、長い文章を用いて回答することも可能である。選択肢型質問は、回答者に質問に対する回答のための2つ以上の選択肢を与え、その選択肢から択一させるものである。例えば、回答者の特定のサービスの利用経験について「1. はい、2. いいえ」からいずれかを選ばせたり、学歴について複数の選択肢から一つを選ばせるというものがこのタイプの質問に該当する。 一方で尺度型質問は、消費者による企業への態度や満足などを捉えるためにマーケティング分野で広く用いられている質問方式である。ここでは、ある質問に対する点数を回答者に選ばせることで、その質問で捉えようとしている性向の程度を測るものである。尺度の形成方法としては、グラフィックを用いるもの、項目別に評価するものなどが存在するがここでは、マーケティング領域で広く用いられている項目別評価尺度について説明する。項目別評価尺度においてよく用いられる尺度はリッカート尺度とSemantic differential (SD) 法と呼ばれる尺度である。リッカート尺度では、調査者は回答者に文章を提示し、回答者がその文章の内容にどの程度同意するかについて情報を得る。設問では、「全くそう思わない,…,とてもそう思う」や「全くあてはまらない,…,とても良くあてはまる」という選択肢について5点尺度や7点尺度で回答させることが多い。以下は、7点のリッカート尺度による質問の例である。 リッカート尺度例 一方でSD法は、両極の意味を持つ一連の尺度を用いて、回答者の評価を得る方法である。例えば、「冷たい-温かい」や、「弱い-強い」というような対極にある言葉を両極に設定して、5点や7点尺度によって回答を得る方法が一般的である。以下は、7点のSD法による質問の例である。 SD尺度例 マーケティングにおける研究では、アンケートを通じて、顧客満足や企業態度などの抽象度の高い構成概念を捉えようと試みることも多い（南・小野, 2010）。その場合、研究者は複数の質問項目を用いてひとつの概念を捉えるが、このような項目の扱いには、専門的な知識や先行研究による知見が必要になる。例えば、2010年代以降よく用いられる概念に、消費者によるブランドエンゲージメントがある。ブランドエンゲージメントは「消費者が特定のブランドをどの程度自分自身の一部のように大事に捉えているか」を表す概念である (Sprott et al., 2009, p. 92)。このような抽象的な概念は多面的な視点から包括的に捉えるが多く、例えば Sprott et al. (2009) では、以下の8つの質問項目を用いて、ブランドエンゲージメントを捉えていた。 I have a special bond with this brand. 私はこのブランドと特別なつながりを持っている。 I consider this brand to be a part of myself. 私はこのブランドを自分の一部として考える。 I often feel a personal connection between this brand and myself. 私はこのブランドと自分自身の間に個人的な関係を感じる。 Part of me is defined by this brand in my life. 私の人生の一部はこのブランドによって規定されている。 I feel as if I have a close personal connection with this brand I most prefer. 私が最も好きなこのブランドとは、まるで個人的な関係を持っているかのように感じる。 I can identify with this brand in my life. 私の人生において、このブランドに共感できる。 There are links between this brand and how I view myself. このブランドと私自身との間には繋がりがある。 This brand is an important indication of who I am. このブランドは、私がどういう人間かを示す重要なものである。 このことから、研究者は、包括的かつ多面的な概念を、より具体的な複数の質問を使って様々な側面から捉えようとしていることが伺える。このような構成概念を用いた分析では、概念を構成する項目の合計値や算術平均を用いて、ひとつ合成変数を作成することでその後の分析に用いる場合や、共分散構造分析（Covariance-based Structural Equation Modeling: CB-SEM）という高度な手法を用いることがある。しかし、本書では共分散構造分析は扱わないため、別のテキスト等を参照してほしい。また、複数項目によって構成概念を捉える場合、それらの項目が十分に似ているか（共通の概念に寄与しているか）をチェックする必要がある。そのようなチェックにおいては、探索的因子分析や確認的因子分析が用いられる。探索的因子分析については、本書の後半で扱う。一方で、確認的因子分析は、通常共分散構造分析の一種として扱われることが多い手法なので本書では扱わない。 "],["質問におけるワーディング.html", "4.3 質問におけるワーディング", " 4.3 質問におけるワーディング 本節では、アンケート作成におけるより実務的な注意点を説明する。具体的には、ここではアンケート内で使用する言葉やフレーズについての注意点を説明する。これは、データ収集前に吟味し、修正する必要がある。研究者がアンケート内で用いる言葉や表現の複雑さは、調査のトピックや対象となる回答者特性に合わせて調整すべきである。研究者はよく犯すアンケートにおける言い回し（ワーディング）誤りとして、以下のものが挙げられる。 曖昧な質問 ダブルバレル質問 誘導的な表現 曖昧な質問は、質問文の解釈が一意に定まらないような文章による質問を指す。このような曖昧かつ多義的な質問をしてしまうと、回答者と調査者が質問に対して異なる意味を見出してしまうことで、不適切かつ予想外の回答を得る可能性が高まる。例えば、「あなたはいつケーキを買いますか？」という質問を考える。この質問で問われている「いつ」が曖昧であるといえる。回答者はこの質問に対して、一年間のうち特定の月やタイミングを答えるべきなのか、次にいつ（例えば、2週間後等）ケーキを買うと答えるべきなのかが曖昧である。この場合、仮に研究者の意図が後者であったとしても、適切な回答を得ることができないかもしれない。 ダブルバレル質問は一つの質問の中に2つの論点が含まれているような質問を指す。このような質問の場合、回答の含意が一意に定まらず、回答に対する適切な解釈が提示できない。例えば、ホテルにおいてリピート客に対してサービスを評価してもらうようなアンケートを考える。そこで、ホテルの調査担当者以下のような質問と回答選択肢を 「当ホテルの食事や接客サービスにおいて、以前利用した際と比べて何か改善は見られましたか？」 「(1) はい (2) いいえ (3) わからない」 この質問の問題点を明確にするために、ある回答者が「 (2) いいえ」と回答した場合を考える。この回答の含意には、以下の3つの可能性を見出すことができる。 食事とサービスどちらにも改善がない。 食事に改善があってもサービスにはない。 サービスに改善があっても食事にはない。 しかしながら、(2) いいえ、という回答が上記のどの理由によって提示されたのかは識別できない。また、このような質問は、回答者を混乱されることにもつながるため、回答にかかる精神的労力を高めるという点からも好ましくない。この場合、「食事に関する改善」と「接客サービスに関する改善」とを別々の質問として問うことが好ましい。 誘導的な質問は、回答者を特定の答えに誘導する傾向のある質問を指す。例えば、以下のような消費者による小売業態への評価や利用に関するアンケートを実施することを考える。 「ドン・キホーテのような安価なディスカウントストアをどの程度利用しますか？」 上記の質問は、質問文による誘導のリスクを含む。「安価」や「ディスカウント」という言葉を強調しており、回答者が頻繁に行くと答えにくいと感じてしまう可能性がある。また、特定の企業を想起させることで、特定の企業に対する評価を誘導してしまう可能性がある点にも注意が必要である。 誘導的な質問は質問文と回答選択肢の組み合わせによっても生じる可能性があるため注意が必要である。例えば、ある政策に対する評価を確認するため、以下のような質問と回答選択肢を考える。 あなたの〇〇政策を支持していますか？ 「(1) はい (2) いいえ (3) わからない」 この質問は、選択肢による誘導のリスクを含む。その問題点を理解するために、まず「(2) いいえ」という回答を得た場合を考える。「いいえ」という回答では、その回答者がこの政策に対して積極的に不支持なのか、それとも積極的に支持しているわけではないのかがわからない。つまり、「(1) はい」には、積極している支持している層しか観察されず、いいえに回答が集まりやすい設計になっていることがうかがえる。このような場合、例えば「賛成-反対」を両極とするSD法によって回答を得ることで、回答者が当該政策に対してどのような立場に立っているのかがわかりやすくなる。 暗黙の前提を含む質問は、研究者と回答者とで異なることを想定し、適切な回答を得ることができない可能性を高める。例えば、研究者が一般消費者の固定電話の利用頻度を知りたいと考えている状況を仮定し、そのために「あなたの電話の利用頻度について教えてください」と質問したとする。この質問における「電話」とは何を想定しているのだろうか。質問において聞かれているのは自宅等に置かれている固定電話、もしくは携帯電話の利用頻度なのかが不明確である。これは、「曖昧な質問」にも通じる問題も含まれるが、研究者が電話という言葉に対して暗黙的に固定電話を仮定していることが原因で生じた問題だと理解できる。このような問題を避けるために、例えば質問の前に固定電話についての説明やフィルター質問（事前質問）を提示することで、回答者が同じ対象（固定電話）を想定できるように調査プロセスを設計することも有効である。 本書は、実際にアンケートを実施する際にはまず先行研究を調べ参照することを強く勧める。マーケティングに関する多くの尺度はすでに対応する質問文が開発されている。それらのほとんどは信頼性や妥当性の分析をクリアした質問内容なので、それらを引用するのが一番確実である。平たく言えば、とにかくまずは先行研究を探すべきだと言える。先行研究については主に、国際的な査読誌（海外ジャーナル）と、国内の文献とに分けることができる。海外ジャーナルに掲載された論文は、Google scholarや図書館システムから検索しアクセスすることが可能である。こちらの方がそもそもの論文量も多く、また、競争率および査読の水準の高いジャーナルの審査を乗り越えたという点から質の高い論文も多い。ただし、これらで参照できる論文及び尺度は英語で書かれているため、日本語で尺度を引用する場合には、日本語への翻訳と、バックトランスレーションによる日本語訳の適切性チェックが必要になる。一方で国内文献はCiniiや図書館システムから検索及びアクセスすることが可能である。こちらの方が相対的に量は少ないが、すでに適切な翻訳プロセスや、信頼性・妥当性チェックを経た尺度を提示しているものもあり、そのような尺度の場合、追加的努力を節約する形で既存の尺度を引用できる。 "],["アンケートデザイン.html", "4.4 アンケートデザイン", " 4.4 アンケートデザイン 前節では個別の質問項目設計に関する注意点を紹介したが、ここでは複数の質問やアンケート全体の構成についての説明を行う。アンケートで用いる各質問項目を確定したら、それをどのような順番で構成するのかについて考えることが必要になる。質問の掲載順においては、できるだけ回答者の回答への心理的負担を下げるような工夫を考慮する必要がある。そのため、基本的には答えやすく単純かつ興味を引く質問を最初に尋ね、広範な質問から特定的な質問へ移行し、難しい質問やセンシティブないしプライベートな情報を問う質問は最後に聞く、というような工夫が求められる。特に、アンケートの最初の質問には興味深く簡潔で威圧的でないものを選ぶと良い。例えば、回答者の素朴な意見や感想を聞く質問が有効であり、仮に調査には不必要であってもこの手の質問を最初に聞くことが有効になる場合もある。また、特定のトピックや製品・ブランドに関する項目はまとめて質問したり、時系列に関する質問は時系列順に問うなどの論理的な順番を守る構成も回答者の認知・心理的負担を減らすことに貢献すると考えられる。 アンケートの構成の他にも、レイアウトや回答の回収方法についても決定する必要がある。アンケート全体のフォーマット、文字スペースや質問文の配置などの装丁は、回答者にとっての可読性および回答率に影響を与えると考えられる。基本的にはシンプルかつ読みやすいレイアウトを心がけてデザインすることが必要になる。また、アンケート自体をいくつかのパートに分けつつ、各質問に番号を割り振るなどの工夫をすることで、回答者がアンケート回答の進捗を把握でき、途中離脱を防ぐことができるかもしれない。 次に回収方法は、大きく分けてオフラインとオンラインによる回収に区別することができる。オフラインでの回答回収には、対面質問（家庭訪問や商業施設での接触設問）、電話、郵送といった方法が存在する。対面法では、回答者と調査員とのやり取りが可能になるため、回答回収率が高かったり、質問に関する理解を促すような即時的なコミュニケーションが可能になるという利点もあるが、一回答あたりの費用が高いことや調査員の介在によるバイアスといった欠点もある。電話による調査は広範囲への調査と比較的高い回収率につながる方法として利用されたが、近年では用いられなくなっている。郵送法は、回答者に印刷された質問票と返信用封筒（切手添付または料金受取人払手続き済み）を送付し、回答後に調査者の住所に回答結果を返送してもらうという方法である。この方法であれば、回答者が好きなタイミングで回答でき、かつ広範囲への調査が可能になる。しかしこの方法には、回答状況を統制できない、回収率が相対的に低いといった欠点も存在する。 一方でオンラインでの回答回収は、eメールによってアンケートを送付する方法と、回答者にアンケートサイトにアクセスしてもらい、回答させる方法とに大別できるが、近年ではアンケートサイトにアクセスを促し回答を回収する方法が主流である。オンラインでの調査の場合、回答者はインターネット環境が整っていれば、いつでもどこでも回答することができる。また、調査者はページの見出し、セクション紹介と進捗バーを組み合わせることで回答者が回答をやめないように工夫する事ができる。加えて、調査者はオンラインであれば、画像、音声、映像やアニメーションなどの要素を含めることが可能である。一方で、調査者はアンケートに関わるプライバシーポリシーについて回答者に説明することが必要になる。ただし、オンラインでのアンケートでは、オンライン調査に参加するようなタイプの回答者からの回答しか収集できないという、回答者の傾向についてのバイアスについても理解する必要がある。 以上のような点に気をつけつつ、実際の調査項目やデザインを決定するのだが、アンケートを通じたデータ収集とそれを用いた分析においてよく観察される失敗とその対策について説明する。ここでは、研究者がアンケートにより得た情報に基づき、 ある二つの変数間の関係を捉える場合を考える。例えば、あなたが「顧客の知覚サービス品質と満足度との間には正の関係ある。」という理論に関心があったとする。ここで着目されている変数（概念）はサービス品質と満足度であり、財務データでは観察不可であるため、一般的に研究者はアンケートを通じた調査が用いることで情報を得る。この2変数間の関係を捉えるために研究者はアンケートをデザインする必要があるのだが、「一つの質問でまとめてこの関係を捉えようとする」、という誤ったアプローチを採用することがよく観察される。例えば、初めてアンケートを実施する学生は「あなたは、品質が高いサービスを経験すると満足しますか？」のような質問項目を設定しがちである4。 このような質問項目は、この項目に対する回答を基にどのように二変数間の関係を検証するのかが明確ではない（検証できない）という点で問題がある。複数の変数間の関係を捉えたい場合、各変数ごとの質問を別々に作成し、それぞれの質問への回答データを用いて二変数間の関係を統計的に分析するというプロセスを経る。この点については、財務データに基づく二変数間関係の分析と対比させるとわかりやすい。例えば、研究者が小売企業の店舗数と売上高の関係に関心があるとする。このとき研究者は企業レベルの店舗数と売上高についてそれぞれ別の変数として情報を集め、これらの変数間の関係を回帰分析などの手法で分析しようと試みるだろう。おそらく、多くの人が、店舗数と売上高の関係を分析するために、これら両方の情報を内包した一つの変数に関する情報を収集しようとは考えないはずである。アンケートによる調査・分析においても原則としては同様であり、特定の変数を排他的に捉えるような質問項目を作成し、それらに対して得た回答を基に、分析を行っていく必要がある。 変数間の関係を捉えるための調査設計について、もう少し具体的な説明を例とともに提示する。ここではあなたが「具体的な小売店舗の属性（価格）とその店舗へのロイヤルティとの関係」に関心があると仮定する。このとき、先述の悪い質問に該当する質問例は「あなたは、価格の低いお店を利用しますか？」や「あなたは、価格の低いお店をどの程度利用しますか？」である。これらの質問は、価格とロイヤルティの関係を検証できるデザインになっていない。では、具体的な店舗の特徴とその店舗への評価や利用状況をアンケートによって捉えるためにはどのようなデザインが考えられるのだろうか。本書では、以下の2つの対応例を紹介する。 回答者が利用している店舗を特定し、その店舗について回答してもらう。 研究者が準備したシナリオや実験刺激としての店舗情報などを提示して、その店舗について回答してもらう。 1の方法の場合、アンケートにおいて回答者が想定する企業は回答者ごとに異質である。この方法では、回答者が頻繁に利用する店舗について特定化するような質問をしたあとに、その店舗についての評価や利用状況を尋ねるという階層的な質問構造を形成する。例えば、以下のような質問を構成することが考えられる。 「Q1 あなたは過去3か月間に次のどのスーパーの店舗に、最も良く食料品を買いに行きましたか。（回答は1つ）食料品を買うスーパーについて伺います。:リストを提示」 「Q2 あなたが食料品を買う際に最もよく利用する店舗【Q1スーパー名引用】でのお買い物の状況について、お答えください。週間の間にあなたがその店舗で食料品の買い物をする頻度をお答えください：1. １回未満, 2. 1回, 3. 2回,…」 「Q3 最もよく利用する店舗【Q1スーパー名引用】での食料品購入額が、ご家庭の食料品購入額全体の何％を占めているか、それぞれお答えください。: 選択肢を提示」 「Q4 最もよく利用する【Q1スーパー名引用】の店舗は従業員のサービスが手厚い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 「Q5 最もよく利用する【Q1スーパー名引用】は取扱製品の品質が高い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 一方で、2. のシナリオを提示する方法は、シナリオ実験アプローチと言われ、基本的な調査の構造は投薬の実験などと同様であり、ある刺激を受ける群と受けない群とでその後の結果に差があるかを捉える。このアプローチの場合、回答者が想定する店舗やその他の状況は特定化され、コントロールされている。この方法では、とある購買状況を想定してもらうための、全ての回答者に共通したシナリオを想定しつつ、検証したい施策のみ変化させた（施策あり vs. 施策なし）２種類のシナリオを準備する。そして研究者は、回答者を検証したい施策ありのシナリオを読むグループ（トリートメント群）と施策なしのシナリオを読むグループ（コントロール群）とにランダムでわけ、それぞれのグループ間で、回答者が異なる情報に直面するように調査を設計する。回答者はシナリオ読了後、成果変数に相当する質問に回答する。そして、成果変数に関するトリートメント群とコントロール群間での差を統計的に分析する、というアプローチを取る。ここでは、例として企業の活動に関するシナリオを読ませるという方法を紹介したが、この方法はシナリオに限らず、何かを実際に体験させたり、回答者にタスクを課すなど、様々な調査設計に応用する事が可能になる。 具体的なアンケートデザインが固まると、研究者は次に実際にデータを収集する段階に移る。ここでは第1に母集団を定め、次に、サンプルについて決定する必要がある。母集団は、研究者が求める情報を持つ要素や物の集合体と考えられ、研究課題に応じて研究者によって決定される。ここでは、研究者が求める情報を有しているのはどのような人たちかという母集団の要素（日本の一般消費者か、東京都内の国立大学の学部生か、等）や、適切な母集団の単位（個人、家計など）はなにかについて定義する必要がある。 母集団という集合体全体を捉えることは通常困難であるため、研究者は母集団に対応するアクセス可能な標本（サンプル）の情報を得る。サンプルに対するデータ収集プロセスでは、サンプリングフレーム、サンプリングテクニック、サンプルサイズを決定する必要がある。サンプリングフレームとは、対象となる母集団の要素を表現したものであり、対象となる母集団を特定するためのリストによって構成される。例えば、電話帳や調査会社から購入した個人や組織のリストがサンプリングフレームの例である。次に、サンプリングテクニックは、サンプルフレームから特定のサンプルをピックアップする方法である。サンプリングテクニックは非確率的サンプリングと確率的サンプリングに大別できる。比較率的サンプリングは、サンプルをランダムな選択により抽出しない方法であり、コンビニエンスサンプリングとスノーボールサンプリングがその典型例である。コンビニエンスサンプリングは、研究者にとって便利な要素のサンプルを集める方法であり、適切なタイミングで適切な場所にいたという理由で回答者が選ばれる。たとえば、学生やある組織の構成員を使った調査や、ショッピングモールでのインターセプトインタビューはその典型的な例である。次に、スノーボールサンプリングは、最初の回答者をランダムで選んだ後、その後の回答者は、最初に選ばれた人による紹介や情報提供によって選ぶ方法である。これらの方法は最もお金も時間もかからないという利点を持つが、サンプルセレクションバイアスに注意することが必要である。一方で確率的サンプリングは研究者による恣意性を排除した抽出方である。この代表例がランダムサンプルである。ランダムサンプルでは、サンプリングフレームからランダムな手順でサンプルを抽出する方法であり、すべての回答者は他の回答者とは独立して選択される。これにより、サンプルは、互いに独立で同一の確率分布に従う。また、システマティックサンプリングと呼ばれるサンプリングフレームからの開始点がランダムで決定され、そこから任意の i 番目の要素がピックアップされる方法も存在する。サンプルサイズは、抽出する標本の数を決定する。サンプルサイズの決定においては、予算の都合、回答者属性に基づく割付、慣習等の歴史的経緯、などの非統計的要因が影響する場合もある。一方で、統計的な要因としては、分析結果において想定される効果量を所与とし、確率的な計算に基づいて適切なサンプルサイズを決定する方法も存在するが、本書で詳しい内容は扱わない。基本的な方針としては、サンプルサイズが多いほど精度が高くなるという前提のもと、必要な精度を達成するために十分なサンプルサイズを抽出するというものになる（池尾等, 2010）。 テキストにこのように書かれていると、「自分はこんなことをしない」と思うだろうが、実際にアンケート設計をさせるとこのような誤りを犯す学生は存外多いので注意されたい↩︎ "],["参考文献-2.html", "4.5 参考文献", " 4.5 参考文献 池尾恭一・青木幸弘・南知惠子・井上哲浩（2010）「マーケティング」, 有斐閣. 田村正紀 (2006) 「リサーチ・デザイン: 経営知識創造の基本技術」, 白桃書房. 南知惠子・小野孔輔 (2010) 『日本版顧客満足度指数（JCSI）のモデル開発とその理論的な基礎』, 「マーケティングジャーナル」,30(1), 4-19. Malhotra, N. (2019) Marketing Research: An Applied Orientation, Pearson Education Limited. Sprott, D., Czellar, S., &amp; Spangenberg, E. (2009). The importance of a general measure of brand engagement on market behavior: Development and validation of a scale. Journal of Marketing Research, 46(1), 92–104. "],["handling.html", "Chapter 5 データ処理と顧客関係管理（CRM）", " Chapter 5 データ処理と顧客関係管理（CRM） これまでの本書の内容は、データを収集するまでの注意点や方法を説明した。しかしながら、収集したデータをただ眺めているだけでは、定量的な知見を得ることはできない。そのため、以降の節では主にデータ処理や分析手法について説明する。まず我々は、データセットの構築から学ぶ。例えばあなたがアンケートを実施したならば、そのアンケートからデータセットを構築する努力が必要になる。アンケート結果に基づくデータセット構築において研究者はコーディング、トランスクライビング、データクリーニングのプロセスを経る。 コーディングは、回答を分析可能なフォーマットへ変換する作業であり、通常回答に対して数値を当てはめる作業を伴う。例えば、回答者が男性ならば 1 を、女性ならば 0 をとるようなダミー変数を作成する作業がこれに当てはまる。コーディングは、不必要な情報を減らすことでデータ化プロセスを担う。トランスクライビングは、質問紙に記載された回答をデータ入力していく作業である。入力に関するヒューマンエラーは起こるものとして考える必要があるため、通常このプロセスは二人一組でダブルチェックをしながら行う。なお、オンラインアンケートの際はこのプロセスは自動で行われるため、不要になる。データクリーニングでは、研究者は不適切な回答のチェックを行う。例えば、回答可能範囲から外れた回答（例、7点尺度における8点回答）や、論理的に非整合的な回答（例、回答者が利用したことないと答えているサービスについて評価している場合） がないかをチェックする。また、欠損値という回答がない観測についてもチェックする必要がある。マーケティング分野においては欠損値のあるサンプルを削除するという方法も用いられるが、欠損値の扱いは奥が深く、いくつかの対応法がある。本書ではその詳細については扱わないが、欠損値に対応するためのデータ処理についての専門書も存在するため、関心のある読者はそれを参照してほしい（高橋・渡辺, 2017）。 データセットの構築が完了したあとは、本書では基本的にRを通じて様々な作業を行う。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、実際に我々分析者が各コマンドもシンプルになる。関数の利用においてはf(argument)のように関数名fのあとにカッコをつけて表記する。なお、argumentは日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。また、我々は通常、パッケージをインストール・起動することで他者が作った関数を利用することが多い。関数とパッケージについての説明や実行例は「Rの基本操作」節で紹介しているのでそちらを参照してほしい。 本章では、Rを用いてデータを整理・処理することで、複雑な統計分析を行わなくてもマーケティング的知見を得られることを、顧客関係管理を例に紹介する。ここで学ぶRでの作業は主に以下の通りである。なお、これらの作業は、統計的な分析を実行する前のデータ前処理にも使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。そのため、顧客関係管理に興味関心のない読者も、以下の手法について学習することを勧める。 データの読み込み（csv, excel, etc.） dplyrの利用とデータ整形 パイプ演算子を用いた複数処理の実行 Wide vs. Long data format (おまけ) "],["データの読み込み.html", "5.1 データの読み込み", " 5.1 データの読み込み 本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動すること。 install.packages(c(&quot;tidyverse&quot;,&quot;readr&quot;,&quot;readxl&quot;)) library(tidyverse) library(readr) library(readxl) ここからは、データセットを用いた分析を行う。基本的な操作においては、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。具体的には、エクセルファイルが誰にでも開けるとは思ってはいけない。そのため、ソフト特性に依存しない汎用的な形式を使うことが好ましい。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない ’mktData.csv’というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。 df1 &lt;- readr::read_csv(&quot;mktData.csv&quot;) df2 &lt;- readr::read_csv(&quot;mktData.csv&quot;, col_name = FALSE) なお、デスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、「（補足）デスクトップ版の利用とプロジェクト機能」節で紹介している「プロジェクト機能」必ずを活用するようにほしい。 本講義では、大学の学務ポータル（manaba）を通じて教員が配布したデータを学生各自のコンピュータにダウンロードし、それを post.cloudに各自がアップロードするという手順によって分析用データを利用する。Manabaからのデータのダウンロードは各自で済ませてほしい。Posit.cloudはR studio 画面を表示する段階でプロジェクトが作成される。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する&lt;デスクトップ版を使用している場合も、Project を指定していれば、以下のコードで全く同じ結果を得ることができる。&gt; 具体的は、以下の通りdir.create() を使って新たに data というディレクトリ（フォルダ）を作成する。 dir.create(&quot;data&quot;) 新たなディレクトリを作成したら、そこに、ポータルよりダウンロードしたデータを入れてほしい。ここではまず “2022idpos.csv”というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに “idpos” というオブジェクト名を定義する。なお、ここで分析実行社はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) 問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によると、このidposデータは、3000行、5列のデータセットであることがわかる。なお、同様の情報は R studio 画面内の Environment タブから確認することできる。 head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。 names(idpos) ## [1] &quot;id&quot; &quot;date&quot; &quot;spent&quot; &quot;coupon&quot; glimpse(idpos) ## Rows: 3,000 ## Columns: 4 ## $ id &lt;dbl&gt; 12, 32, 30, 29, 46, 44, 44, 32, 3, 34, 36, 3, 42, 18, 38, 4, 19… ## $ date &lt;chr&gt; &quot;2019/9/25&quot;, &quot;2019/9/10&quot;, &quot;2019/9/9&quot;, &quot;2019/9/4&quot;, &quot;2019/9/10&quot;, … ## $ spent &lt;dbl&gt; 14326, 10232, 6881, 6365, 7595, 7858, 9405, 1821, 8375, 1828, 6… ## $ coupon &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, … なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定し作成した、簡易的な人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。 "],["データの整形.html", "5.2 データの整形", " 5.2 データの整形 データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。しかしながら、tidyverseのインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。 summarize() mutate() filter() select() arrange() パイプ演算子 %&gt;% summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。 summarize(data, M = mean(var_name)) mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、dataというデータセットに対し、definition で定義した変数をnew_varとして追加するには、以下のコマンドを用いる（以下は見本コードである）。実際にdefinitionを定義する場合には、様々な関数や論理式を利用する必要がある。例えば、“new_var = var1/100” という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、“new_var = var1 – mean(var1)”という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。 mutate(data, new_var = definition) mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いるのが良い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には “==” （同値）, “&gt;=”（以上）, “&lt;=”（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。 mutate(data, new_var = ifelse(var1 == 2, 1, 0)) filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == “male”）のサンプル情報のみ抽出したい場合には以下のような指示になる。 filter(data, gender == &quot;male&quot;) なお、特定の条件以外のものを指定したいときは、 という論理式 “!=” (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。 filter(data, gender != &quot;male&quot;) select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。 data2&lt;- select(data, var1, var2, var3) 反対に、取り除きたい変数を指定するときには、以下のように “-” を使う。 data2&lt;- select(data, -var1) 列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。 data2&lt;- select(data, var1:var5) また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、“v” という文字から始まる変数を取り出すための指示は、いかのようになる。 data3&lt;- select(data, starts_with(&quot;v&quot;)) arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある。 data2 &lt;- arrange(data, var1) data2 &lt;- arrange(data, desc(var1)) また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。 data3 &lt;- rename(data2, var1 = sales) Tidyverse 内の dplyr を使うことでパイプ演算子（%&gt;%）が使える（ショートカット: Command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず \\(\\small 10-6\\) が計算され、その結果である “4” が sqrt() の引数として利用される（sqrt(4) は 2）。 (10-6) %&gt;% sqrt() ## [1] 2 パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。 男性の情報だけ抜き出す(filter) Var1の値について降順に並べ替える(arrange) 第一位から最下位までの順位を割り当てた ranking 変数を作る(mutate) var1 , var2, var3, var4, orderだけ残し(select) new_dataとして定義する 上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。 new_data &lt;- select(  mutate(   arrange(    filter(data, gender == &quot;male&quot;),    desc(var1)),    ranking = 1:n()),   var1, var2, var3, var4, order) パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。 new_data &lt;- data %&gt;% filter(gender == &quot;male&quot;)%&gt;% arrange(desc(var1)) %&gt;% mutate(ranking = 1:n()) %&gt;% select(var1, var2, var3, var4, order) パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に “.” （ドット）を使う。たとえば、\\(\\small 10-2\\)の計算結果を用いて2から8の偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。 (10-2) %&gt;% seq(from = 2, to = ., by = 2) ## [1] 2 4 6 8 データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、“#Rds” 以降のコードを用いる。 readr::write_csv(df, path = &quot;data/new_data.csv&quot;) #Rds readr::write_rds(df, path = &quot;data/new_data.Rds&quot;) "],["統計的検定を用いない顧客分析.html", "5.3 統計的検定を用いない顧客分析", " 5.3 統計的検定を用いない顧客分析 5.3.1 データの構造変化とソート 顧客の購買データを用いて、（統計的な分析を要さず）重要顧客や顧客層を発見することが、小売企業を中心に行われている。ここでは基本的に、ID-POSデータを用いたデータベースの正規化と集計2焦点を合わせる。特に、顧客個人に関する情報を用いながら企業や店舗にとって重要な顧客を特定し、その顧客との関係性を深めた場合を考える。店舗の運営効率から考えると、単に来店客数を増やすだけでなく、より頻繁に、より高額の買い物をする顧客を特定し、その人（達）の購買を促進することが効果的になる。言い換えると、企業や店舗は、ロイヤルカスタマーを特定し、その顧客との関係性を構築したいと考えるのである。そのためにはまず、ロイヤルカスタマーを特定する作業が必要になる。そこで本節では、データから企業にとって価値のある顧客を発見する方法について、データの前処理技術を応用する形で紹介する。 本節では簡単に、単純なデータハンドリングから顧客インサイトを得る方法を考える。特に、データ処理とソーティング（順番の入れ替え）を用いる方法を用いる。本節ではID-POSデータを用いた分析として、デシル分析とRFM分析を紹介する。デシル分析は、支出額をもとに上位から顧客を並べ替え、その順番に基づき顧客を10分割することで、上位の支出額を担うランクに属する顧客を特定する。なお、他の指標で同様の分析を実行することも可能だが、一般的には支出額を用いることが多い。例えば、月当たり5000人の顧客がいるとすると、500人ずつのグループに分け、購買額の大きい順にデシル1〜10 (10〜1の場合もある) とする形でランク分けする方法がこれにあたる。このとき、各顧客の情報がポイントカードやアプリで紐付いているのであれば、最も購買額の多いグループの特徴を整理することで、現在購入額の高い顧客がどんな特徴を持つのか理解できる。 一方、RFM（Recency, Frequency, Monetary）分析は、取引情報から、最近いつ買ったか、どれだけの頻度で買い物するか、どれだけ支出しているかといった情報を総合的に勘案し、どの顧客が最重要かを特定する方法である。これらの指標は、ロイヤルティや再購買確率が高い顧客を判別するのに役立つ3つの指標である。例えば、最終購買日から時間が経っている顧客は離反しているかもしれないし、購買頻度や購買額が高いと、ロイヤルティが高い可能性が高い。また、クーポンや割引利用の有無の情報と紐付けることができれば、当該顧客がチェリーピッカーか否かも判断することができる。 ID-POSデータは、各顧客の会員IDについての情報はありながらも取引ベースで情報が整理されている。このようなデータに対して以下の手順を用いてデータを集計・ソーティングする。 顧客IDごとに、各取引情報を集計する。 顧客ID情報についてまとめたデータベースにおける順番をソートし、重要顧客を識別する。 このようなデータの集計は、データ構造を取引ベースから顧客IDベースに変換することを可能にする。下図は、取引ベースのID-POSデータを、集計作業によって顧客IDベースのデータ構造に変換するイメージを示したものである。ID-POSデータは、顧客ID情報が含まれていながらも、データの行（観測）は各取引を示している。そのため、仮に同じIDの顧客がデータ収集期間に複数回取引を行っている場合、同じIDを含む観測がいくつも見られることになる。一方で下部の顧客IDベースのデータは、ID-POSデータを顧客ID情報によって集計したものであり、一定期間中に特定のIDを持つ顧客がどのような購買行動を示していたかを捉えたデータである。そのため、データの行は各顧客IDを示している。本節では、まずはじめにこのようなデータ構造の変換について説明する。 データ構造変化 ここでは、先程利用した idpos データを用いて作業を進める。改めて、当該データを以下のコマンドで確認する。 head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 ここで、date変数を用いて直近で何日前の来店かを示す変数を作成する。このデータは、2019年09月01日 から 2019年10月01日までの一ヶ月間、とある店舗で記録された取引データであると仮定し作成されている。そのため、データ収集終了最新時点（2019-10-2）と来店日時の差を表す変数を作成する (ここでの処理にエラーが出る場合は、idpos$date &lt;- as.Date(idpos$date) というコマンドを事前に試してから変数の定義を行ってほしい)。head関数により出力された結果によって新たな変数（datediff）が追加されたことがわかる。 idpos$datediff&lt;- as.numeric(difftime(&quot;2019-10-02&quot;,idpos$date,units=&quot;days&quot;)) head(idpos) ## # A tibble: 6 × 5 ## id date spent coupon datediff ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 7 ## 2 32 2019/9/10 10232 1 22 ## 3 30 2019/9/9 6881 1 23 ## 4 29 2019/9/4 6365 0 28 ## 5 46 2019/9/10 7595 1 22 ## 6 44 2019/9/14 7858 0 18 続いて、パイプ演算子を使ったデータ処理によって顧客IDベースの形へ集計する。ここでは特に、group_by() という関数を使い、顧客id (今回は性別情報も残したいので gender も加えている)をグループ化の基準と指定する形で集計を行う。また、CRM分析で使う変数のために、idレベルでの集計という形で以下の変数を作成する。そして、以下の変数を用いて集計した新たなデータセットを “idpos_cust” として定義する。 frequency：各idの出現頻度をn()でカウントする monetary：spentの合計をsum()で計算する cherry (picker)：クーポンの利用回数の合計をsum()で計算する recency：datediffの最小値をmin()で求め、直近でいつ来たかを判別 idpos_cust &lt;- idpos %&gt;% group_by(id) %&gt;% summarize(frequency = n(), monetary = sum(spent), cherry = sum(coupon), recency = min(datediff) ) head(idpos_cust) ## # A tibble: 6 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 ## 2 2 20 157976 13 2 ## 3 3 21 134673 5 1 ## 4 4 19 154416 10 4 ## 5 5 20 177156 11 3 ## 6 6 19 151853 11 3 上記の操作によって、元々のidposデータから、顧客ベースのデータ構造（idpos_cust）に変換できたはずである。しかしこれだけでは、まだ我々は誰が重要顧客か特定できない。そのため、次に我々はデータの並べかえを行う。具体的には、支出額が高い順に並び替えたあとに上位20人の顧客を表示する。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) ##Customers in the top 20 (Monetary) idpos_cust_m[1:20,] ## # A tibble: 20 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15 31 305665 13 1 ## 2 35 30 262181 13 1 ## 3 31 31 254804 19 3 ## 4 33 29 251352 14 2 ## 5 22 24 214150 9 4 ## 6 20 21 199197 11 2 ## 7 18 24 196686 13 3 ## 8 49 24 196143 12 4 ## 9 9 24 195788 14 1 ## 10 12 25 194860 15 1 ## 11 42 21 193582 5 1 ## 12 30 26 190687 17 2 ## 13 36 20 189937 13 6 ## 14 14 21 188600 14 1 ## 15 26 22 184657 10 2 ## 16 32 20 184276 12 1 ## 17 17 19 181947 13 1 ## 18 44 23 181397 12 1 ## 19 34 20 179533 8 2 ## 20 37 21 179169 11 2 5.3.2 データ結合 ここまでの結果からは購買額の高い顧客IDを特定することができた。しかしながら、これらの顧客がどのような特徴を持っているのかについては推察できない。そのため、別で管理されていた顧客情報を捉えたデータセットと結合することでこれらの顧客についての属性を把握する。 以下では、今回使用する顧客情報データセットを読み込み、その概要を示している。このデータセットには、3000人分の会員登録済み顧客情報が蓄積されており、以下の変数を含む： id: 顧客ID gender: 性別 age: 年齢 famsize: 世帯人数 id_data &lt;- readr::read_csv(&quot;data/id_data.csv&quot;, na = &quot;.&quot;) str(id_data) ## spc_tbl_ [3,000 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:3000] 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : chr [1:3000] &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;female&quot; ... ## $ age : num [1:3000] 51 38 41 24 48 46 36 30 26 57 ... ## $ famsize: num [1:3000] 2 2 1 1 5 1 1 1 3 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. famsize = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ここで、顧客データと購買データを left_join() を用いて、idpos_cust をメインとする形で id によって結合する。left_join() は左側に指定したデータフレームに存在知するキーの行を返す形でデータの結合を行う。言い換えると、左側のデータセットに存在する行（観測）はすべて残され、そこに新たな変数を加える形でデータフレーム間の結合を行う。 idpos_cust &lt;- left_join(idpos_cust,id_data, by = &quot;id&quot;) head(idpos_cust) ## # A tibble: 6 × 8 ## id frequency monetary cherry recency gender age famsize ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 female 51 2 ## 2 2 20 157976 13 2 male 38 2 ## 3 3 21 134673 5 1 female 41 1 ## 4 4 19 154416 10 4 female 24 1 ## 5 5 20 177156 11 3 female 48 5 ## 6 6 19 151853 11 3 female 46 1 上記の通り、顧客ベースの取引情報に、各顧客の属性情報が追加された事がわかる。これを利用し、以下のように上位20顧客の性別比率を以下のように確認する。これによって、主要顧客に占める性別比率が確認できる。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) #gender ratio in the top 20 table(idpos_cust_m[1:20,]$gender) ## ## female male ## 16 4 続いては先述のデシル分析を実行する。具体的には、idpos_custに対し、cut()関数を使うことで、monetaryの大きさに基づきサンプルを10等分し、新たに “decile_rank” という変数（列）をデータに追加し、その新たなデータセットを “idpos_cust_m”と定義する。なお、次節にてこのデータを改めて使うため、データをprojectのdataディレクトリ内に保存しておいてほしい。 idpos_cust_m$decile_rank &lt;- cut(idpos_cust_m$monetary, quantile(idpos_cust_m$monetary, (0:10)/10,na.rm=TRUE), label=FALSE,include.lowest=TRUE) head(idpos_cust_m) ## # A tibble: 6 × 9 ## id frequency monetary cherry recency gender age famsize decile_rank ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 15 31 305665 13 1 female 19 3 10 ## 2 35 30 262181 13 1 female 22 3 10 ## 3 31 31 254804 19 3 female 55 1 10 ## 4 33 29 251352 14 2 male 54 5 10 ## 5 22 24 214150 9 4 female 53 4 10 ## 6 20 21 199197 11 2 female 50 1 10 readr::write_csv(idpos_cust_m, &quot;data/idpos_customer.csv&quot;) head()関数によって新たな変数の追加を確認したあとは、各デシルの店舗売上への貢献度を確認する。ここでは、decile_rankをグループ化の基準として設定し、summarize() によって集計する方法を用いる。その後、各デシルの売上比率を計算し、高い順に並び替える。集計・分析の結果は、上位20%の顧客で、ID-POSに計上されている売上の57%を締めていることを示した。 decile &lt;- idpos_cust_m %&gt;% group_by(decile_rank) %&gt;% summarize(freq = n(), monetary = sum(monetary)) total &lt;- sum(decile$monetary) decile2 &lt;- decile %&gt;% mutate(percent = monetary/total*100) %&gt;% arrange(desc(decile_rank)) decile2 ## # A tibble: 10 × 4 ## decile_rank freq monetary percent ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 149 11141710 45.1 ## 2 9 149 3042467 12.3 ## 3 8 147 2220019 8.98 ## 4 7 150 1976807 8.00 ## 5 6 148 1687031 6.83 ## 6 5 149 1466609 5.93 ## 7 4 149 1213922 4.91 ## 8 3 148 939338 3.80 ## 9 2 149 670413 2.71 ## 10 1 149 358988 1.45 本節で示したように、高度な統計的分析を実行せずとも重要顧客を特定する事が可能になる。本節では特に、データの集計や処理技術を使った方法を紹介した。このような分析によって回答できる問いは「企業にとっての重要顧客は誰か」というものだろう。この問いは非常に興味深く実務的にも有意義なものであるが、以下の点に注意することが重要である。第一に、何をもって顧客の重要性を定義するかという問題である。本節では特にRFMなどの基準を用いて、観察可能な購買結果をもとに重要顧客を識別する方法を捉えた。しかしながら、例えば購買頻度と購買額では異なる側面を捉えており、どの指標を用いて分析するかによって（通常は）結果が異なる。そのため、研究者自身が「重要性」をどのように定義するのかを注意深く判断し、なおかつそれをレポートやプレゼンテーション内できちんと明示する必要がある。さらに、データでは捉えきれない側面は分析結果に反映されないという点についても注意が必要である。例えば日本には江戸時代から続く小売企業もいくつも存在する。仮に、そのような小売企業と、長期間代々取引を続けている顧客（一族や企業）がいたとして、さらにその顧客が分析による上位顧客に含まれなかったとする。その場合、この顧客を重要顧客でないと切り捨てて良いのでだろうか。災害、国家の統治体制の変化、戦争、などの激動を経てなお取引が続いている顧客は重要でないと言い切れるのだろうか。もちろん、このような顧客を重要でないと捉えることも、経営判断として間違ったものではない。しかしながら、少なくともデータを用いた分析結果を過信しすぎず、データによって何が捉えられており、何が捉えきれていないのかについて研究者・意思決定者のどちらも自覚的になることが必要になる。 第二に、分析を行うだけでマーケティング実務が完結するわけではないという点についても注意が必要である。「重要顧客を特定する」という研究課題の背後には、「CRMを実行して収益性を向上させる」という実務的課題が存在しているはずである。そのため、今回の発見物をもとに、マーケティング活動への示唆を与えていくことが重要になるのだが、誰が重要顧客か、という問いに答えるだけでは具体的な活動指針（アプリを通じた囲い込みや、訪問販売等）を与えるのは難しい。そのため、重要顧客のライフスタイルや価値観などの彼/彼女らの特徴に踏み込んだ調査を行うことも必要になるかもしれない。昨今のロイヤルティプログラム（ポイントシステム）では、モバイルアプリを通じて個人の様々な行動履歴が記録されたり、アンケートへの回答を促されたりすることがあるだろう。これらの情報とID-POSデータをうまく接合できれば、重要顧客を特定しつつ、それらの顧客に適したCRM方策を策定できるかもしれない。 "],["おまけ-wide型とlong型データセット.html", "5.4 (おまけ) Wide型とLong型データセット", " 5.4 (おまけ) Wide型とLong型データセット インターネットを通じて、とても都合の良いかつ信頼できるデータセットが入手できたとしても、それが分析のために望ましい形で保存されているとは限らない。特に、横長(wide)と縦長(long)データが存在し、それらのデータの型の違いには注意が必要である。我々人間がデータを眺め、解釈を与える場合にはwide型データのほうが扱いやすいのだが、コンピュータやソフトウェアがデータを分析する際には、long型のほうが好ましい。例えば、下図は4店舗のある年の6月から10月までの売上情報（単位：千円）を示したデータセットである。これは、複数サンプル-複数時点という構造のデータだが、各時点の観測値が横に並んでおり、wide型データだといえる。 Wide型データ 手元にあるWide型データをLong型に変換したい場合の対応策として、ここでは tidyverseに含まれているtidyrのgather()関数を用いる方法を紹介する。この関数の利用方法を実演するためにManabaにアップされている sales_wide.csv をダウンロードし、プロジェクト内のdataディレクトリに移して欲しい。データを読み込むと、以下のようにデータ構造を確認することができる。 sales_wide &lt;- readr::read_csv(&quot;data/sales_wide.csv&quot;, na = &quot;.&quot;) ## Rows: 4 Columns: 6 ## ── Column specification ──────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): store, sales06, sales07, sales08, sales09, sales10 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sales_wide ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 ここで用いる gather()関数は、以下の引数を指定する： - data: 変換元のデータ - key: 変数を１列にまとめたあと、元の列を区別するための列につける名前 - value: 変数を１列にまとめたあと、値が入る列につける名前 - どの範囲を一列にまとめるかの範囲指定 その上で、さきほどのsalesデータをLong型に変換するために、以下のようなコマンドを利用する。 sales_long&lt;- gather(data = sales_wide, key = &quot;month&quot;, value = &quot;sales&quot;, starts_with(&quot;sales&quot;)) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 sales06 1500 ## 2 2 sales06 900 ## 3 3 sales06 700 ## 4 4 sales06 1000 ## 5 1 sales07 2000 ## 6 2 sales07 1500 ## 7 3 sales07 900 ## 8 4 sales07 1300 ## 9 1 sales08 2300 ## 10 2 sales08 1600 ## 11 3 sales08 1000 ## 12 4 sales08 2000 ## 13 1 sales09 1700 ## 14 2 sales09 1600 ## 15 3 sales09 800 ## 16 4 sales09 1500 ## 17 1 sales10 1500 ## 18 2 sales10 1000 ## 19 3 sales10 650 ## 20 4 sales10 1400 編集された縦長データは上記の通り示されるが、その中身を見ると、monthの列にsales06などの情報が記載されており、好ましくない。この問題は、元のデータにおける変数名に該当する情報を新しいkey列の値に使うというgather関数の仕様に影響するものである。この問題に対しては、パイプ演算子を使ってgatherの実行前に、列名を年だけの形に変更することで対応可能である。以下が、修正版のコードであり、出力結果より、先述の問題点が解決されたことが確認できる。ただし、gather関数の実行において、下記コード内では範囲の引数の指定についても修正されていることに注意が必要である。 sales_long &lt;- sales_wide %&gt;% rename(`06` = sales06, `07` = sales07, `08` = sales08, `09` = sales09, `10` = sales10) %&gt;% gather(key = &quot;month&quot;,value = &quot;sales&quot;, `06`:`10`) %&gt;% arrange(store) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 06 1500 ## 2 1 07 2000 ## 3 1 08 2300 ## 4 1 09 1700 ## 5 1 10 1500 ## 6 2 06 900 ## 7 2 07 1500 ## 8 2 08 1600 ## 9 2 09 1600 ## 10 2 10 1000 ## 11 3 06 700 ## 12 3 07 900 ## 13 3 08 1000 ## 14 3 09 800 ## 15 3 10 650 ## 16 4 06 1000 ## 17 4 07 1300 ## 18 4 08 2000 ## 19 4 09 1500 ## 20 4 10 1400 一方で、Long型データからWide型データへ変換するためには、tidyrのspread()を用いることが多い。spread() では、主に以下の引数を用いる。 data: 変換元のデータ key: 変数を複数列にわけるとき、列を区別するための変数 value: 複数列に分ける値 例えば、以下のコードで先程の sales_long データをwide型に変換することができる。 wide_test &lt;- sales_long %&gt;% spread(key = &quot;month&quot;, value = &quot;sales&quot;) %&gt;% rename(sales06 = `06`, sales07 = `07`, sales08 = `08`, sales09 = `09`, sales10 = `10`) wide_test ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 "],["参考文献-3.html", "5.5 参考文献", " 5.5 参考文献 高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版. 松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社. "],["データの要約と可視化.html", "Chapter 6 データの要約と可視化", " Chapter 6 データの要約と可視化 本章では、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。なお、本章の作業においても tidyverse を用いるので、以下のように tidyverse を起動してほしい。 library(tidyverse) "],["記述統計.html", "6.1 記述統計", " 6.1 記述統計 記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。「データのタイプ」という節で確認したように、データには量的変数とカテゴリ（を示す質的）変数がある。量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。 その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値はおもにデータの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる5。 d &lt;- c(1, 3, 2, 5, 4) median(d) ## [1] 3 d2 &lt;- c(1, 3, 2, 5, 4, 6) median(d2) ## [1] 3.5 平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、\\(\\small x_1,x_2,...,x_n\\) に対して以下のように定義される。 \\[\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\] 観測値と平均値の差（\\(x_i - \\bar{x}\\)）は偏差と呼ばれ、偏差の和はゼロである（\\(\\sum_ix_i - \\bar{x}=0\\)）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、mean() 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。 数学: (3,3,5,5,5,5,5,7,7) 国語: (2,3,3,5,5,5,7,7,8) このときの平均値は以下のように求まる。 math &lt;- c(3,3,5,5,5,5,5,7,7) jpn &lt;- c(2,3,3,5,5,5,7,7,8) mean(math) ## [1] 5 mean(jpn) ## [1] 5 計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。 分散 (Variance, \\(S^2\\)で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。 \\[S^2 = \\frac{1}{n}\\sum_i^n (x_i-\\bar{x})^2\\] しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 (\\(\\sqrt{\\cdot}\\)) を取った値を標準偏差と呼び、この標準偏差を用いることも多い6。なお、Rでは var() と sd() によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では \\(s^2=\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2\\) で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これは、これらの指標のほうが統計的に好ましい性質を持っているためであるが、Rを用いた分散の計算値が、nで割った際の手計算値と異なることがあるのでその点には注意が必要である。 var(math) ## [1] 2 var(jpn) ## [1] 4.25 先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。 観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 \\(x_1,...,x_n\\) に対して、標準化されたスコア \\(z_1,..,z_n\\) は以下のように定義できる。 \\[ z_i=\\frac{(x_i-\\bar{x})}{\\sqrt{(S^2)}} \\] ただし、 \\(S^2\\) は変数 \\(x\\) の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。 一方でデータの観測数（ \\(n\\) ）が偶数である場合、\\(\\small n/2\\) 番目と、\\(\\small (n/2)+1\\) 番目が中央となるため、n個のデータの観測値を、\\(x_1,x_2,...,x_n\\) とすると、これらふたつの値の平均値（ \\(\\small \\frac{x_{\\frac{n}{2}}+x_{ \\frac{n}{2}+1}}{2}\\) ）が中央値となる。Rにおいてはmedian() 関数によって以下のように計算することができる。↩︎ 偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。↩︎ "],["カテゴリ変数の要約.html", "6.2 カテゴリ変数の要約", " 6.2 カテゴリ変数の要約 一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、table() 関数を用いる。また、with()関数を用いて同様の結果を得ることも可能である。ここでは、前節で保存した顧客ベースのidposデータを用いる。 idpos_cust &lt;- readr::read_csv(&quot;data/idpos_customer.csv&quot;) table(idpos_cust$gender) ## ## female male ## 985 502 with(idpos_cust, table(gender)) ## gender ## female male ## 985 502 また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける各デシルランクと性別の関係は以下のように示される。 with(idpos_cust, table(gender,decile_rank)) ## decile_rank ## gender 1 2 3 4 5 6 7 8 9 10 ## female 102 100 99 94 106 103 97 83 98 103 ## male 47 49 49 55 43 45 53 64 51 46 特定のカテゴリ（例、デシルランク）に着目して、カテゴリ変数（例、性別）についての集計を行うことも可能である。例えば、デシルランク10における男女差のみを調べたいときには、filter() 関数を用いれば良い。 idpos_cust %&gt;% filter(decile_rank == 10) %&gt;% with(table(gender)) ## gender ## female male ## 103 46 カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。また、そのための手法はすでに我々は学習済みである。具体的には、前節で利用した group_by() 関数を用いる。例えば、合計支出額と購買頻度の平均と標準偏差を男女ごとに確認することは、以下のような指示で可能になる。なお、tidyverseを起動していない場合には、必要に応じて library(tidyverse) を事前に指示してほしい。 idpos_cust %&gt;% group_by(gender) %&gt;% summarize(mon_m = mean(monetary), mon_sd = sd(monetary), freq_m = mean(frequency), freq_sd = sd(frequency)) ## # A tibble: 2 × 5 ## gender mon_m mon_sd freq_m freq_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 17511. 32770. 2.13 3.88 ## 2 male 14878. 23222. 1.80 2.77 "],["データの可視化.html", "6.3 データの可視化", " 6.3 データの可視化 本書でのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データは一般的に、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化される。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて library(tidyverse) によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。 head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。 ##For mac users theme_set(theme_gray(base_size = 10, base_family = &quot;HiraMinProN-W3&quot;)) 本書の可視化では、まず、ggplot2の ggplot() 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。 data: 可視化に用いるデータフレームの指定 mapping: データから抽出する変数と画面に表示される図との関係の指定 mapping内で、aes() 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。 ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定されている。続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) 用いてグラフィックの層(layer)を加えることで図を作成する。このプロセスでは、geom_point() による散布図や、geom_histogram() によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に labs() というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。 ggplot2を用いたデータ可視化の例として、まず本書はヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに geom_histogram() を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例である。価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。 p1 &lt;- ggplot(diamonds, mapping = aes(x = price)) p1 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム1: ダイアモンド価格&quot;) なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、“p1” というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。 p1 + geom_density(fill = &quot;black&quot;, alpha = 0.5) + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム2: ダイアモンド価格（geom_density）&quot;) 箱ひげ図は、四分位数と四分位範囲等を図示化したもの。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。 p2 &lt;- ggplot(diamonds, mapping = aes(x = cut, y = price)) p2 + geom_boxplot() + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;箱ひげ図1: ダイアモンド価格&quot;) 箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上小さい、ないしは、Q3よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。 バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、geom_violin() を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。 p2 + geom_violin() + geom_boxplot(fill = &quot;gray&quot;, width = 0.1) + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;バイオリンプロット: ダイアモンド価格&quot;) バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。 "],["二変数間の関係の要約.html", "6.4 二変数間の関係の要約", " 6.4 二変数間の関係の要約 ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（\\(S_{xy}\\)）は、以下のように定義される。なお、Rで共分散を求める際には cov() 関数を用いる。 \\(S_{xy}=\\frac{1}{n}\\sum_i^n (x_i-\\bar{x})(y_i-\\bar{y})\\) また、\\(S_x\\)と\\(S_y\\)をそれぞれxとyの分散とし、相関係数（\\(\\rho_{xy}\\)）は以下のように定義される。Rで相関係数を求める際には cor() 関数を用いる。 \\(\\rho_{xy}=\\frac{S_{xy}}{\\sqrt{S_x^2}\\cdot \\sqrt{S_y^2}}\\) 共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を、着目している二つの変数が持つことが知られている。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、線形関係の程度である。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。 例えば、以下のようなデータセットを考える。 ## # A tibble: 5 × 3 ## x1 y1 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 16 8 ## 2 -1 12 6 ## 3 0 10 5 ## 4 2 6 3 ## 5 5 0 0 このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（\\(y=-2x+10\\)）に並ぶことがわかる。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は \\(y=-x+5\\)である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。 cor(X$x1, X$y1) ## [1] -1 ggplot(data = X, mapping = aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = lm) また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。 ## # A tibble: 5 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 4 ## 2 -1 1 ## 3 0 0 ## 4 1 1 ## 5 2 4 cor(AB$A, AB$B) ## [1] 0 しかしながら、両者の関係を描画すると、\\(y = x^2\\) という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。 ggplot(data = AB, mapping = aes(x = A, y = B)) + geom_point() + geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE) 勘の良い読者であればすでに気づいているかも知れないが、二変数間の関係についての可視化もggplot2にて対応できる。具体的には、geom_point()という関数を用いるのだが、それだけではなく、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関を計算する。 cov(diamonds$carat,diamonds$price) ## [1] 1742.765 cor(diamonds$carat,diamonds$price) ## [1] 0.9215913 これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。 p3 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price)) p3 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図1: カラット：価格&quot;) 研究目的次第では、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。 Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。 facet_grid() や facet_wrap() を用いる。 まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。 p4 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut)) p4 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, color = &quot;カット&quot;, title = &quot;散布図 2: カット別、カラット：価格&quot;) このように、mapping = aes() 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、color =という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合には、この可視化の方法だと逆に見にくいかもしれない。そこで、以下の方法を紹介する。facet_wrap() を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。geom_point() で散布図作成の指示を与えたあとに、facet_wrap() のレイヤーを重ねる指示を与えれば、散布図 3が作成される。 p3 + geom_point() + facet_wrap(~cut) + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図 3: カット別、カラット：価格&quot;) 散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。 これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。 p5 &lt;- diamonds %&gt;% filter(carat &gt;= 1.0) %&gt;% ggplot(mapping = aes(x = price)) p5 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット以上&quot;) p6 &lt;- diamonds %&gt;% filter(carat &lt; 1.0) %&gt;% ggplot(mapping = aes(x = price)) p6 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット未満&quot;) Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。 ggsave(filename = &quot;plot1.pdf&quot;, plot = plot1, width = 10, height = 5, units = &quot;cm&quot;) 日本語を含む頭の場合、quartz() を用いた以下の手順を経て図を保存する。 1. quartz()で作図デバイスを起動する。 2. 作図デバイスを開いたまま、Rstudio内で図を表示する。 3. dev.off()という指示で作図デバイスを閉じることで図が保存される。 また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -&gt; Save as Image/ Save as PDF -&gt; Directory -&gt; File name）。 "],["参考文献-4.html", "6.5 参考文献", " 6.5 参考文献 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. Healy, Kieran (2018) Data Visualization: A Practical Introduction, Princeton University Press. "],["test.html", "Chapter 7 基礎統計学復習", " Chapter 7 基礎統計学復習 Rを用いて、統計的な分析（区間推定や検定）を実行すること自体はさほど難しくない。基本的な分析に必要な関数は基本パッケージに搭載されており、コードの書き方（引数の設定など）もネット上で検索すれば容易に知ることができる。しかしながら、あなた自身もしくは他者実行した分析をきちんと理解するためには、基礎的な統計学の内容を理解している必要がある。本資料は、基礎統計学を履修済みの学生を想定して書かれているが、本章でいくつかの基礎的な統計学の知識を復習する。また、本資料ではあくまで簡易的に説明を加えるのみなので、統計学を未習の場合は、基礎統計学の図書や講義で学習することを強く推奨する。また、章末に統計学や計量経済学の学習に役立つ参考文献を提示しているので、各自の学習に役立ててほしい。 "],["確率モデル期待値と分散.html", "7.1 確率モデル、期待値と分散", " 7.1 確率モデル、期待値と分散 伝統的なデータ分析では、標本から得た情報に基づき母集団の性質について推測する。母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈できる（倉田・星野、2011）。そのうえで確率とは、起こりうる事象の集合内において、各事象の起こりやすさの度合いを0以上1以下の実数で表したものである。より詳細な定義として、標本空間を \\(\\small \\Omega\\)、任意の事象 A に対して実数 P(A) が定まっていて、以下の三つを満たすとき、P(A)は事象 A の確率という： 確率は非負であり、以下を満たす： \\[0\\leq P(A)\\leq 1\\] 全事象を \\(\\Omega\\)、空事象を \\(\\emptyset\\) とするとこれらの確率は以下の様に示される： \\[P(\\Omega)=1,~~ P(\\emptyset)=0\\] 事象 \\(A_1,A_2,...\\) が互いに排反ならば、これらのうち少なくとも１つが起こる事象 \\(A_1\\cup A_2\\cup ...\\) の確率は以下となる： \\[ P(A_1\\cup A_2\\cup ...)=P(A_1)+P(A_2)+... \\] 確率変数とは、ある標本空間上で定義される取りうる各事象に対してそれぞれ一定の確率と対応関係のあるような変数である。例えば、細工のないサイコロを投げるとき、出た目の値を \\(x\\) とすると、\\(x\\) は1から6までの整数を取りうる変数だと言い換えることができる。この場合標本空間は、取りうる出目に対応した6個の標本点からなる。またこれらの標本点には、それぞれ対応する確率が以下のように付与されている。 Table 7.1: サイコロの確率分布 x 1 2 3 4 5 6 確率 1/6 1/6 1/6 1/6 1/6 1/6 このように、確率変数の取る値に対応して確率が付与されるルール（ \\(x\\) の関数としての確率 \\(P(x)\\) ）を確率分布や確率分布関数という。確率変数は主に、離散確率変数と連続確率変数に分けることができる。離散確率変数は、サイコロのように、取りうる値が離散的な確率変数である。一方で、連続確率変数は、ある範囲の中で連続的にどんな値も取りうる確率変数である。離散確率変数では、サイコロの表で示されているように、取りうる特定の値に対応する確率を確率分布に基づき計算できる。 一方で連続確率変数の場合、取りうる値の数が無限に存在する。例えば、-1 から 1 までの区間を取りうる値の範囲とする連続確率変数があったとする。この変数は例えば、0.90という値を取りうるが、同様に、0.91 や 0.900001 といった値も取りうる。このように、連続確率変数が取りうる値の数は無限に存在するため、取りうる特定の値に対応する確率は0になる。もし取りうる各値に確率が付与されていると、確率の合計が無限大になってしまうという問題に直面する。そのため、連続確率変数の場合、取りうる区間に対して確率が付与される。これを踏まえて連続確率変数を捉え直すと、連続確率変数は、その取りうる任意の区間に対して一定の確率が対応するような変数であるといえる。また、連続確率変数における取りうる区間の起こりやすさには「確率密度」が対応することで計算可能になる。言い換えると、確率変数 \\(x\\) の値に確率密度がどのように対応するのかという関係は、\\(f(x)\\) という確率密度関数（probability density function: PDF）として示される。PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) が区間 [a, b] を取る確率 \\(P(a\\leq x \\leq b)\\) は、以下の積分計算で求められる。 \\[ P(a\\leq x \\leq b)=\\int^b_a f(x) dx \\] 以下の図はPDFの例であり、図内の曲線はPDFを、灰色に塗られている面積はある区間の確率を示している。なお、上記の式で示されている関係から、PDFを特定（仮定）することで、ある確率に対応する区間 [a, b] を求めることも可能である。以降の節で紹介する統計的分析では、この関係を用いて分析することもあるが、詳しくは後述する。 連続確率変数例 連続確率変数を用いた具体的な確率計算例を紹介するために、ここでは一様分布（uniform distribution）を用いる。区間 [a, b] を持つ一様分布に従う確率変数 \\(x\\) のPDFは以下のように示される。 \\[ f(x) = \\begin{cases} \\frac{1}{b-a} &amp; a\\leq x \\leq b\\\\ 0 &amp; otherwise \\end{cases} \\] 具体的な計算を実行するために、ここで区間 [-1, 3] を持つ一様分布を考える。この一様分布のPDFは、\\(\\small f(x)=\\frac{1}{4}~for~-1\\leq x\\leq 3\\)（その他の区間の確率は0）となる。このとき、\\(x\\) が区間 [0, 2] を取る確率は、以下のように求められる。 \\[ P(0\\leq x \\leq 2) = \\int^2_0 \\frac{1}{4} dx=\\left[\\frac{x}{4}\\right]^2_0=\\frac{1}{2}-0=\\frac{1}{2} \\] 次に、期待値を求める。期待値とは確率の考え方を含む理論的な平均値（\\(\\mu\\)）といえる。確率分布 \\(P(x)\\) を持つ離散確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\sum_x x \\cdot P(x)=\\mu \\] 一方、PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\int_{-\\infty}^\\infty x \\cdot f(x) dx=\\mu \\] この定義に基づき、先程の区間 [-1, 3] を持つ一様分布の期待値を以下のように求める。 \\[ E(x)= \\int^3_{-1} \\frac{x}{4} dx=\\left[\\frac{x^2}{8}\\right]_{-1}^3=\\frac{9-1}{8}=1 \\] 期待値 \\(E(x)\\) は一般的に、\\(a\\) を定数、\\(g(x)\\) と \\(h(x)\\) を \\(x\\) の関数とするとき、以下が成り立つ： \\(E(a) = a\\) \\(E[a\\cdot g(x)]=a\\cdot E[g(x)]\\) \\(E[g(x)+h(x)]=E[g(x)]+E[h(x)]\\) これらの性質により、確率変数 \\(x\\) の分散（\\(\\sigma^2\\)）は以下のように求まる。つまり、分散は二乗の期待値から期待値の二乗を引くことで計算できる。 \\[ \\sigma^2=E\\left[\\bigl(x-E(x)\\bigr)^2\\right]=E\\left[(x-\\mu)^2\\right]=E(x^2)-E(x)^2 \\] "],["統計的推測.html", "7.2 統計的推測", " 7.2 統計的推測 前節で述べた通り、我々は研究の対象となる集団全体ではなく、その一部から情報を取得し分析を行う。このとき、その集団全体を母集団、母集団から抽出した一部を標本と呼ぶ。統計的な分析においては、確率分布を用いて母集団をモデル化し、標本をその確率分布に従う確率変数とみなすことで母集団と標本の関係を捉える。そのため、データ分析は標本を対象とするものの、分析者の関心は、母集団の特徴である母数（parameter）についてであることが多い。母集団の平均（\\(\\mu\\)）や分散（\\(\\sigma^2\\)）は母数の代表例である。しかし、母数は通常未知であり直接知ることはできないため、標本の情報を用いて母集団の特徴について推測する。このプロセスを統計的推測と呼ぶ。統計的推測を行うためには、原則として母集団からの無作為標本抽出（random sampling）が必要になる。統計的推測では、互いに独立に同一の分布に従う（Identically Independently Distributed: IID）ような標本が好ましく、無作為標本は、IIDを満たすことが知られている。 統計的推測においては、「推定」、「統計量」、「推定量」、「推定値」などの似たような言葉が利用されるが、これらはそれぞれ異なる意味を持つ。推定とは、標本の情報に基づき母数について把握しようとする作業そのものを示す。一方で、標本として得られるデータに基づき計算できる値（計算式）を一般的に統計量というのだが、その中でも特に推定に用いる統計量を推定量という。そして推定値は、推定量についてデータから求めた実際の計算値を表す。また、推定には「点推定」と「区間推定」がある。点推定とは、未知の母数について１つの数値に基づいて推定する方法である。例えば、標本平均は母平均（\\(\\mu\\)）を点推定するための推定量である。一方で区間推定は、未知の母数を一定の確率で含む区間を推定する方法である。これは、点推定では捉えきれない統計的誤差を考慮して区間を推定する方法であり、母平均の信頼区間の測定は区間推定の代表例である。 "],["点推定.html", "7.3 点推定", " 7.3 点推定 点推定は特定の推定量によって母数を捉えようとするが、どのような推定量を用いるべきなのだろうか。本節では、不偏性（unbiasedness）、一致性（consistency）、効率性（efficiency）という統計的に重要な推定量の性質について説明する。なお、以下の説明では、未知パラメータ \\(\\small \\theta\\)（シータ）に対する推定量 \\(\\small \\hat{\\theta}\\)（シータハット）を考える。不偏性とは、推定量の「期待値」が未知パラメータの真の値に等しいという性質であり、以下のように示すことができる。 \\[ E(\\hat{\\theta})=\\theta \\] つまり、実際の推定量の実現値がどうかは置いておいて、期待値の下では推定量が未知パラメータを示していることを表すものであり、サンプルサイズに関係のない推定量の性質である。そして、不偏性を満たす推定量のことを不偏推定量（unbiased estimator）という。なお、上記の定義から、統計的なバイアス（B）は、以下のように定義できる。 \\[ B=E(\\hat{\\theta})-\\theta \\] 第二に一致性とは、サンプルサイズが十分に大きいとき、推定量が未知パラメータの真の値と等しくなる確率が1に近づくという性質である。この性質について詳しく論じるには、漸近理論を学ぶ必要があるため、本書では詳細を省略するが、サンプルサイズを大きくすると未知パラメータの真の値に近づくような推定量を示した性質だと解釈できる。なお、任意の\\(\\small \\epsilon &gt;0\\)（\\(\\small \\epsilon\\): イプシロン）に対して以下のような性質を持つ推定量を一致推定量という。 \\[ \\lim_{n\\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|\\leq \\epsilon \\right)=1 \\] 第三に効率性は、推定量の分散の小ささを示している。分散の小さい推定量の方が、期待値から離れた値を取りにくく、好ましい推定量と考えられる。複数の不偏推定量や一致推定量がある場合、効率性を元に好ましい推定量を考える。 なお、代表的な推定量である標本平均は母集団期待値の推定量として好ましい性質（不偏性と一致性）も持っている。以下では、期待値 \\(\\mu\\)、分散 \\(\\sigma^2\\) の確率分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\)を考える（つまり、\\(\\small E(X)=\\mu\\), \\(\\small Var(X)=\\sigma^2\\)）。このとき、標本平均（\\(\\small \\bar{X}\\)）の普遍性は以下のように示すことができる。 \\[ E(\\bar{X})= \\left[\\frac{1}{n}(X_1+X_2+...+X_n)\\right] = \\frac{1}{n}~\\left[E(X_1)+E(X_2)+...+E(X_n)\\right] = \\frac{1}{n}\\cdot n\\mu=\\mu. \\] また標本平均の分散については、以下となることが知られている（計算は省略）。 \\[ Var(\\bar{X})=\\frac{\\sigma^2}{n} \\] 上記と同様の無作為標本による標本平均の一致性については、任意の \\(\\small \\epsilon&gt;0\\) に対していかが成り立つことが知られている。 \\[ \\lim_{n\\rightarrow \\infty}P(|\\bar{X}-\\mu|\\leq \\epsilon)=1 \\] 言い換えると、サンプルサイズが増えることで標本平均 \\(\\small\\bar{X}\\) は母集団の真の平均 \\(\\small \\mu\\) と等しくなる確率が1に近づく。なお、標本平均がもつこの特性は「大数の法則（Law of Large Number）」として知られている。 また、標本平均はその分布の収束に関しても重要な特性を持っている。ここで、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) を持つ確率分布に従う母集団からのn個の無作為標本 \\(\\small X_1,.., X_n\\) を考える。サンプルサイズが十分に大きい場合、 \\(\\small \\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)（\\(\\small \\bar{X}\\) が平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布に従う）となることが知られている。この性質を「中心極限定理（Central Limit Theorem）」という（詳細な証明や定義は省略）。また、この定理を以下のような\\(\\small \\bar{X}\\) を標準化した確率変数に応用することも可能である。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 中心極限定理より、サンプルサイズが十分に大きい場合、Z の分布関数は標準正規分布（N(0,1)）の分布関数に収束する。詳細については割愛するが、サンプルサイズが十分に大きい場合、「標本平均」や「標本平均を標準化した確率変数」の確率分布が正規分布や標準正規分布に近似できるという定理は、統計的な推定や検定において重要なものである。 また、母集団分散の推定量としては、不偏標本分散が使われる事が多い。上記と同じ無作為標本に対し、標本分散 \\(S^2\\) と、不偏標本分散 \\(s^2\\) は以下のように定義される。 \\[ S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X}) \\] \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}) \\] そして、それぞれの推定量の期待値は以下のようになることが知られている（計算省略）。そのため、母集団分散の推定量として、不偏標本分散（\\(s^2\\)）が用いられる。 \\[ E(S^2)=\\frac{n-1}{n}\\sigma^2 \\] \\[ E(s^2)=\\sigma^2 \\] "],["推定量もまた確率変数.html", "7.4 推定量もまた確率変数", " 7.4 推定量もまた確率変数 次に、推定値と母数との関係を標本平均（\\(\\small \\bar{X}\\)）を使って考える（標本平均の定義は、「記述統計」の節を参照）。ある母集団からランダムサンプルを収集し、標本平均を計算することを考える。ここで計算された数値は真の母平均を捉えた唯一の値なのだろうか。 結論としては、点推定の推定値は母数そのものではなく、ひとつのある実現値でしかないことに注意が必要である。この理由は、「確率変数から計算される推定量もまた確率変数である」という事実から理解することができる。 例えば、我々が一橋大学商学部生の一ヶ月あたりの平均収入（仕送りは除く）に関心があるとする。（実現可能性は置いておいて）商学部全体を母集団とする無作為標本を100件収集し、標本平均を計算した結果 \\(\\small \\bar{X}=\\) 0 だったとする。もしこのような極端な結果を得た場合、多くの人が「標本平均の実現値は必ずしも真の母平均そのものではない」という説明に納得がいくだろう。同様の調査（100件のランダムサンプリング）をもう一度行い平均収入を計算し直すと、おそらく0とは異なる推定値を得る可能性が高い。仮に、\\(\\small\\bar{X}=\\) 50,000 だった場合、その結果をどのように解釈するだろうか。仕送りを除く大学生の月当たり収入の平均が5万円だという結果はなんとも尤もらしい。しかしながら、たとえ尤もらしい結果を得たとしても、それはひとつの分析結果であり、真の母平均を示す唯一の値ではない。 確率変数から計算される推定量もまた確率変数であるという点を直感的に経験するために、細工のない6面サイコロを（バーチャルに）振ってもらう。なお、コード内では、出力結果を少し見やすくするために、knitr というパッケージを利用しているため、以下のようにインストールしてほしい。 install.packages(&quot;knitr&quot;) 標本平均についての議論を行う前に、理論的な期待値を求める。6面サイコロの出目の期待値 \\(\\mu\\) は以下の通りである。 \\[ \\mu = 1\\cdot \\frac{1}{6}+2\\cdot \\frac{1}{6}+...+6\\cdot \\frac{1}{6}=3.5 \\] ここで、以下のようなコマンドを用いてR内でサイコロを振ってみてほしい（実際にサイコロを振ってもらっても構わないが、面倒くさい）。 set.seed(442)# 乱数の再現性確保のための指示。関数内の数字に特に意味はないため各自別の値を使っても良い。 die &lt;- 1:6 d &lt;- sample(die,size=1,replace = TRUE) d ## [1] 6 この講義ノート内では以上で示されている通り、6という出目を得た。上記のコマンドを実施した各自がそれぞれ異なる値を得ているだろう。ここで得た6という数字は、サイコロの出目という確率変数の実現値（\\(n=1\\)）である。そのため、本データの標本平均も6であり、真の期待値とは異なる。ただし、読者によっては1件の標本による標本平均という表現を直感的に理解しにくいかもしれない。そのため、以下のように サイコロを10回振る試行を3回実施し、各サンプリング結果に基づき標本平均を以下のように計算する。 set.seed(352) d1 &lt;- sample(die,size=10,replace = TRUE) d2 &lt;- sample(die,size=10,replace = TRUE) d3 &lt;- sample(die,size=10,replace = TRUE) d_mean &lt;- matrix(c(mean(d1),mean(d2),mean(d3)),nrow = 1) colnames(d_mean) &lt;- c(&quot;d1の平均&quot;, &quot;d2の平均&quot;, &quot;d3の平均&quot;) knitr::kable(d_mean, caption = &quot;サイコロの標本平均比較&quot;, align = &quot;ccc&quot;) Table 7.2: サイコロの標本平均比較 d1の平均 d2の平均 d3の平均 3.2 3 2.4 上記の通り、d1, d2, d3 いずれの標本平均も互いに異なるものであり、また3.5とも異なる。このことからも、確率変数（サイコロの出目）を用いて計算された推定値（標本平均）もまた確率変数であり、推定値と未知パラメータとの間にはズレ（誤差）が生じうることがわかる。なお、中には3.5と等しい標本平均を偶然得た読者もいると考えられるが、それもあくまで一つの実現値である。 では、標本平均の推定値がサンプルサイズによってどれだけ真の期待値に近づくのかについて、サイコロの試行回数を10回、100回、1,000回と増やして確認する。以下の結果を見ると、サンプルサイズ（試行回数）が増えるごとに真の期待値に近づいていることが伺える。ただし、これらの結果もあくまで確率的な試行結果の実現値である。そのため、読者によっては異なる傾向を示すような結果を得る可能性があることに注意が必要である。 set.seed(541) d10 &lt;- sample(die,size=10,replace = TRUE) d100 &lt;- sample(die,size=100,replace = TRUE) d1000 &lt;- sample(die,size=1000,replace = TRUE) d_lln &lt;- matrix(c(mean(d10),mean(d100),mean(d1000)),nrow = 1) colnames(d_lln) &lt;- c(&quot;10回試行の平均&quot;, &quot;100回試行の平均&quot;, &quot;1,000回試行の平均&quot;) knitr::kable(d_lln, caption = &quot;サイコロの標本平均比較２&quot;, align = &quot;ccc&quot;) Table 7.3: サイコロの標本平均比較２ 10回試行の平均 100回試行の平均 1,000回試行の平均 3 3.36 3.516 "],["補足いくつかの確率分布の関係性.html", "7.5 補足（いくつかの確率分布の関係性）", " 7.5 補足（いくつかの確率分布の関係性） 統計的な分析の際によく用いられる確率分布として、正規分布、カイ二乗分布、t分布、F分布間の関係性について簡単に紹介する。なお、各分布の確率密度関数などは記載しないため、関心のある読者は参考文献を参照してほしい。 7.5.1 正規分布 正規分布は、様々な分布の基準として用いられる重要な分布である。期待値を中心に左右対称であり「ベルカーブ」と言われる形状を持つ。また、平均0、分散1の正規分布は特に標準正規分布と言われ、正規母集団からの無作為標本の標本平均等の分布を特定する際などに用いられる。 7.5.2 カイ二乗分布 標準正規分布からの無作為標本の二乗和はカイ二乗分布に従う。カイ二乗分布は、正規母集団からの無作為標本の不偏標本分散の分布を特定する際などに用いられる。 7.5.3 t分布 標準正規分布とカイ二乗分布の比はt分布に従う。t分布は、正規母集団からの無作為標本による標本平均と不偏標本分散の比の分布を特定する際などに用いられる。 7.5.4 F分布 カイ二乗分布の比はF分布に従う。F分布は、異なる正規母集団からの無作為標本の不偏標本分散の比の分布を特定する際などに用いられる。 "],["区間推定.html", "7.6 区間推定", " 7.6 区間推定 点推定の節で示した通り、推定値と未知パラメータの間には、ずれ（誤差）がある。標本平均の様に好ましい性質（不偏性や一致性）を持つ推定量であっても、計算の結果示された一つの推定値がどの程度信頼できるものなのかはわからない。そこで区間推定という、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を計算する方法を用いて、統計的な誤差を加味した母数への検討を試みる。区間推定においては、「信頼水準zz%で、xx以上、yy以下という区間は真の母数を含む」という区間[xx, yy]を調べる。このような区間は信頼区間（confidence interval）と呼ばれ、多くの統計分析において用いられている。 Rで信頼区間を求める事自体は難しくない。最も手間のかからない方法としては、t.test()（詳細は後述）の分析結果を用いて、conf.int()によって信頼区間が計算できる。信頼区間の計算を実行するために、倉田・星野（2011, p.248）で提示されている以下の電球の製品寿命に関する例を考える。ある製品（電球）の寿命は平均1700（時間）である。企業は性能を改良するために新型の電球が開発したが、新型化に伴い製品寿命も変化したのかについては不明である。ただし、この製品の寿命は新型も旧型のものも正規分布に従い、その標準偏差は \\(\\sigma=\\) 180（時間）であると仮定する。 工場で生産された新型製品を16個無作為に選びその寿命を計測した所、以下の結果を得た。 1873 1685 2275 1760 1769 2176 1748 1760 1994 1473 1715 1771 1784 1684 2038 1850 このデータは、平均が \\(\\small \\mu\\)、分散が \\(\\small 180^2\\) である正規分布（\\(\\small N(\\mu, \\sigma^2=180^2)\\) と表記する）からの無作為標本 \\(X_1,..., X_{16}\\)の実現値とみなすことができる。なお、このデータの標本平均は1,835（時間）、不偏標本標準偏差は 200である。このデータに基づく、新型電球寿命の期待値に関する95% 信頼区間（95%の確率で真の母数を含む区間）はt.test() 関数を用いると以下の様に求まる（ただし後述するが、この方法はこの例に対しては適切ではない）。 bulb &lt;- c(1873, 1685, 2275, 1760, 1769, 2176, 1748, 1760, 1994, 1473, 1715, 1771, 1784, 1684, 2038, 1850) bulb_ci &lt;- t.test(bulb) #t検定の実施と格納 bulb_ci$conf.int #信頼区間の出力（デフォルトで95%信頼水準） ## [1] 1728.235 1941.140 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 出力されている [1] 1728.235 1941.140 が信頼区間、## attr(,\"conf.level\") ## [1] 0.95 が今回計算に用いられた信頼水準（confidence level）（もしくは信頼係数（confidence coefficient）ともいう）である。 分析の結果、95%の確率で真の新製品寿命期待値が 1728.2 から 1941.1 の間に含まれることがわかった。したがって、どうやら新製品寿命は平均的に旧型製品（1,700）よりも長そうである。では、ここで示された区間がどのように計算され、どのようなことを意味するのだろうか。以下では、一度電球の例から離れ、もう少し一般的な形で信頼区間の導出や解釈を説明する。 はじめに、標準正規分布に基づくある区間の確率の求め方を説明する。 \\(Z_1,Z_2,...,Z_n\\) は、N(0, 1) （標準正規分布）に従う母集団からの無作為標本とする。このとき、標準正規分布がある区間 [\\(-\\infty,~z_\\alpha\\)] をとる確率は、以下の式および図のように示すことができる7。なお、\\(\\small z_\\alpha\\) は、確率\\(\\small \\alpha\\) に対応する標準正規分布上の上側確率 \\(\\small \\alpha\\) 点とする。このとき、この分布における \\(\\small z_\\alpha\\) 以下（以上）の範囲を取る確率は 1 \\(\\small -\\alpha\\)（\\(\\small \\alpha\\)） である。 \\[ P(Z\\leq z_\\alpha) = \\int^{z_\\alpha}_{-\\infty}~f(z)~dz =\\int^{z_\\alpha}_{-\\infty}~\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)~ dz = 1-\\alpha \\] 標準正規分布と確率計算 同様に、以下のような関係も捉えることができる。この場合、斜線部で示されている範囲の確率は両側合わせて \\(\\small \\alpha\\) であり、その内側の確率は 1 \\(\\small - \\alpha\\) である。 \\[ P(-z_{\\alpha/2} \\leq Z\\leq z_{\\alpha/2}) = \\int^{z_{\\alpha/2}}_{-z_{\\alpha/2}}~f(z)~dz = 1-\\alpha \\] 標準正規分布と両側確率 上述の関係を、区間推定に応用するために、あるデータの標本平均に関する議論を捉える。標準正規分布に従う確率変数は、正規分布に従う確率変数を標準化することで得ることができる。ここで、\\(\\small X_1,...,X_n\\) を、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) の正規分布に従う母集団からの無作為標本とする。これまで学んだ標準化および標本平均の特性から、以下の通り、標本平均を標準化したものは標準正規分布に従うことがわかる。 \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] このことから、先述の標準正規分布における確率計算の関係を応用し、以下を得る。 \\[ P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha \\] したがって、区間 [\\(\\small \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\)] は、確率 \\(\\small 1-\\alpha\\) で未知の母平均 \\(\\small \\mu\\) を含むと解釈できる。また、上記の関係から任意の確率 \\(\\alpha\\) を指定することで、区間の上限と下限（\\(\\pm z_{\\alpha/2}\\)）の具体的な値を（統計学テキスト巻末などに記載されている）標準正規分布表などから求めることができる。 そして、このような区間を「信頼区間」といい、信頼区間の計算にて仮定された確率 \\(\\small 1-\\alpha\\) を「信頼水準」もしくは「信頼係数」という。信頼係数は、信頼区間の計算のために研究者によって事前に選択される。慣習としては、90%, 95%や99% (\\(\\small \\alpha =\\) 0.10, 0.05, 0.01)を用いる事が多い。なお、信頼係数を大きくすると、信頼区間も広くなる。 上記の区間推定は母分散 \\(\\sigma^2\\) が既知である場合に計算可能であるが、多くの場合母分散は未知である。そのような場合には、自由度 n-1 の「t分布」を用いて、両端の確率 \\(\\small \\alpha\\) 点を \\(\\small t_{\\alpha/2}(n-1)\\) とする信頼区間を求める。\\(N(\\mu,\\sigma^2)\\) に従う母集団からの無作為標本を考えるが、今回は母分散が未知である場合を仮定する。このような場合は、母分散 \\(\\small \\sigma^2\\) のかわりに母分散の不偏推定量である不偏標本分散 \\(\\small s^2\\) を用いた以下の統計量 t をもとに信頼区間を計算する。このとき、統計量 t は自由度 n-1 の t 分布に従うことが知られている（t分布に関する詳細および証明は省略）。 \\[ t=\\frac{\\bar{X}-\\mu}{\\sqrt{s^2/n}}\\sim t(n-1) \\] ここで、先述の標準正規分布に基づくある区間の確率計算と同様の計算を、自由度 \\(\\small n-1\\) のt分布に基づき実行すると、以下のような確率と区間の関係に書き換えることができる。 \\[ P\\left(-t_{\\alpha/2}(n-1)\\leq \\frac{\\bar{X}-\\mu}{s^2/n}\\leq t_{\\alpha/2}(n-1)\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\right)=1-\\alpha \\] 信頼区間を求めるための手順は標準正規分布の場合もt分布の場合も同様だが、標準正規分布のかわりに t 分布を用いた場合、特定の確率に対応する閾値が変わることが知られている（\\(\\small z_{\\alpha/2}\\neq t_{\\alpha/2}(n-1)\\)）。 t分布は、左右対称であり標準正規分布よりもテールが厚いという特徴を持つが、自由度が大きくなると正規分布に近づくことが知られている。標準正規分布と、自由度の異なる t 分布との関係は以下のように図示化できる。自由度（df）3の t 分布よりも自由度20の t 分布のほうが標準正規分布に近い形状であることが伺える。 標準正規分布と t 分布によってある確率に対応する閾値が異なるということは、ある信頼水準に対応する信頼区間も仮定する分布によって異なるということである。新型電球の例では正規分布を仮定しているため、標準正規分布により信頼区間を求める。 先述の通り、標準正規分布に基づく信頼区間は以下のように示すことができる。 \\[ \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n} \\] このとき、仮定より \\(\\small \\bar{X}=1835\\), \\(\\sigma=180\\) であることがわかっている。また、慣習より95%信頼水準を仮定すると、確率 \\(\\small \\alpha = 0.05\\)となる。そのため、区間推定の計算で必要な要素のうち現時点で不明なのは、 \\(z_{\\alpha/2}=z_{0.025}\\)の値である。この値は、任意の確率に対応する区間の閾値を表している。今回の場合、分布が左右対称の分布であり正負どちらか一方の値さえ分かればよいため、閾値（\\(z_{0.025}\\)）以上の区間を取る確率が2.5%になるような閾値に着目する。このような閾値は、Rによって以下のように求める（なお先述の通り、統計学教科書に掲載されている分布表を使っても同様の数値を求めることができる）。 qnorm(0.025, lower.tail=FALSE) ## [1] 1.959964 これにより、計算に必要な情報が揃ったため、以下の要領で信頼区間を出力できる。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 #信頼区間の計算 upper &lt;- xbar+z*(sigma/sqrt(n)) lower &lt;- xbar-z*(sigma/sqrt(n)) #結果のまとめと出力 ci.bulb &lt;- matrix(c(lower,upper),nrow=1) colnames(ci.bulb) &lt;- c(&quot;ci.lower&quot;, &quot;ci.upper&quot;) knitr::kable(ci.bulb, caption = &quot;Bulb data CI（95%）&quot;, align = &quot;cc&quot;) (#tab:ci_bulb)Bulb data CI（95%） ci.lower ci.upper 1746.489 1922.886 分析の結果、新型製品の平均寿命に関する95%信頼区間は、標準正規分布と t 分布どちらの分布を仮定しても旧型の平均1700（時間）を含まず、それよりも大きい値を取るものであった。したがって、新型製品は製品寿命の面においても95%の確率で旧型製品よりも優れていると考えられる。 では、この95%信頼区間は、そもそもどのように解釈すべきだろうか。結論から述べると、95%信頼区間の直感的解釈については以下のように説明できる： “「母集団からサンプルを取り平均値の95%信頼区間を構築する」という手順を100回繰り返すと考える。95%という信頼水準（確率）が示していることは、計算された区間が100回に95回は母数を含むということである。言い換えると、今回得た標本平均に基づき計算された信頼区間がはずれ（真の母数を含まない区間）である可能性が5%存在するということである。” 95%信頼区間の解釈として、「分析対象としている母数の値がこの区間の値をとる確率が95%である」という旨の説明を行う人がいるが、これは\\(\\color{red}{\\text{誤り}}\\)である。確率的な議論を母数に適応するのは適切ではない。確率的に変動するのはあくまで区間の両端である点を理解しなければならない。なぜならば、\\(\\small \\bar{X}\\) が確率変数であるため、そこから計算される区間の両端もまた確率変数となるためである（岩田，1996）。ここで示されている信頼水準は、計算された区間が真の母数を含んでいる確率である。つまり、信頼水準は、サンプルを収集し、信頼区間を求めるという「手順そのものに対する信頼度」を表す指標であると解釈できる。 また、分析の結果、t.test によって出力された信頼区間よりも短い区間を得た。これは、t 分布のほうがテールが厚く、中心より遠い値を取る確率密度が高いことから、95%水準における臨界値が標準正規分布に比べて大きくなるためである。また、t 分布の自由度が大きくなるにつれ、正規分布との差が小さくなる。 ただし、標準正規分布の確率密度関数 \\(\\small f(z)\\) は、\\(\\small 1/\\sqrt{2\\pi}\\exp\\left(-z^2/2\\right)\\) だと知られている。↩︎ "],["統計的仮説検定.html", "7.7 統計的仮説検定", " 7.7 統計的仮説検定 データを用いた研究では、統計的分析によって提示した仮説が支持できるか否かを判断したいという目的を持つこともある。その時に用いられる方法が統計的仮説検定である。本節ではまず検定の手順について説明したあと、分析結果の意味や解釈について説明する。統計的仮説検定は、基本的に以下の手順によって実施される。 仮説（帰無仮説・対立仮説）を設ける。 仮説を検定するための統計量を選ぶ。 統計量の値について、有意確率に基づく臨界値を設定する。 帰無仮説が正しいと仮定した上で統計量を計算し、その値が棄却域と採択域のどちらの領域に入るかを分析する。 統計的仮説検定で重要になる仮説は、帰無仮説と対立仮説である。理解を容易にするために、「問題と分析をつなげる仮説の提示」という節で議論した仮説を「作業仮説」と呼ぶ。作業仮説は、リサーチクエスチョンに答えるための論理的予測である。例えば、「女性に比べ男性の方が新製品購買意図が高い。」のような予測が考える。このような仮説を検証する場合、男女（グループ）間で購買意図の平均値を比較することが現実的な分析方法として考えられる。 帰無仮説と対立仮説は、統計的仮説検定の基準になる母集団の統計的特徴に関する仮説であり、検定という手続き上ではこれらの仮説に着目する。特に、帰無仮説は、統計的仮説検定の考察、分析の基準となる仮説であり、この仮説を棄却（否定）できるか否かを調べることが基本的な統計的仮説検定の枠組みだと言える。帰無仮説は棄却しうる仮説であり、\\(\\small H_0\\) という記号で表される事が多い。また、多くの場合において「差がない」、「効果がない（0である）」や、「特定の値と等しい」といった仮説が設計される。一方で対立仮説は、帰無仮説とは排反な仮説であり、帰無仮説が棄却された際に採用される推測であり、\\(\\small H_1\\)や\\(\\small H_a\\)という記号で表される。データ分析を用いた研究においては対立仮説と作業仮説は論理的に整合的ないしは等しいことが好ましい。つまり、作業仮説という研究上重要な論理的推測を検証するために、その作業仮説とは排反な帰無仮説を設計し統計的仮説検定を実施する必要がある。それによってもしその帰無仮説が棄却されたならば、対立仮説ひいては作業仮説がデータ分析によって支持されたと解釈することが可能になる。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_m\\)、女性における購買意図の期待値を \\(\\small \\mu_f\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\(H_0:~\\mu_m=\\mu_f\\) \\(H_1:~\\mu_m\\neq\\mu_f\\) \\(\\small H_0\\) は、男性における購買意図の期待値と女性における期待値が等しいというものであり、 \\(\\small H_1\\)はそれらが等しくないということを示している。そのため上記の二つの仮説は、どちらも未知パラメータについての関係を捉えており、\\(\\small H_0\\) と \\(\\small H_1\\) は互いに排反であることがわかる。その上で、もし帰無仮説が棄却され、男性の平均値のほうが女性よりも高い場合には、作業仮説が支持されたと解釈することができる。つまり統計的な検定においては、作業仮説として提示している推測を直接検証するのではなく、作業仮説と排反な帰無仮説を設計し、それが棄却されるならば暫定的に作業仮説の主張を指示しようという立場で検証を行う。なお対立仮説として\\(\\small H_1:~\\mu_m&gt;\\mu_f\\) を設定することも可能である。このような仮説に基づく検定方法は片側検定と呼ばれ、その詳細については後述する。 ここで改めて、より一般的な形でマーケティングリサーチで用いられる仮説と検定で用いられる仮説との関係を整理する。 分析上の基準である帰無仮説は何かをきちんと理解し定義する。 それが棄却された際にはどのような結論（対立仮説）が採用されるのかを理解する。 そしてその結論が自身の立てた作業仮説と帰無仮説・対立仮説の関係が整合的かを考える。 言い換えると、自身の立てた作業仮説を帰無仮説・対立仮説の対比という分析手続きで証明できるような調査・分析法を採用する必要がある。ただし、レポートや論文には、帰無仮説・対立仮説を記載せず、作業仮説のみを記載することがほとんどである。 先程の新型電球の例を再度使い母平均の検定を実施する。新型電球について我々が関心を持っていたのは、新型電球の製品寿命が旧型の寿命（1700時間）より長いか否かである。そのため、新型電球の製品寿命の期待値を \\(\\small \\mu\\) とすると、帰無仮説と対立仮説は以下のように設計できる。 \\(H_0:~\\mu=1700\\) \\(H_1:~\\mu\\neq1700\\) 改めて以下の通り、新型電球に関する16個の無作為標本から得た製品寿命の平均値を計算すると、\\(\\small \\bar{X}=1835\\) であった。では、この1835は 1700 から十分に離れていると言えるのだろうか？もし、十分に離れていると判断されれば帰無仮説を棄却するが、この差が十分でなければ帰無仮説を採択する。 mean(bulb) ## [1] 1834.688 Rを用いて統計的検定を実行すること自体は難しくない。母平均の検定は t.test() で実施することが可能である。母平均が特定の値を取るか否かについての検定では、mu= という引数を使って帰無仮説に対応する値を指定する。今回の分析に関するコマンドおよびその結果は以下のとおりである。 t.test(bulb, alternative = &quot;two.sided&quot;, mu = 1700) ## ## One Sample t-test ## ## data: bulb ## t = 2.6968, df = 15, p-value = 0.01657 ## alternative hypothesis: true mean is not equal to 1700 ## 95 percent confidence interval: ## 1728.235 1941.140 ## sample estimates: ## mean of x ## 1834.688 分析結果の t= と df = はそれぞれt値（検定統計量の推定値）と自由度を表している。p-valueはp値と呼ばれるある確率を表しており、この確率が小さい場合、帰無仮説を棄却する。また、t.test() は、信頼区間や標本平均も出力してくれるため、これらの結果に基づき解釈を行うことも可能である。帰無仮説の棄却に至るp値の基準は慣習的に、0.10（10%）、0.05（5%）、0.01（1%）が用いられる。今回の結果では、p値が0.016であり、5%水準で帰無仮説を棄却することができるため、新型電球の寿命は旧型（1700時間）よりも有意に高いと結論づけることができる。では、このp値とはどのような確率を示しているのだろうか？この点を理解するために統計的仮説検定についてもう少し深掘りしていく。 検定における統計量や有意確率について理解するために、再度新型電球の例を用いる。上述の t.test() は母分散が未知である際に用いられる検定方法である。この点は、信頼区間において説明した内容と同様である。なお実際のデータ分析作業においては多くの場合母分散は未知であるため、t.test() を用いることが多い。しかしながら、電球の例では母集団の分散は \\(\\small 180^2\\) であることを仮定した。そのため、ここからは母分散が既知（\\(\\small \\sigma^2=180^2\\)）であることを仮定した標準正規分布に基づく母平均の検定を軸に説明していく。 「区間推定」節の信頼区間の説明でも述べた通り、今回のように正規分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\) の標本平均は以下の分布に従うことがわかっている8。 \\[ \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] また、これまでの議論の通り、\\(\\small \\bar{X}\\)を標準化した統計量Zは以下の分布に従うことが知られている。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] ただし、今回の例においては、 \\(\\small \\bar{X}=1835\\)、\\(\\small= \\sigma=180\\) であることがわかっている。統計的仮説検定においては、この標準化された統計量を検定統計量（検定に用いる統計量）として用いて計算を行うのだが、我々の関心の中心でもある \\(\\small \\mu\\) は未知であり、通常この統計量を計算することはできない。すなわち、未知であるパラメーターを何かしらの値で代替しなければ、上記の検定統計量は計算できない。そこで、統計的な仮説検定では、「帰無仮説が正しいと一旦仮定」した上で統計量を計算するというプロセスを経る。言い換えると、未知のパラメーターについて帰無仮説で示されている値を代入することで、検定統計量を計算可能にする。 電球の例においては、\\(H_0:~\\mu=1700\\)と設計していたため、検定統計量 Z は以下の通りに書き換えることができる。 \\[ Z=\\frac{1835-1700}{\\sqrt{180^2/16}} \\] そして、もし「帰無仮説が正しければ」Zは標準正規分布に従うはずであり、言い換えると Z の計算結果は0に近い値を取る可能性が高いはずである。そこで、この Z を計算し、\\(\\small |Z|\\) がある閾値 c よりも大きい（十分に0から離れている）場合には帰無仮説を棄却する。なお、ここで用いる閾値 c のことを一般的に臨界値と呼ぶ。つまり、検定統計量 Z の計算結果に対して、以下の方針で仮説検定を行うといえる。 \\[ \\begin{cases} |Z|&gt;c &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq c &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 臨界値 c の求め方は区間推定と同様、分析に対応する確率分布（今回であれば標準正規分布）に基づくある区間の確率計算で求まる。研究者はまず、任意の確率 \\(\\small \\alpha\\) を決める。この確率は「有意水準（significance level）」と呼ばれ、この有意水準と標準正規分布に基づく確率計算によって臨界値（下図内では \\(\\small \\pm z_{\\alpha/2}\\)）を求める。その上で、統計量の計算結果が臨界値より外側（下図における斜線部）にある場合には帰無仮説を棄却する。そのため、斜線部のような領域を棄却域、確率 \\(\\small 1-\\alpha\\) に対応する範囲を採択域と一般に呼ぶ。 臨界値と確率計算 また、ここまでの例では対立仮説を \\(H_1:~\\mu\\neq1700\\) とし、左右対称の分布の両端に棄却域を設定した。このような検定方法を一般的に両側検定と呼ぶ。しかし、現実的ないしは理論的な根拠をもとに、ある値よりも高い（もしくは低い）ことが事前に予測できる場合がある。その場合には、例えば \\(\\small \\mu&gt;1700\\) や \\(\\small \\mu&lt;1700\\)といった対立仮説を設定することも可能である。このような対立仮説を利用した検定方法を一般的に片側検定と呼ぶ。ここでは、仮に\\(\\small \\mu&gt;1700\\)という対立仮説を立てた場合を考えるが、\\(\\small \\mu&lt;1700\\)のような対立仮説を設計しても正負を入れ替えることで同様の議論ができる。なお、片側検定において帰無仮説が棄却された場合、直ちに帰無仮説の値よりも大きい値を取ると判断する。しかしながら、たとえ異なる対立仮説を提示しても、採用する検定統計量や帰無仮説に基づく分布の仮定などは同じである。 片側検定を利用した場合の特殊性はその棄却域に現れる。片側検定の場合の棄却域は以下の図のように片側のみとなる。なお、その場合分布の両端に棄却域を設ける必要がないため、正の方向に \\(\\small \\alpha\\) 分の棄却域を設定する。 片側検定（正の場合） ここで再び話を両側検定に戻し、統計的検定において用いられる有意水準について説明する。ここまでの内容をまとめると、帰無仮説を仮定して検定統計量を計算する場合、帰無仮説が正しければ、棄却域内の値を取る確率は \\(\\small 100\\times \\alpha\\)%であると言える。そして、検定統計量の計算結果が棄却域に含まれる場合、帰無仮説を棄却するという判断を下す。そのため、統計的に帰無仮説を棄却したからと言って、その結果が必ず正しいとは言い切れない。統計的検定には、根本的に第一種の誤り（Type 1 error）と第二種の誤り（Type 2 error）という二種類の誤りの可能性が内包されている。 第一種の誤りとは、帰無仮説が真であるにも関わらず、帰無仮説を棄却してしまう誤りである。一方で第二種の誤りは、帰無仮説が真ではないのにも関わらず、帰無仮説を採択してしまう誤りである。例えば、ある薬に期待される効果があるかどうかを検証する場合を考える。この時、帰無仮説は「投薬による効果がない」、対立仮説は「投薬による効果がある」と設計する。この場合における第一種の誤りとは、「本当は効き目のない薬を効くと判断してしまう誤り」であり、第二種の誤りとは、「本当は効き目のある薬を聞かないと判断したしまう誤り」である。 Table 7.4: 検定の誤り H0が真 H0が偽 H0を棄却 Type 1 error ✓ H0を採択 ✓ Type 2 error どちらの誤りも見過ごすことのできないものではあるが、第一種の誤りによる損失と、第二種の誤りによる損失を比較し、一般的な統計的検定においては、第一種の誤りの確率を下げることに注視する。なお、研究によっては下記にある検定力という指標に着目し、第二種の誤りに対応した議論を提示することもあるが、本書では割愛する。仮説検定では特に、第一種の誤りの確率を有意水準 \\(\\small \\alpha\\) と設定し分析する。また有意水準は、先述の通り棄却域の特定に用いられる。つまり、統計的仮説検定とは、帰無仮説が正しいと仮定した上で有意水準 \\(\\small \\alpha\\) の分だけ第一種の誤りの確率を許容したうえで仮説が正しいか否かを確認する作業である。 上記の統計的検定に関わる誤りは、\\(\\small T_0\\) は統計量 T の観測値、Rは\\(\\small H_0\\) の棄却域、Aは\\(\\small H_0\\)の採択域とし、以下のように示される。 有意水準: \\(\\alpha\\) \\[ P(T_0\\in R|H_0~\\text{is True})=\\alpha \\] 第二種の誤りの確率: \\(\\beta\\) \\[ P(T_0\\in A|H_0~\\text{is False})=\\beta \\] 検定力: \\(1 - \\beta\\) \\[ P(T_0\\in R|H_0~\\text{is False})=1-\\beta \\] ここで、先程の新型電球の例に対し、仮説検定に関わる有意水準を \\(\\small \\alpha=0.05\\) と設定し、適応する。帰無仮説が正しいという条件のもとで、帰無仮説を棄却する確率であるため、有意水準は以下のように示すことができる。 \\[ \\alpha=P(|Z|&gt;c|\\mu=1700) =0.05 \\] しかしながら、このままだと確率計算が複雑になるため、上式を以下のように書き換える。 \\[ 1-\\alpha=P(|Z|\\leq c|\\mu=1700) = \\int^c_{-c}f(Z)dZ~_{|\\mu=1700}=0.95 \\] 再掲になるが、上式の関係を表した図が、以下のものになる。 標準正規分布の両側検定 このとき、検定統計量 Z は帰無仮説が正しければ標準正規分布に従うはずである。したがって、臨界値 \\(\\pm c\\) は、\\(\\pm z_{0.025}\\) として分布表などより導出が可能である。Rにおいては前節と同様、qnorm() により、\\(z_{0.025}=1.96\\) だとわかる。すなわち、検定統計量の計算結果が 1.96（-1.96）を上回る（下回る）場合には、帰無仮説を棄却するが、そこには第一種の誤りを犯す確率が5%残されていると解釈できる。 ここまでの議論を踏まえ、新型電球に関する統計的検定を標準正規分布に基づき以下のように実施する。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 mu &lt;- 1700 #Test statistic Z &lt;- (xbar - mu)/(sigma/sqrt(n)) Z ## [1] 2.993056 分析の結果、検定統計量 Z の実現値が5%有意水準に基づく臨界値（1.96）よりも大きいことが示されたため、5%有意水準で帰無仮説が棄却された。つまり、5%の第一種の誤りの確率を残した上ではあるが、新型電球の製品寿命は旧型製品の寿命よりも長いと言える。このような結果は一般的に、「統計的に有意な結果」と表現される。 ここまでは、有意水準の意味を踏まえ、検定の手順及び結果の解釈について説明した。上述の例では、ある有意水準のもと帰無仮説を棄却できる「統計的に有意な」結果を得た。しかしながら、統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。具体的には、統計的に有意でないからと言って、帰無仮説が正しい（つまり \\(\\small \\mu =1700\\) である）と結論づけることはできない。ここまでの説明の通り、有意水準とは第一種の誤りを犯す確率であり、有意水準に基づく統計的検定では主にこの確率に対応した分析を行っている。そのため、第二種の誤りである、本当は \\(\\small \\mu \\neq 1700\\) であるにも関わらず、\\(\\small \\mu =1700\\) と判断している可能性については未対応である。これらの点から「有意でない」ということを理由に、帰無仮説が正しいと結論づけることは適切ではない。そのため、もし今回の仮説検定で帰無仮説を棄却できていなかったとしたら、その結論は「新型電球寿命の平均は1700時間ではないとは言えない」となる。なんとも歯切れの悪い結論だということは理解できるが、統計的検定の特性上、このような解釈を提示しないといけない。 なお、この電球の例のように母集団の分散が既知の場合、検定統計量は標準正規分布に従うと仮定できる。 しかし母集団の分散が未知の場合は、信頼区間での議論と同様、標準偏差の不偏推定量を用いて、自由度 n-1 の t 分布を仮定した分析を行う。そして、t分布に基づく母平均に関する検定を一般的に「t検定（t-test）」と呼ぶ。 なお、t.test() を用いた分析例でも紹介したが、R (他のソフトウェアでも)で統計的検定を実行すると “p-value”（p値）という値を得る。p値については、もう少し詳しい説明が必要であり、解釈にも注意が必要である。 p値は、計算された検定統計量の実現値を臨界値とし、有意水準を計算していると解釈できるが、棄却域と有意水準の関係に基づきもう少し詳細に述べると、以下のように説明することができる（cf. 西山など, 2019）。有意水準を小さく取ると、棄却域は狭くなる。例えば、ある仮説検定において、5%有意水準では帰無仮説を棄却できるが、1%ではできない場合がある。計算された検定統計量の実現値に基づき、有意水準を変えながら検定を行っていくと、これ以上有意水準を小さくすると帰無仮説が棄却されなくなるという有意水準の限界を見つけることができる。この限界をp値と呼ぶ。そのため、p値によって示されている確率は有意水準と同様のものを捉えているのだが、その計算過程が異なるという点において注意が必要である。 ここで改めて、標準正規分布を用いた母平均の検定に着目し、統計的仮説検定についてのより一般的な説明を提示する。\\(\\small X_1,..,X_n\\) を正規母集団 \\(N(\\mu, \\sigma^2)\\) からのサンプルサイズ n の無作為標本とする。このとき、帰無仮説の下でのパラメータの値を\\(\\mu_0\\)として、以下の帰無仮説と対立仮説を設計する。 \\[ H_0:~\\mu=\\mu_0,~~~H_1:~\\mu\\neq\\mu_0 \\] このとき、検定統計量 Z を以下のように定義する。 \\[ Z=\\frac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n}} \\] そのうえで、有意水準 \\(\\small \\alpha\\) に基づく両側臨界値 \\(\\pm z_{\\alpha/2}\\) を設定し、以下の方式で検定する。 \\[ \\begin{cases} |Z|&gt;z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布↩︎ "],["平均値に関するその他の検定.html", "7.8 平均値に関するその他の検定", " 7.8 平均値に関するその他の検定 これまでは、母平均が特定の値を取るか否かに着目し、統計的仮説検定の基礎について説明した。しかしながら本章の冒頭でも例に挙げた通り、平均値をあるグループ間で比較したいと考えることも多い。本節では、期待値の比較に着目し、平均の差の検定と、分散分析について説明する。これらの検定では、用いる検定統計量は先述のものと異なるが、統計的仮説検定そのものの手順や、肝となる考え方は共通である。 前節で考えた、「女性に比べ男性の方が新製品購買意図が高い。」という作業仮説を再度考える。このとき、我々が観察可能なのは男性グループの標本平均（\\(\\small \\bar{X}\\)）と女性グループの標本平均（\\(\\small \\bar{Y}\\)）であるが、検定においてはそれぞれの期待値（\\(\\small \\mu_x\\) と \\(\\small \\mu_y\\)）に着目し、帰無仮説を作成する。なお、\\(\\small X_1,...,X_n\\) は\\(\\small N(\\mu_x,\\sigma^2_x)\\)に従う母集団からの無作為標本であり、\\(\\small Y_1,...,Y_n\\) は\\(\\small N(\\mu_y,\\sigma^2_y)\\)に従う母集団からの無作為標本であるとする。また、\\(\\small X_1,...,X_n\\) と \\(\\small Y_1,...,Y_n\\) は互いに独立であり、母分散は未知であるとする。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_x\\)、女性における購買意図の期待値を \\(\\small \\mu_y\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\[H_0:~\\mu_x=\\mu_y,~~H_1:~\\mu_x\\neq\\mu_y\\] 統計的検定の手順と直感的な検定統計量の作り方は前節の内容と同じである。そのため、検定における推定量と帰無仮説条件下での未知パラメータの値を特定し検定統計量を定義したい。この検定ではグループ間の平均の差を捉えているため、標本上での情報として \\(\\small \\bar{X}-\\bar{Y}\\) という関係を捉える。したがって、上記の帰無仮説と対立仮説は以下のように書き直すことができる。 \\[H_0:~\\mu_x-\\mu_y=0,~~H_1:~\\mu_x-\\mu_y\\neq0\\] また、母分散が未知である場合にはt検定を行うということも前節と同様である。このことから、以下の検定統計量を用いる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})-(\\mu_x-\\mu_y)}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] ただし、\\(s^2\\) は標準誤差と呼ばれる母集団の標準偏差の推定量である。なお、\\(s^2\\)は母分散を捉えた推定量であるが、母分散が両群で等しい（等分散: \\(\\small \\sigma^2_x=\\sigma^2_y=\\sigma^2\\)）である場合には上記の検定統計量を自由度（\\(\\small m+n-1\\)）のt分布として分析可能である。一方で等分散出ない場合には、Welchのt検定と呼ばれる、自由度の計算を修正した分析方法を用いる。なお、Welchのt検定で用いられる自由度の式は複雑なのでここでは省略する。 このとき、帰無仮説が正しいという仮定のもとでは、\\(\\small \\mu_x-\\mu_y=0\\)である。そのため、上記の検定統計量は以下のように観察可能な情報のみで構成される形で書き換えることができる。また、帰無仮説が正しければ、この検定統計量は自由度（\\(\\small m+n-1\\)）のt分布に従うと考えられる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] そのため、データに基づき計算された検定統計量tの実現値を用いて、以下の方式で検定を行う。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(m+n-1) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(m+n-1)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] Rにおいて平均の差の検定を行うことはさほど難しくない。先程の等分散性についても、var.equal=TRUEまたはvar.equal=FALSEという引数で設定できる。var.equal= 引数についてはTRUEが等分散性を仮定するが、デフォルトでは、FALSEとなっている。 平均の差の検定では、 t.test(outcome ~ category) のように、はじめに着目する成果変数を、その後 ~（チルダ）のあとに着目するカテゴリ変数を指示することで、どの変数（outcome）の平均の差をどのカテゴリ変数（category）で検定するのかが特定化できる。ここでは、以前の章で利用したidposデータを利用して、総支出額の平均が男女間で異なるか否かを以下のように分析する。 idpos_cust &lt;- readr::read_csv(&quot;data/idpos_customer.csv&quot;) t.test(monetary ~ gender, data = idpos_cust) ## ## Welch Two Sample t-test ## ## data: monetary by gender ## t = 1.7895, df = 1334.2, p-value = 0.07377 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## -253.4545 5518.7863 ## sample estimates: ## mean in group female mean in group male ## 17511.03 14878.36 t.test(monetary ~ gender, data = idpos_cust,var.equal=T) ## ## Two Sample t-test ## ## data: monetary by gender ## t = 1.6061, df = 1485, p-value = 0.1085 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## -582.7229 5848.0547 ## sample estimates: ## mean in group female mean in group male ## 17511.03 14878.36 分析の結果、等分散性を仮定するか否かで、計算結果は微妙に異なるが、どちらの検定結果においても10%有意水準で帰無仮説を棄却できなかった。そのため、本データにおいて総支出額に関する男女差があるとは言えない。 なおRにおいては、等分散性に関する検定もvar.test(outcome ~ category)で実行可能である。先程のidposデータに関する男女差について、等分散性の分析を以下のように実施する。 var.test(monetary ~ gender, data = idpos_cust, ratio = 1) ## ## F test to compare two variances ## ## data: monetary by gender ## F = 1.9913, num df = 984, denom df = 501, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.706794 2.314410 ## sample estimates: ## ratio of variances ## 1.991318 等分散性の検定では、ひとつのグループの分散ともう一方のグループの分散の比が1（等分散）であるという帰無仮説を設計する。詳細は割愛するが、帰無仮説が正しい場合には両グループの不偏標本分散の比が自由度（\\(m-1\\), \\(n-1\\)）のF分布に従う。分析の結果、帰無仮説は棄却されたため、等分散とは言えないと結論づけることができる。そのため、平均の差の検定においては、Welchのt検定を利用した分析結果を採用して議論することが好ましい。 "],["分散分析.html", "7.9 分散分析", " 7.9 分散分析 ここまでの内容では、２グループ間の平均の差に関する分析を捉えた。しかしながら、三つ以上のグループ間の平均の差に関心があることもある。例えば、異なる地域における売上高の差を比較したい場合が挙げられる。このような目的を持つ場合によく用いられるのが分散分析（Analysis of Variance; ANOVA）である。本節では、ANOVAの実行方法を中心に説明を行う。なお、ANOVAに関するより詳細な説明は別添の補足資料を参照してほしい。 ANOVAの構造については、要因と水準という二つの要素から説明が行われる。要因とは、観測値に影響を与えていると考えうるカテゴリ変数のことを指し、水準とは、要因を構成するいくつかの条件やグループを指す。例えばある小売企業における各店舗の一定期間内の売上高が出店エリア特性によって差があるのかという問いに関心があるとする。その際各店舗を、都市エリア、郊外エリア、農村エリアという三つのグループに分類し、それぞれのグループにおける標本平均を求めれば、エリアごとの差を分析できるだろう。この場合、「地域」が売上高に影響を与えうる要因であり、「都市、郊外、農村」という三つのグループが水準だと言える。 分析において取り上げる要因が一つである分散分析を一元配置分散分析と呼ぶ。二元配置分散分析については本資料では扱わないため、補足資料を参照してほしい。なお、分散分析をRで実行することは難しくない。ここでは、reshape2というパッケージに内包されてる tipping データを用いてANOVAを実行するため、以下のようにreshape2パッケージをインストールして欲しい。 install.packages(reshape2) パッケージをインストールしたら、以下のようにreshape2を起動し、今回使用する tips データを確認する。本データに含まれている変数は以下の通りである。 total_bill: 支払料金（ドル） tip: チップ額（ドル） sex: 支払い者の性別 smoker: グループ内に喫煙者がいるか day: 曜日 time: 時間帯 size: 人数 library(reshape2) str(tips) ## &#39;data.frame&#39;: 244 obs. of 7 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels &quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ time : Factor w/ 2 levels &quot;Dinner&quot;,&quot;Lunch&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... ここでは、以下の通りチップ額が曜日によって異なるか否かを分析する。ANOVAの実行においてはaov()関数によって分析するモデルとデータを指定し、anova() によって分析結果を出力する（summary() を用いることも可能である）。なお、上記のデータサマリーより、day という要因には四水準 (levels) 含まれていることがうかがえる。 s &lt;- aov(tip ~ day, data = tips) anova(s) ## Analysis of Variance Table ## ## Response: tip ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## day 3 9.53 3.1753 1.6724 0.1736 ## Residuals 240 455.69 1.8987 分析結果における Sum Sq は、水準間変動和（i.e.,平方和）と呼ばれ、特定要因の水準間によって説明される観測値の変動を表している。 Mean Sq は平均平方と呼ばれ、Sum SqをDF(自由度)で割ったものである。F value は F値という検定統計量の実現値であり、Pr(&gt;F) は本検定の p値を表している。また、Residuals の行で示されているのは、残差平方和と呼ばれ、郡内変動、つまり同グループ（水準）内での値のばらつきの程度を表している。 分析の結果、チップ額について曜日による統計的に有意な差は確認されなかった。それでは、ANOVAでは具体的にどのような帰無仮説を用いた検定を行っているのだろうか？結論を先に述べると、「すべての水準間で平均値は同じ」という帰無仮説を検定しており、仮に帰無仮説が棄却された場合、「少なくとも一つの水準では値が異なる」という対立仮説を指示する。そのため、ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。そのため、ANOVAの検定結果だけでは具体的にどの水準間に差があるのかはわからない。そこで、ANOVAを用いた研究では事後分析として、多重比較と呼ばれる分析を行うことが多い。しかし、この多重比較には統計的な問題が伴うと言われている（詳細は補足資料参照）。その問題に対応した手法として広く用いられているのが、Tukeyのpair-wise 比較である。Rではこの分析を、TukeyHSD() という関数で実行できる。具体的には、aov() でストアしたANOVAの分析結果を用いて以下のように実施する。また、plot()を用いて、pair-wise比較の結果を95%信頼区間とともに図示化することもできる。 tukey_result &lt;- TukeyHSD(s) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tip ~ day, data = tips) ## ## $day ## diff lwr upr p adj ## Sat-Fri 0.25836661 -0.6443694 1.1611026 0.8806455 ## Sun-Fri 0.52039474 -0.3939763 1.4347658 0.4558054 ## Thur-Fri 0.03671477 -0.8980753 0.9715049 0.9996235 ## Sun-Sat 0.26202813 -0.2976929 0.8217492 0.6203822 ## Thur-Sat -0.22165184 -0.8141430 0.3708394 0.7678581 ## Thur-Sun -0.48367997 -1.0937520 0.1263921 0.1724212 plot(tukey_result) 分析結果における diff 列は平均値の差を表している。 lwer と upr は信頼区間の下限と上限を表しており、一番右の列はp値を示している。分析の結果、P-valueが10%水準よりも低い結果がないため、どのペアに関する検定でも有意な差は確認できなかった。したがって、分析結果は必ずしもチップ額が曜日によって変化するとは言えないことを示した。この結果は、アメリカにおけるチップ額が会計額に対する割合や提供されたサービス品質によって決まるという慣習から考えると妥当な結果である。しかしながら、前節で注意した通り、このような統計的に非有意な結果をもって「曜日はチップ額に影響を与えない」と結論づけるのは不適切である。 なお、今回の分析においてはANOVAもTukeyのpair-wise比較も有意な結論を得ることができなかったという点で、両者の結果に一貫性があった。しかしながら、ANOVAでは有意だが、Tukeyの分析ではどの組み合わせも有意ではないという一見整合でない結果を得ることもある。その場合には、慣習としてTukeyの多重比較結果を優先して解釈を提示することが多い。しかしながら、研究者が自身の実施した検定の「意味」を理解し、解釈や議論を提示することも重要である。例えば、ANOVAとTukeyでは用いている帰無仮説が異なるため、異なる比較対象を用いた検定を実施している。そのため、自身が実行した分析がどのような帰無仮説を採用しており、何と何の比較を行っているのかを正確に把握し、実施した検定の意味に適した解釈や議論を展開する事が重要になる。 本章では、基礎的な統計学の復習として、主に区間推定と統計的仮説検定について説明した。区間推定では、主に信頼区間の計算に着目し、信頼区間の意味についてきちんと理解、解釈することの重要性を強調した。また、統計的仮説検定では、母平均の検定を起点とし統計的検定の基礎的な構造と考え方について説明した。検定においては、母集団の統計的特徴に関する予測である帰無仮説と対立仮説を設計することが重要である。また、グループ間の差異に着目した検定を行う場合には、関心のある未知パラメータについての差や比に着目し検定統計量を作成する事が多いが、基本的な統計的検定の考え方と手順は変わらないという点についても説明した。 "],["参考文献-5.html", "7.10 参考文献", " 7.10 参考文献 浅野正彦・矢内勇生（2018）「Rによる計量政治学」，オーム社. 岩田暁一（1996）「経済分析のための統計的方法 第２版」，東洋経済新報社. 倉田博史・星野崇宏（2011）「入門統計解析」，新世社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 宮川公男（2002）「基本統計学」，有斐閣. "],["regintro.html", "Chapter 8 回帰分析", " Chapter 8 回帰分析 本章では、変数間の関係を捉える回帰分析について、そのモデルの基礎と統計的推測に基づく解釈を説明する。回帰分析結果から得られる含意は、「予測」と「検証」の二つに大別することができる。その上で特に本書では、「検証」という側面、特に「研究上関心のある説明変数の係数の解釈」を重視する立場を取る。立場が異なれば、回帰分析上何を重視するかという観点も異なるため、特に回帰分析による予測に関心のある読者においては別の図書を参照してほしい。 なお、本節では、MktRes_firmdata.xlsxという企業データを用いた分析を行う。次節に移る前に以下の要領でデータを読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) このデータは、小売・サービス分野の企業約160社（企業数は年によって異なる）に関する2010年から2019年までの財務データである（計1440件）。このデータは、日本生産性本部における顧客満足度調査の対象になっている企業リストを作成し、その企業の中から金融領域の企業や、データを入手できなかった一部の企業を教育的意図から排除したものである。したがって、日本の小売・サービス分野において全国的に知名度のある代表的な企業の財務データ（の一部）だと考えられる。 なお、本データには以下の変数が含まれており、データ内の単位は従業員数（人）を除き百万円である。 fyear: 決算年 legalname: 企業名 ind_en: 日経業種名（英文） parent:親会社名（もしあれば） fiscal_month: 決算月 current_liability: 流動負債 ltloans: 長期借入金 total_liability: 負債合計 current_assets: 流動資産 ppent: 有形固定資産 total_assets: 資産合計 net_assets_per_capital: 純資産合計／資本合計 sales: 売上高 sga: 販売費及び一般管理費 operating_profit: 営業利益 net_profit: 当期純利益 pnet_profit: 親会社株主に帰属する当期純利益（連結）／当期利益（単独） re: 利益剰余金 adv: 広告・宣伝費 labor_cost: 人件費 rd: 研究開発費 other_sg: その他販売費及び一般管理費 emp: 期末従業員数 temp: 平均臨時従業員数 tempratio: temp/(emp+temp) indgrowth: 産業成長率 adint: 広告集中率（adv/sales） rdint: 研究集中率（rd/sales） mkexp: (sga - rd) / sales op: operating_profit / sales roa: pnet_profit / total_assets 本データセットは、複数年にわたる複数サンプルからのデータであり、一般的にこのような構造のデータをパネルデータという。パネルデータの分析は本講義の範囲外なので、本章では主に、2019年のデータ抽出し、クロスセクショナルデータとして用いる。以下の様に全データから2019年の情報を抽出してほしい。 library(tidyverse) firmdata19 &lt;- firmdata %&gt;% filter(fyear == 2019) データを用いた分析を行う場合、取得したデータの記述統計や分布を確認する必要がある。本来であれば研究上重要な変数を対象にデータの特徴を整理するが、ここでは複数の変数の特徴を一括で整理、図示化する方法を提示する。この方法では、GGallyというパッケージのggpairs()という関数を用いるため、以下のようにパッケージをダウンロードしてほしい。 install.packages(&quot;GGally&quot;) firmdata19 データセットから、例として四つの変数を抽出して、ggpairsを実行する。これにより、各変数のヒストグラム（密度形式）と、それぞれの変数間の相関係数と散布図が同図内で示されている。また、ggpairs()内の引数設定によって様々な図示形式を指定できるため、興味のある人は調べてみてほしい。 firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% GGally::ggpairs()+ labs(title = &quot;ggpairs example&quot;) なお、記述統計については既出の summary()関数にデータフレームを指定することで、データセット全体の記述統計を出力する。ここでは例として先程と同じ変数の記述統計を以下のように出力してみる。 ds1 &lt;- firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% summary() knitr::kable(ds1, align = &quot;cccc&quot;) sales mkexp emp operating_profit Min. : 11333 Min. :0.01137 Min. : 163 Min. :-40469 1st Qu.: 186830 1st Qu.:0.16714 1st Qu.: 3488 1st Qu.: 7788 Median : 464450 Median :0.25508 Median : 7825 Median : 24824 Mean :1194513 Mean :0.29894 Mean : 20156 Mean : 80811 3rd Qu.:1164243 3rd Qu.:0.37438 3rd Qu.: 24464 3rd Qu.: 63026 Max. :9878866 Max. :0.75650 Max. :160227 Max. :656163 "],["回帰モデル概要.html", "8.1 回帰モデル概要", " 8.1 回帰モデル概要 本章では、二変数間の関係について「予測」と「検証」の二点からより深掘りしていく方法を紹介する。マーケティングを実行する企業の立場に立てば、マーケティングに関連する方策や投資を導入することで成果（売上や利益）を向上させたいと考えるだろう。しかしながら、マーケティングの施策により得られる成果や、そのために投入される労働力、金銭、時間などの資源は、採用する施策案により異なる。そのため、企業も闇雲に意思決定を行うわけには行かず、その意思決定によって、どの程度の成果の向上を見込めるかの予測ができると良い。最も基本的な予測の形式は、ある説明変数（先行要因）が被説明変数（成果）に対してどのような影響を与えるのかを特定し、それに基づき予測値を算出するという方法である。 二変数間の関係を捉える分析手法として6章では相関係数を紹介した。相関係数は二変数間の線形関係を表す -1から 1 の値を取る指標である。 しかしながら、相関係数は線形関係の強さ（どれだけデータが直線上に近く分布しているか）を表しているものの、示されている直線の切片や傾きといった線形関数の特徴は捉えられない。例えば、下図の二つのデータは異なる切片と傾きを有しているが、相関係数は等しくなるはずである。 相関と線形関係 ここで、二つの異なる変数 \\(y,~x\\) の関係を \\(\\small y=f(x)\\) のように\\(y\\)を\\(x\\)の関数（\\(f(x)\\)）で示すというアイディアで分析を行う。このとき \\(y\\) を「被説明変数（Explained variable）もしくは従属変数（dependent variable）」、\\(x\\) を「説明変数（Explaining variable）もしくは独立変数（independent variable）」という。そして、被説明変数と説明変数の関係を特定化した式のことを一般的に回帰モデルという。最も基本的な関数型の特定方法は以下のような一次関数による特定化である。 \\[ y=\\beta_0+\\beta_1x \\] このとき、\\(\\small \\beta_0\\) は切片、\\(\\small \\beta_1\\) は傾きを表す係数であり、回帰係数と呼ばれる。 回帰モデルは線形の関係を捉えているものの、実際にデータを入手し散布図を作成すると、以下のように、直線とは異なる結果を得る。そのため、上記のモデルは正確な表現でないことがわかる。 分析者がデータとして得る情報は、\\(y\\) や \\(x\\) の実現値であり、回帰モデルの切片や傾きの値は直接はわからない。そこで、モデルで捉えた直線と実現値のズレを考え、得たデータから回帰モデルのパラメータ（係数）を推定するという方針をとる。モデルで捉えた直線による（係数の推定値に基づく）\\(y\\) と\\(x\\) の関係は、\\(\\small x=x_i\\) のとき、\\(y_i\\) の予測値である\\(\\small \\hat{y}_i\\)（ワイハット）と、係数の推定値 \\(\\small \\hat{\\beta}_0\\)、 \\(\\small \\hat{\\beta}_1\\) を用いて以下のように定義できる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+ \\hat{\\beta}_1 x_i \\] 係数の推定値 \\(\\small \\hat{\\beta}_0\\) と \\(\\small \\hat{\\beta}_1\\) を求めるための計算方法は、（最尤法や積率法など）いくつかあるものの、本書では最小二乗法（Ordinary least square: OLS）という方法に着目し紹介する。OLS推定量（OLS Estimator: OLSE）の求め方の直感は、以下の図の通り、観測値と回帰直線間の距離の合計（残差平方和）を最小にするように計算される。 OLSE概要 予測値のモデルで示されているのは、データを分析した結果求めたOLSEに基づく説明変数 \\(x_i\\) と、\\(\\small \\hat{y}_i\\) との関係である。\\(\\small \\hat{y}_i\\) は被説明変数 \\(y_i\\) の「予測値（predicted value）」や「理論値（fitted value）」と呼ばれるものであり、\\(y_i\\) の観測値とは異なる値であることに注意が必要である。このとき、観測値と予測値のズレ（\\(\\small y_i-\\hat{y}_i\\)）を「残差（residual）」という。OLSは残差を \\(\\hat{u}_i\\) とし、以下で示される、残差平方和（二乗和）を最小にするように推定量を求める方法である。なお、OLSEの計算仮定については、秋山（2018）が詳しく説明をしてくれている。 \\[ \\sum_{i=1}^n\\hat{u}_i^2=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2=\\sum_{i=1}^n\\left(\\hat{y}_i-(\\hat{\\beta}_0+ \\hat{\\beta}_1 x_i)\\right)^2 \\] これを解くと、以下のような推定量を得る。 \\[ \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x} \\] \\[ \\hat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\] Rによる回帰分析は、lm()という関数（linear model）を用いて簡単に実行できる。この関数内では、lm(y ~ x, data = df) という要領で、説明変数と被説明変数を \\(\\sim\\)（チルダ）で繋いでモデルを指定する。例えば、先程の企業データにおける2019年の観測を用いて、従業員数と売上高の関係について分析するためには、以下のように分析を実行する。 reg1 &lt;- lm(sales ~ emp, data = firmdata19) coef(reg1) ## (Intercept) emp ## 22809.67863 58.13194 分析の結果、定数項（Intercept）は 22809.7 で、傾きは 58.1 であることがわかった。つまり、従業員数を一単位増やすと、売上高が58.1（百万円）増えることを示唆している。仮に従業員数が10人であれば、売上高の「予測値」は以下のように計算できる。 \\[ 22809.7+58.1\\times 10=23390.7 \\] 回帰分析によって被説明変数の予測値を計算が可能なことを説明したが、この予測値は実際の観測値とは異なる。では、ここで求められた予測値はどのように解釈できるものなのだろうか。それを理解するために、残差と予測値に関する以下の四つの性質を紹介する。 残差の和は0： \\[ \\sum_{i=1}^n\\hat{u}_i=0 \\] 残差と説明変数の積和は0： \\[ \\sum_{i=1}^n x_i\\hat{u}_i=0 \\] 1と2より： \\[ \\sum_{i=1}^n \\hat{y}_i\\hat{u}_i=0 \\] 予測値の平均と観測値の平均は等しい: \\[ \\bar{y}=\\bar{\\hat{y}} \\] 回帰直線は（\\(\\bar{x}, \\bar{y}\\)）の座標を通る: \\[ \\bar{y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x} \\] つまりOLSでは、回帰直線と各観測値のプラス方向のズレとマイナス方向のズレが釣り合う（残差の和が0）ような予測を行っている。その上で、予測値は、説明変数 \\(x\\) が与えられたとき、被説明変数が「平均的に」どんな値を取るのかを示していると解釈できる。回帰モデルと平均との関係については次節で確率的側面からより詳しく説明を加える。 回帰分析による予測による精度を調べるために、分析したモデルがどの程度被説名変数全体の分散を説明しているかという指標によってモデルの適合度を測る。一般的には、決定係数（\\(\\small R^2\\)）という指標によってモデル適合度が示される。\\(\\small R^2\\) は以下のように定義される。 \\[ R^2=1-\\frac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y})^2}=\\frac{\\sum(\\hat{y}_i-\\bar{y})^2}{\\sum(y_i-\\bar{y})^2} \\] この指標は、被説明変数の分散を説明変数がどの程度説明するかの割合を表しており、0以上1以下の値を取る。例えば \\(\\small R^2\\) が0.80であるならば、被説名変数の変動の80%をモデルが説明しているということになる。そのため、\\(\\small R^2\\) は、回帰モデルの説明力として解釈される。しかしながら、予測という目的に対して近年は、機械学習などの発展的な手法が応用される事が多く、\\(\\small R^2\\) を軸に予測を行うことは少なくなってきている。 また、予測ではなく説明変数の効果（係数）についての検証や解釈に関心がある場合、回帰分析における \\(\\small R^2\\) の重要性は低くなる。特に、ビジネス分野における研究では、係数の推定や検定に焦点をあわせることが多い。本書においても、予測よりも係数に関する検証を重視する立場を取る。社会科学領域での分析では、\\(\\small R^2\\) が低くなることは珍しくない。そんな中で、「\\(\\small R^2\\) が低いからその回帰分析結果は意味がない」ということにはならない。研究者の目的が、関心のある変数同士（例、市場志向と企業成果）の関係性（有意性や影響の強さ）を検証したいというものである場合、仮に \\(\\small R^2\\) が低くても、きちんと両変数の関係を分析できる調査設計や分析を実行しているならば、その検証は有意義なものになる。つまりここで強調したいのは、係数の検証や解釈を重視して研究を行う場合、「\\(\\small R^2\\) がいくつ以上（以下）だから良い（ダメ）」という議論は目的と整合的ではなく、重要ではなくなるということである。 本節では、OLSを中心にデータから回帰係数を推定するプロセスに目を向け、予測値と決定係数について紹介した。しかしながら、先述の通り我々は多くの場合特定の変数が成果変数に与える影響の検証に関心がある。次節では確率的な視点から理論的に回帰分析を理解する事により、回帰分析の結果の解釈についてより詳しく学ぶ。 "],["回帰分析における推定と検定.html", "8.2 回帰分析における推定と検定", " 8.2 回帰分析における推定と検定 回帰分析によって係数を推定すると、それが真の（母集団での）値であると勘違いしてしまう人もいる。しかし、分析の結果傾きの係数が正の値だったとしても、母集団においては0と相違がないかもしれない。したがって、説明変数が被説明変数に与える影響を検証するためには回帰係数を計算するだけでは不十分であり、区間推定や検定を行う必要がある。そこで本節では回帰分析に関わる理論的な説明を行う。以下は、我々の着目する回帰モデルである。 \\[ y_i = \\beta_0+\\beta_1x_i+u_i, \\] ただし、\\(\\small u_i\\) は誤差項、\\(\\small \\beta_0\\)切片、\\(\\small \\beta_1\\)は傾きのパラメータである。つまり、このモデルは母集団での統計的特徴を反映した理論的モデルだと理解できる。なお、通常レポートや論文内に回帰モデルを記載する際には、上記のような回帰パラメータと誤差項を含んだ理論モデルを用いる。理論的なモデルは \\(\\small y\\) を説明する要素として確率的な誤差項 \\(u\\) を含んでおり、被説明変数 \\(\\small y\\) は確率変数として捉えられる。一方で、説明変数 \\(\\small x\\) は、定数として扱う。また、前節で紹介した回帰係数の推定量（OLSE）は、 \\(\\small y\\) や \\(\\small \\bar{y}\\) を含んでいる ため、推定量（OLSE）もまた確率変数であると考えられる。 \\(y\\) の値が確率的にバラつくと考え誤差項を含んでいるのだとしたら、回帰直線は何を表しているのだろうか。理論的には、ある \\(x\\) の値が与えられたとき、\\(y\\) の「平均値（期待値）」と \\(x\\) の間には線形の関係があることを捉えている。より具体的には、回帰直線は以下のように \\(x\\) を所与とした際の \\(y\\) の条件付き期待値として表される。 \\[ E(y|x)=\\beta_0+\\beta_1x \\] この関係は、以下の図のように示すことができる（Wooldridge, 2013）。 条件付期待値としての回帰直線 したがって、回帰モデルは、ある \\(x\\) の値に基づき \\(y\\) の期待値（平均）示してくれるが、具体的にどのような値を取るかは確率的に決まるものだと言える。つまり、回帰分析の結果によって言えることは、例えば、「身長（\\(x\\)）の高い人は、\\(\\color{red}{\\text{平均的に}}\\)重い（\\(y\\)）」ということである。 なお、モデル化に際して誤差項について以下のようないくつかの仮定が存在する。なお、5つめの仮定は回帰係数の検定の際に用いられる。 \\(E(u)=0\\) \\(E(u|x)=0\\) \\(Var(u)=E(u^2)=\\sigma^2\\) \\(Cov(u,x)=E(xu)=0\\) \\(u\\) は正規分布に従う このような仮定に従うOLSEは、以下の統計的性質を持つ。 不偏性: \\(E(\\hat{\\beta})=\\beta\\) 漸近的性質: \\(\\hat{\\beta}\\) は、サンプルサイズ \\(n\\) が十分大きいときには正規分布 \\(N(\\beta,~se(\\hat{\\beta})^2)\\) に従う（\\(se(\\hat{\\beta}\\))は、OLSEの標準誤差）。 先述の通り、推定されたOLSEは、モデルの真の値ではない。そのため、仮に分析の結果得た推定値が正の値であっても、母数においては0と大差がないかもしれない。そのため、統計的検定や推測を用いて、未知パラメータに対する検討を行う。なお、Rで回帰係数の検定結果を得るのは非常に簡単である。 lm() 関数の実行結果をストアしたオブジェクトに対して、summary() 関数を実行することで統計的検定結果を得ることができる。先程分析した reg1 を再度利用すると、以下のような結果を得る。 summary(reg1) ## ## Call: ## lm(formula = sales ~ emp, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1835142 -280706 -34523 134095 3292333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22809.679 88556.098 0.258 0.797 ## emp 58.132 2.559 22.721 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 872800 on 145 degrees of freedom ## Multiple R-squared: 0.7807, Adjusted R-squared: 0.7792 ## F-statistic: 516.2 on 1 and 145 DF, p-value: &lt; 2.2e-16 回帰係数の推定と検定に関する結果は Coefficients: の下に記載されている。推定・検定結果は行列形式で表示されており、Estimate の列は回帰係数の推定結果、Std. Error は標準誤差（詳細は省略するが、誤差項の分散推定量の平方根）、 t valueはt値、 Pr(&gt;|t|)はp-value をそれぞれ示している。そして、出力結果下欄には決定係数（R-squared）や自由度調整済み決定係数（Adjusted R-squared）、F検定結果、といったモデル適合度に関する結果が提示されている（詳しくは次節で説明する）。 上記の結果を解釈するために、回帰分析における検定について説明する。ソフトウェアで自動的に出力される統計的仮説検定は、基本的には以下の帰無仮説と対立仮説を採用したものである（添字は省略）。 \\[ H_0:\\beta=0,~~H_1:\\beta\\neq0 \\] なお、R以外のソフトウェアを用いて回帰分析を実行しても係数に関する検定結果を返すが、通常はこの帰無仮説を採用した検定結果を出力する。 検定では、以下のような検定統計量を用いる。 \\[ t=\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} \\] \\(H_0\\) が正しいと仮定する（\\(\\small \\beta=0\\)）と、検定統計量 t は計算可能であり、自由度（\\(\\small n-2\\)）のt分布に従う。検定の手順は 7.8 節で紹介したのと同様、有意確率に基づく臨界値を定めた後、t 値を計算し、棄却域と採択域のどちらに入るのかを確認する。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(n-2) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(n-2)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] これを踏まえて分析結果を確認すると、emp が sales に与える影響（係数: 58.132）は有意に0とは異なると理解できる。また、切片の係数（Intercept）は大きな値を取っているが、統計的には0ではないとは言えないことが示されている。この項は、従業員数が0のときの企業の売上を示しており、この結果が統計的に有意ではないということは、我々の直感とも整合的である。 上記の検定によって、どうやら emp の係数は0ではなさそうだということが伺えた。しかし、具体的にどのような値を取るのだろうか。おおよその値だけでも把握したいのが人情である。そこで、信頼区間を求め、おおよその確率（95%など）で真のパラメータが含まれている区間を確認したい。OLSEの漸近的性質と中心極限定理により、サンプルサイズが十分に大きいとき、先述の統計量 t は標準正規分布にに近づく（西山ほか,2019）。 そのため、7.6節で紹介した、標準正規分布に基づく信頼区間の推定を応用できる。信頼係数を \\(\\small \\alpha\\) とすると、以下の確率と区間の対応関係を得る。 \\[ P\\left(\\left|\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})}\\right|\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] そして、上記を \\(\\small \\beta\\) に関する不等式に変換すると、以下の信頼区間を得る。 \\[ P(\\hat{\\beta}-se(\\hat{\\beta})\\cdot z_{\\alpha/2}\\leq\\beta\\leq\\hat{\\beta}+se(\\hat{\\beta})\\cdot z_{\\alpha/2})=1-\\alpha \\] したがって、\\(\\small [\\hat{\\beta}\\pm se(\\hat{\\beta})\\cdot z_{\\alpha/2}]\\) という観察可能な情報によって信頼区間推定できる。Rによって信頼区間を得るには、回帰分析の結果に対して、confint() 関数を用いる（デフォルトで95%信頼係数が設定されている）。例えば、先程の reg1の結果を用いて、99%信頼区間を得ると、以下のような結果を得る。 confint(reg1,level = 0.99) ## 0.5 % 99.5 % ## (Intercept) -208335.9722 253955.32943 ## emp 51.4537 64.81018 したがって、emp の99%信頼区間が [51.45, 64.81] であることがわかった。すなわち、企業の従業員が一名多いと、売上高が 51から64 百万円高くなりそうだと解釈できる。一方で、(Intercept) の信頼区間には0を含んでいることが伺える。なお、confint() 関数によって計算される信頼区間の計算では上述の通り正規分布が仮定されており、詳しくはヘルプ（?confint）で確認できる。 "],["重回帰モデル1.html", "8.3 重回帰モデル1", " 8.3 重回帰モデル1 ここまでは、回帰分析の概要や係数の検定・推定について説明した。回帰分析を実行することで得る情報は前節の内容がほとんどなのだが、モデルの特定化に関して、もう一つ重要な点が存在する。それが本節で扱う重回帰モデル（multiple regression model）の採用である。重回帰モデルとは、二つ以上の説明変数を含む回帰モデルのことである。一方で、前節で扱ったような説明変数が一つの回帰モデルのことを単回帰（simple regression model）という。回帰分析を用いた研究を行う際には、基本的に単回帰分析ではなく、重回帰分析を実行することが好ましい。通常の分析においては、ある被説明変数に対して考慮すべき説明変数は一つだけではなく、複数の説明変数を考慮すべき状況が多い。しかし、分析に不慣れ学生においては、複数の説明変数に関心がある場合であっても、複数の単回帰モデルを分析することで、それぞれの変数についての分析結果を得ようとすることが散見される（例えば、三つの説明変数の影響を捉えるために単回帰モデルを三本分析する等）。しかしながら本書は、基本的にはこのような分析アプローチは好ましくなく、複数の説明変数を含めた一本の重回帰分析を実施すべきだと主張する。本節では、この主張の理由と、重回帰モデルの特徴・結果解釈について説明していく。 8.3.1 重回帰モデル概要 ある成果変数を説明するために、複数の説明変数が必要になることは、マーケティングリサーチにおいても珍しいことではない。例えば、ある製品のパフォーマンスを月次売上高で測るとする。マーケティング部門として、売上高に対してプロモーション施策がどれだけ貢献しているかを分析する際、プロモーションと売上高の関係を回帰分析で捉えるというアプローチが実現可能な分析方法として考えられる。しかしながら、売上高を説明する変数として、プロモーションだけで十分だろうか。マーケティング変数に着目するだけでも、価格や製品品質、流通網など、異なる変数が売上に関係していることが考えられる。例えば、一見プロモーションによる効果のような結果を得たとしても、実際には同時期に実行していたディスカウント（価格）の影響であり、プロモーションそのものにはあまり効果がないかもしれない。そのため、他の要素の影響を排除した上での純粋なプロモーション効果を明らかにすることは務的有意義な研究課題となりうる。そしてこのような研究課題に対応する分析方法が、重回帰分析である。本節ではまず、重回帰モデルに関する特徴を整理する。 売上とマーケティング変数 重回帰分析においても単回帰同様、回帰モデルを記述する。k 個の説明変数を含む重回帰モデルは、以下のように示される。 \\[ y_i = \\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+...+\\beta_kx_{ki}+u_i \\] 論文やレポート内に重回帰モデルを記載する際にも、多くの場合上記の誤差項を含む理論モデルを用いる。 以下ではまず、重回帰モデルの係数、予測値や、残差に関する性質について説明する。係数の推定は、以下のような行列モデルで捉えることで、単回帰モデルと同様OLSで求められることができる（詳細は省略）。 \\[ Y = X&#39;\\beta + u \\] \\[ \\hat{\\beta}=(X&#39;X)^{-1}X&#39;Y \\] 重回帰分析を実行すると、各説明変数に対応する係数が推定される。各OLSE（\\(\\small \\hat{\\beta}\\)）は 未知パラメータ（\\(\\small \\beta\\)）の不偏推定量である。また、それらの検定や区間推定では、各変数に対応する係数の検定・推定を個別に行う。OLS推定に関わる残差と予測値はそれぞれ以下のように意義される。 予測値: \\[ \\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki} \\] 残差: \\[ \\hat{u}_i=y_i-\\hat{y}_i =y-( \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki}) \\] そして、残差は以下の \\(k+1\\) 個の制約を満たす。 \\(\\sum_{i=1}^n\\hat{u}_i=0\\) \\(\\sum_{i=1}^n x_{1i}\\hat{u}_i=0,~\\sum_{i=1}^nx_{2i}\\hat{u}_i=0...,~\\sum_{i=1}^nx_{ki}\\hat{u}_i=0\\) そのため、重回帰モデルの残差の自由度は \\(n-(k+1)\\) となる。 8.3.2 重回帰分析におけるモデル適合度 単回帰モデルにおけるモデル適合度指標として前節では決定係数を紹介した。しかしながら、この指標は致命的な欠陥を有している。それは、モデルに含む説明変数の数が増えると決定係数も上昇する（より正確には、説明変数の数に対して非減少）ということである。つまり、被説名変数と全く関係ない変数をモデルに加えても、決定係数は上昇し、そのモデルの説明力が高いという結論に至ってしまう。そのため、通常の決定係数から説名変数の数を調整した指標である調整済み決定係数（Adjusted R-squared: \\(\\bar{R}^2\\)）を用いて適合度を検討する。この指標は、以下のように定義される。 \\[ \\bar{R}^2= 1 - \\left(\\frac{\\sum(y_i-\\hat{y}_i)}{n-k-1}\\cdot \\frac{n-1}{\\sum(y_i-\\bar{y})}\\right) \\] モデルの適合度を考えるもう一つの分析として、Rの分析結果で出力されていたF検定について説明する。回帰分析の結果として必ず出力されるF検定は、重回帰モデルにおける\\(\\small \\beta_0\\)（定数項） 以外の係数が全て0であるか否かをチェックする検定である。この検定では、k個の説明変数を含む回帰モデル（\\(\\small y_i = \\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+...+\\beta_kx_{ki}+u_i\\)）に対して（これをフルモデルと呼ぶ）、以下のような帰無仮説と対立仮説を用いた検定を行う。 \\[ H_0:~\\beta_1=...=\\beta_k=0,~H_1:\\text{少なくともどれか一つの係数は0ではない} \\] 帰無仮説が正しいと仮定した場合、重回帰モデルは以下のようになり、この定数項のみのモデルをモデル0と呼ぶ。 \\[ y_i = \\beta_0+e_i \\] そして、フルモデルとモデル0の残差平方和の比を取った統計量は、帰無仮説が正しいときには自由度（\\(k,~n-k-1\\)）の F分布に従うことが知られている。この性質を活かし、回帰分析においては自由度（\\(k,~n-k-1\\)）のF分布を前提とした統計検定を行い、それをF検定（F-test）と呼ぶ。回帰分析結果にて出力される F-statistic:は、この検定統計量の実現値である。なお、この検定の検定統計量は、以下のように示される。 フルモデルとモデル0の残差平方和（\\(SSR_1\\)と\\(SSR_0\\)）をそれぞれ以下のように定義する。 \\(SSR_1=\\sum\\hat{u}^2_i=\\sum\\left[y-( \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki})\\right]^2\\) \\(SSR_0=\\sum\\hat{e}^2_i=\\sum(y_i-\\bar{y})^2\\) そして、以下の統計量 F は帰無仮説が正しければ、自由度（\\(k,~n-k-1\\)）の F分布に従うことが知られているため、これを検定統計量として用いて検定を行う。 \\[ F=\\frac{(SSR_0-SSR_1)/k}{SSR_1/(n-k-1)}=\\frac{SSR_0-SSR_1}{SSR_1}\\cdot\\frac{n-k-1}{k} \\] "],["重回帰モデル2.html", "8.4 重回帰モデル2", " 8.4 重回帰モデル2 8.4.1 回帰係数の解釈 ここからは、重回帰分析の係数の解釈について説明する。ここで説明する解釈は、なぜ基本的には重回帰モデルを採用すべきなのか、を理解するために重要な内容である。結論から述べると、重回帰分析における説明変数の係数は、同モデル内の「他の変数の効果をコントロールしたうえで」説明変数が被説明変数へ与える影響を表している。そして重回帰分析のこの特徴が、学術的にも実務的にも重要な含意を与えうる分析手法として機能する。 重回帰モデルにおける各説明変数の係数は、パーシャル効果として解釈できる。以下では、このパーシャル効果の直感について、Wooldridge（2013）を参考に説明する。まず、以下のような説明変数が二個である重回帰モデルを考える。 \\[ y_i=\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+u \\] そして、上モデルの予測値は以下のように示すことができる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i} \\] このとき、説明変数 \\(\\small x_1\\) と \\(\\small x_2\\) の変化を \\(\\small \\Delta x_{1i}\\) と \\(\\small \\Delta x_{2i}\\) とすると、予測値の変化（\\(\\small \\Delta \\hat{y}\\)）は以下のように表すことができる。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i}+\\hat{\\beta}_2\\Delta x_{2i} \\] ここで、\\(\\small x_2\\) を固定（\\(\\small \\Delta x_{2i}=0\\)）すると、以下を得る。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i} \\] つまり、重回帰モデルにおける \\(\\hat{\\beta}_1\\) は、別の説明変数をコントロール（\\(\\small \\Delta x_{2i}=0\\)）した上で、\\(\\small x_1\\) が \\(\\small \\hat{y}\\) に与える影響（\\(\\small x_1\\) が変化した際の \\(\\small \\hat{y}\\)の変化の程度）を捉えていると解釈できる。また、\\(\\small \\hat{\\beta}_2\\) についても同様に解釈できる。そしてこの特徴は、k個の説明変数を用いたモデルにも同様に適応できる。なお、パーシャル効果に関するもう少し詳細な説明は本書では割愛する。 では、このパーシャル効果という重回帰モデルの特徴は、どのように応用できるのだろうか。多くの実証研究では、重回帰モデルの特徴を利用し、「コントロール変数」を用いた分析を行っている。本節では、先程の企業データを用いて、「企業の広告支出が営業利益に与える影響を明らかにする」という問いを考える。まずは、学習的意図から以下のように単回帰分析を実施してみる（通常の論文・レポートであればこのようなプロセスを記載する必要はない）。 reg2 &lt;- lm(operating_profit ~ adv, data = firmdata19) summary(reg2) ## ## Call: ## lm(formula = operating_profit ~ adv, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -450515 -55314 -40160 -1096 599313 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.685e+04 1.050e+04 5.414 2.49e-07 *** ## adv 1.258e+00 2.152e-01 5.846 3.20e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 117200 on 145 degrees of freedom ## Multiple R-squared: 0.1907, Adjusted R-squared: 0.1852 ## F-statistic: 34.18 on 1 and 145 DF, p-value: 3.197e-08 confint(reg2) ## 2.5 % 97.5 % ## (Intercept) 3.609692e+04 77602.573216 ## adv 8.325719e-01 1.683065 分析の結果、広告支出（adv）の係数は正に有意であり、その95%信頼区間は [0.83, 1.68] であることが確認できた（8.325719e-01は0.8325719）。 しかしながら、このモデル化は不十分であり他の要素も考慮すべきである。営業利益に影響を与えうる要因は色々とあり 、実際の研究においては先行研究を参照しつつ、コントロールすべき変数を含める形で回帰モデルを特定する必要がある。しかしながら、ここでは便宜上いくつかの要因にのみ焦点を合わせて簡単に特定化する。本データは主に小売・サービス産業の企業に焦点を合わせている。そのため、対人サービス水準は企業のパフォーマンスに影響を与えうる要因である。そのため、従業員に関する変数（従業員数: emp、パートタイム従業員数: temp）と人件費（labor_cost）をモデルに含める。また、資産合計（total_assets）、研究開発費（rd）もモデルに含める。今回の回帰モデルは以下のように示される。 \\[ \\text{opretating_profit}_i = \\beta_0 + \\beta_1 adv_i + \\beta_2emp_i+\\beta_3temp_i+\\beta_4\\text{labor_cost}_i+\\beta_5\\text{total_assets}_i+\\beta_6rd_i+u_i \\] Rにおいて重回帰分析を実行するのは簡単である。lm(y ~ x1 + x2 + x3) のように \\(+\\) 記号と変数を追加すれば、重回帰モデルとして分析を実行してくれる。 reg3 &lt;- lm(operating_profit ~ adv + temp + emp + labor_cost + total_assets + rd, data = firmdata19) summary(reg3) ## ## Call: ## lm(formula = operating_profit ~ adv + temp + emp + labor_cost + ## total_assets + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -360570 -27367 -14970 3349 284990 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.168e+04 8.056e+03 2.691 0.00800 ** ## adv -1.430e+00 2.946e-01 -4.852 3.22e-06 *** ## temp -1.866e+00 6.292e-01 -2.965 0.00356 ** ## emp -1.489e+00 7.008e-01 -2.125 0.03533 * ## labor_cost 8.770e-01 1.686e-01 5.202 6.86e-07 *** ## total_assets 3.507e-02 5.823e-03 6.023 1.43e-08 *** ## rd 1.380e+00 5.254e-01 2.627 0.00957 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78740 on 140 degrees of freedom ## Multiple R-squared: 0.6474, Adjusted R-squared: 0.6323 ## F-statistic: 42.84 on 6 and 140 DF, p-value: &lt; 2.2e-16 confint(reg3) ## 2.5 % 97.5 % ## (Intercept) 5749.50978947 3.760568e+04 ## adv -2.01201729 -8.470087e-01 ## temp -3.10966464 -6.216177e-01 ## emp -2.87467416 -1.038058e-01 ## labor_cost 0.54369783 1.210293e+00 ## total_assets 0.02355591 4.657974e-02 ## rd 0.34159701 2.419018e+00 見ての通り、結果の出力方式も単回帰分析のものとほぼ同様である。回帰係数の結果下にあるモデル適合度については前のサブセクションを参照して欲しい。 分析の結果、広告支出の係数は「負」に有意であり、その信頼区間も [-2.01, -0.84] であった。したがって、本データの分析によると、労働や資産に加え研究開発といった側面を一定とすると、広告支出は営業利益に負の影響を与えることがわかった。他の変数に着目すると、従業員数に関する変数はどちらも負に有意であった。一方で、人件費と総資産、研究開発費は正に有意な影響を与えることが示された。これらの結果から、単純に従業員数を増やしても営業利益には負の影響を与える一方で、従業員数を一定とした上で人件費を上げるほうが営業利益が高いことが示された。また、資産や研究開発費も営業利益につながることが示された。 このように、重回帰モデルを採用し複数の説明変数を含めることで、各係数の持つ含意が大きく変わることに注意して欲しい。また、reg2 と reg3の比較のように、特定の説明変数に対応する係数の符号が変わることも珍しくない。そのため、回帰モデルの定式化には非常に慎重になる必要があり、先述の通り、先行研究を参照して必要な変数をコントロールすることが求められる。 また、これらの結果を踏まえて、3.1節で強調した「マネジメントとリサーチの分離」の重要性を思い出して欲しい。あなたが reg2 と reg3の分析を実施したリサーチャーであり、本リサーチのクライアントがマーケティング・広告部門の部長だっとしよう。そして、（1）あなた自身もマーケティング部門の社員であり、クライアントは直属の上司、（2）あなた自身はクライアントとは無関係の立場、という二つの異なる立場に立っている状況を想像して欲しい。（1）の場合、逆恨みによるあなた自身への不利益を恐れて重回帰分析の結果を「ありのまま」伝えられないかもしれない。もちろん研究倫理に基づけば、都合の良い研究成果を作為的に発表することは問題であり、そのようなことはすべきではない。したがって研究成果を「ありのまま」を伝えるべきであり、都合のいい結果を発表する決断をした人（リサーチャー）が悪い。しかしながら、その背後にある利害関係を鑑みれば、reg2 の結果を報告したくなる人がいることも理解できる。個人の判断を批判するだけでなく、このような研究不正を働く誘因や構造についても考慮すべきである。例えば、上記のようなリサーチャーをとりまく利害関係には気をつけなければならない。なお、定量的な分析に慣れている研究者からすると、特別な理由がないにも関わらず重回帰でなく単回帰分析を用いるのは不自然な分析アプローチである。そのため、 もし調査・分析の設計上意図的に単回帰分析を実施する場合には、その意図と有用性をきちんと説明すると良い。 8.4.2 重回帰モデルにおける変数選択 重回帰モデルでは、複数の説明変数を採用することができるため、どの変数をモデルに含めるべきかという点を考察しモデルを特定化する必要がある。このような説明変数の選択に関わる問題として、本サブセクションでは、欠落変数バイアス、多重共線性、過剰統制の三つを紹介する。まず、欠落変数バイアスについてだが、本来含めるべき変数を含めずにモデルを定式化し推定を行うと、推定された係数にバイアスが生じ、OLSEが不偏推定量でなくなるという問題が生じる。また、このバイアスの方向（正負）は欠落された変数と被説明変数との真の関係（欠落された変数が被説明変数に対して持つ母集団での回帰係数）と、モデルに含まれる説明変数と欠落変数との相関で決まる（Wooldridge, 2013）。以下で欠落変数バイアスについての簡単な説明を提示する。 はじめに、以下の式が正しいモデルだと仮定する。 \\[ y=\\beta_0+\\beta_1x_1+\\beta_2x_2+u \\] これに対して\\(x_2\\)を含まずに欠落変数モデルを推定した場合、以下のような結果を得る。 \\[ \\tilde{y}=\\tilde{\\beta}_0+\\tilde{\\beta}_1x_1 \\] このとき、真のモデルの推定値（\\(\\small \\hat{\\beta}_1\\)）と \\(\\small \\tilde{\\beta}_1\\) の間には、以下の関係がある。 \\[ \\tilde{\\beta}_1=\\hat{\\beta}_1+\\hat{\\beta_2}\\tilde{\\delta_1} \\] ただし、\\(\\small \\tilde{x}_2=\\tilde{\\delta}_0+\\tilde{\\delta}_1x_1\\)とする。つまり、\\(\\small \\tilde{\\delta}_1\\) は \\(\\small x_1\\) の \\(\\small x_2\\) に対する回帰係数の推定値である。したがって以下のように、欠落変数モデルでは推定結果に\\(\\small \\beta_2\\tilde{\\delta}_1\\) の分だけバイアスが生じる。 \\[ E(\\tilde{\\beta}_1|x_1,x_2)=E(\\hat{\\beta}_1)+E(\\hat{\\beta_2} \\tilde{\\delta_1})=\\beta_1+\\beta_2\\tilde{\\delta}_1 \\] そして、欠落変数によってバイアスが生じるということが、重回帰分析の重要性を主張する根拠となる。そのため、仮に研究課題上ではあまり重要でない変数であっても、自身の関心のある説明変数の影響を分析するためにコントロール変数をモデルに含めることが重要になる。 第二の論点として、単回帰分析の際には存在しなかった多重共線性（multicollinearity）という問題がある。これは説明変数同士の相関が高いことによる推定上の問題であり、Variance Inflation Factor という指標9を使ってその程度を測ることもある。また、マーケティング領域の研究においてはまれに「多重共線性があるから、説明変数を除外すべきだ」という主張を聞くことがある。しかしながら、本書はできる限り説明変数の除外を行わないほうが良いという立場を提示する。詳しくは後述するが、分析の対象となる説明変数によって極端にVIFが高くなる場合には、次の段落にあるようにその変数の意義を検討し、その変数をそのまま残す、何かで比を取る、対数化する（詳しくは次章）などの変数変換の工夫をしながら、その変数をモデルに含めるということが現実的かつ実践的な対応となる。なおこの立場は、本書が回帰分析における係数の検証や解釈に注視していることに大きく依存するため、注意してほしい。 多重共線性について語る際には、(1) 説明変数同士が完全に相関している（相関係数が 1 もしくは -1）場合と、(2) 完璧ではないが相関係数が高い場合、という二つの異なる状況を区別し理解する必要がある。まず一つ目の場合、そもそも係数の推定値が計算できないという問題が生じる。そのため、完璧に相関しあっている変数を同時にモデルに含めることは出来ず、変数の除外を考えないといけない。この問題は言い換えると「同じ変数を同モデル内に複数入れてはいけない」という制約だと理解できる（西山ほか, 2019）。では（2）の場合にはどのような問題が生じるのか。結論から述べると、説明変数同士の相関は、推定量の分散 \\(\\small Var(\\hat{\\beta}_j)\\) を高めてしまう（Wooldridge, 2013）。しかし、 \\(\\small Var(\\hat{\\beta}_j)\\) は、多重共線性だけでなく以下の三つの要素から影響を受ける。第一に、誤差項の真の分散（\\(\\small \\sigma^2\\)）、第二に独立変数間の相関10、第三に、独立変数の変動（\\(\\small SST_j=\\sum(x_{ji}-\\bar{x}_j)^2\\)）である。第一と第二の指標が高い場合には、推定量の分散は大きくなる。一方で、第三の要素である\\(\\small SST_j\\) が高い場合には \\(\\small Var(\\hat{\\beta}_j)\\) は小さくなる。そのため、\\(\\small Var(\\hat{\\beta}_j)\\) を改善するためには、サンプルサイズを大きくし\\(\\small SST_j\\) を大きくすることも有用な対処方法となる。しかしながら、社会科学分野においてはデータ取得可能性の観点からそれが難しいことも多い。そのときには、推定量分散の増加と説明変数除外による弊害とのトレードオフを考慮して意思決定することになる。そのうえで本書は、先述の通りできる限り説明変数を除外することは避けるほうが良いという立場を取る。必要な説明変数を含めずに回帰分析を行うと、欠落変数バイアスの問題が生じる。また、もし分析における主な関心が、着目する説明変数 \\(\\small x_{ji}\\) の \\(y_{i}\\) への効果を検証・解釈することであり、かつそれをきちんと捉えるために他の説明変数の存在が必要なのであれば、VIFは無視して構わない（cf. Wooldridge, 2013）。そのため、「VIFが10以上11だから、多重共線性があり、変数を除外すべきだ」というような考えは、上記の研究上の関心に対しては恣意的であり有意義でないと考える。そもそも先述のように、VIFによって生じる問題は\\(\\small Var(\\hat{\\beta}_j)\\) の増加であるが、この分散は他の要素にも影響を受けるため、VIFが10以上だから必ず \\(\\small Var(\\hat{\\beta}_j)\\) が大きすぎて推定量が有用でないということはない。これらのことから、回帰モデルに含む変数選択は、VIFの値に依存して判断するよりも、分析モデルの意図や用いる変数の理論的・分析的意義について検討し決断されるべきものだと考える。 第三に過剰制御は、モデル内での係数解釈の変化を捉えた問題である。ここまでの説明では、欠落変数バイアスの議論を中心に、基本的には変数をモデルに含めることの重要性を説明してきた。しかしながら、重回帰分析の係数の解釈（パーシャル効果）を鑑みると、主要な説明変数の効果に関する理論的なメカニズムにおいて中間経路として機能する変数をモデルに含めることには注意が必要である。例えば、製品開発におけるクラウドソーシングという非専門家（一般消費者）の意見を製品開発に活用する戦略（e.g., Nishikawa et al., 2017）が製品の売上に与える影響を、製品レベルデータを用いて分析する場合を考える。その際、クラウドソーシングの有効性に関するメカニズムとして、実際の製品ユーザーである消費者の意見を反映することで、品質の高い製品を開発でき、結果として売上向上につながるというものを考えているとする。つまり、クラウドソーシングが売上に与える中間経路として、製品の品質が機能するというメカニズムを考えていることになる。しかしながら、売上を被説明変数とする回帰モデルを考える際に、製品品質も売上に影響を与えうる変数なので説明変数として回帰モデルに含めたいと考えるかもしれない。このような考えが、過剰制御の問題につながる。製品品質は、クラウドソーシングから売上への影響に関する中間経路として機能する変数であり、これをコントロールしてしまうことは、クラウドソーシングの係数の解釈を大きく変えてしまう。具体的には、このようなモデルでは、製品品質を一定とした上でクラウドソーシングが売上に与える影響を捉えることになってしまう。上記の関係は以下の図のように示される。 クラウドソーシングと売上の中間経路（品質） 製品品質以外のクラウドソーシングの効果が研究の関心と整合的であるならば、品質を含めた定式化で問題はない。しかしながら、重回帰モデルの変数選択では、自身が論じているメカニズムと係数解釈の整合性を保つように、中間経路変数を含めることで過剰制御の問題に陥っていないかを慎重に検討する必要がある。 重回帰モデルに含める変数の指針について、西山ほか（2019, p.186）は以下の表のように整理している。なお、本書においては主に欠落変数バイアスと係数の解釈という観点から変数選択に関する考え方を整理した。これは、研究上関心のある変数の効果（係数）について検証・解釈するという観点に基づく議論である。しかしながら、「予測」という側面に着目すれば推定や予測の精度を高める（誤差を小さくする）ことが重要になり、変数選択の基準も変わる。そうなれば、欠落変数による問題や係数の解釈の変化はあまり重要でなくなるかもしれない。このように、立場が変わることによって回帰モデルの特定化の基準も変化することを最後に付け加えておく。 Table 8.1: 変数選択の指針 x に影響を与える or x と同時決定 x から影響を受ける x とは無相関 y に影響を直接与える 必ず含める（欠落変数を防ぐ） 含めていけない（過剰制御） 含めることで推定誤差は減る（含めなくてもバイアスは増えない） y に影響を与えない 含めないほうが良い（ただし推定誤差は増えるがバイアスは増えない） 左と同様 左と同様 後述する \\(\\small R^2_{j}\\) を用いて、\\(VIF_j=1/(1-R^2_{j})\\) と定義される。↩︎ より具体的には説明変数 \\(x_{ji}\\)を従属変数に、その他の全ての説明変数を独立変数として回帰分析をした際の決定係数 \\(R^2_j\\)、。なお、先述のVIFはこの\\(R^2_j\\)の関数である。↩︎ 慣習としてよく用いられる閾値↩︎ "],["参考文献-6.html", "8.5 参考文献", " 8.5 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Nishikawa, H., Schreier, M., Fuchs, C., &amp; Ogawa, S. (2017). The Value of Marketing Crowdsourced New Products as Such: Evidence from Two Randomized Field Experiments. Journal of Marketing Research, 54(4), 525-539. Wooldridge, J. (2013) Introductory Econometrics A Modern Approach,Cengage Learning. "],["regtechnic.html", "Chapter 9 回帰分析上の工夫紹介", " Chapter 9 回帰分析上の工夫紹介 前章で説明した通り、回帰分析はビジネス領域におけるデータ分析手法として有力なものである。本章では、回帰分析を用いたマーケティング研究においてよく利用されるモデルの定式化や分析上の工夫について紹介する。具体的には、ダミー変数、交差項を用いた回帰モデル、対数線形モデル、について説明する。なお、章末にRを用いた分析結果表の作成方法も紹介しているので、参照してほしい。 "],["ダミー変数.html", "9.1 ダミー変数", " 9.1 ダミー変数 本節では、説明変数としてカテゴリ変数を用いる場合の方法と、その結果の解釈について説明する。マーケティング領域の研究においては、あるカテゴリに属することが成果変数にどのような影響を与えるのかに関心を持つことも多い。そのような場合には、「ダミー変数」と呼ばれる形にカテゴリ変数を定義し、分析することが多い。ダミー変数とは、特定のカテゴリに属するならば1を、それ以外なら0を取るような変数を指す。例えば、女性ならば1、それ以外の性別であれば0を取るようなダミー変数を、「女性ダミー」として扱うことができる。ダミー変数 D を用いた回帰モデルは以下の様に表すことができる、 \\[ y_i=\\alpha+\\beta x_i + \\gamma D_i+u_i \\] ただし、\\(x_i\\) は連続尺度の説明変数である。ダミー変数は取りうる値が1か0に限定されているため、y の条件付期待値は以下のように解釈できる。 \\[ E(y_i|D_i=1)=\\alpha+\\beta x_i + \\gamma \\] \\[ E(y_i|D_i=0)=\\alpha+\\beta x_i \\] つまり、ダミー変数に対応する回帰係数はベースライン（\\(D_i=0\\)）グループとの「切片の差」を表しているということがわかる。例えば、\\(\\small \\alpha\\)、\\(\\small \\beta\\)と、\\(\\small \\gamma\\)が正の値を取るような場合、上記のダミー変数の関係は以下の図のように示す事ができる。例えば、このダミー変数が女性ダミーであるならば、女性はその他の性別に比べて、y の値が相対的に高い、と解釈できる。 ダミー変数関係図 ダミー変数を説明変数に含む回帰分析をRで実行することはとても簡単である。lm() 関数内のモデル定義において、カテゴリ変数を含めば良いだけである。Characterという文字列情報のデータ型で示されているカテゴリ変数を用いると、自動でダミー変数化して、回帰分析を実行してくれる。 例えば、本章で使っている firmdata19 を使って分析を行ってみる。具体的には、営業利益率（営業利益/売上）が小売産業に属する企業とそれ以外で異なるかを分析する。しかし、データセット上にこの分類に対応するカテゴリー変数は存在しないため、以下のように、“Retail Stores, NEC” もしくは “Supermarket Chains” のどちらかに含まれる企業であれば1、それ以外であれば0を取るカテゴリー変数（format）を作成する。%in% は二つのベクトル間の要素の一致を確認するための演算子で、下記mutateコマンドでは、ind_en 列に書かれている情報が retail リストに「含まれる」か否かをチェックしている。そのため、以下コマンドでは、事前に作成した retail を参照し、ind_en で観察されるカテゴリーが retail に当てはまれば、\"Retail\"を、そうでなければ、\"Others\" を返すように指示していることになる。 library(tidyverse) #産業名を特定したオブジェクト&quot;Retail&quot;の作成 retail &lt;- c(&quot;Retail Stores, NEC&quot;, &quot;Supermarket Chains&quot;) # Retailを使ったカテゴリー変数の作成 firmdata19 &lt;- firmdata19 %&gt;% mutate(format = ifelse(ind_en %in% retail, &quot;Retail&quot;, &quot;Other&quot;)) #カテゴリーの頻度チェック with(firmdata19, table(format)) ## format ## Other Retail ## 98 49 そして、作成したカテゴリー変数も含めて、以下のような回帰モデルを分析する。 fit.d1 &lt;- lm(op ~ mkexp + format, data = firmdata19) summary(fit.d1) ## ## Call: ## lm(formula = op ~ mkexp + format, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09224 -0.03182 -0.00752 0.01799 0.26461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.098489 0.008859 11.117 &lt; 2e-16 *** ## mkexp -0.069204 0.025444 -2.720 0.00734 ** ## formatRetail -0.023734 0.009751 -2.434 0.01616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05454 on 144 degrees of freedom ## Multiple R-squared: 0.1042, Adjusted R-squared: 0.09178 ## F-statistic: 8.377 on 2 and 144 DF, p-value: 0.0003617 上記のように、カテゴリー変数（format）を回帰モデルに含めるだけで、自動的にダミー変数化して分析を実行してくれる。今回はたまたま我々の意図通り Othersをベースライングループに設定されているが、これは、指示した結果ではない。  もし確実に特定のカテゴリーを1と定義したい場合には、自身でダミー変数を作成して回帰分析を行えばいい。 firmdata19 &lt;- firmdata19 %&gt;% mutate(retail = ifelse(format == &quot;Retail&quot;, 1, 0)) #確認 with(firmdata19, table(retail, format)) ## format ## retail Other Retail ## 0 98 0 ## 1 0 49 fit.d2 &lt;- lm(op ~ mkexp + retail, data = firmdata19) summary(fit.d2) ## ## Call: ## lm(formula = op ~ mkexp + retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09224 -0.03182 -0.00752 0.01799 0.26461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.098489 0.008859 11.117 &lt; 2e-16 *** ## mkexp -0.069204 0.025444 -2.720 0.00734 ** ## retail -0.023734 0.009751 -2.434 0.01616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05454 on 144 degrees of freedom ## Multiple R-squared: 0.1042, Adjusted R-squared: 0.09178 ## F-statistic: 8.377 on 2 and 144 DF, p-value: 0.0003617 分析の結果、マーケティング支出と小売ダミーのどちらも営業利益率に対して負に有意な影響を持つことがわかった。したがって、小売企業はデータセット内の他の産業よりも利益率が低いといえる。 "],["交差項.html", "9.2 交差項", " 9.2 交差項 マーケティングに関する研究では、ある説明変数の被説明変数に与える影響が、別の説明変数に影響を受ける形で変化することを捉えることも多い。このような変数間の関係に影響するような効果を調整効果（moderating effect）もしくは相互作用効果（interaction effect）と言い、以下のような図で示されることが多い。 ダミー変数関係図 上の図では、メインの説明変数と調整変数が存在するかのように見えるが、分析に置いてはどちらか一方をメインと扱うことはなく、XとZをかけ合わせた交差項を含めたモデルを分析する。なお、調整効果もしくは相互作用効果の分析ではダミー変数もしくは量的変数のどちらも用いることができる。 交差項概念図 回帰モデルにおける交差項の利用には、以下の点に注意が必要である。 交差項には、条件付き効果に関する作業仮説を論じる必要がある。 例、XがYに与える影響は、Zの値に応じて変化する。 交差項を含むモデルには交差項を構成する二つの変数も含める。 推定モデル上、どちらか一方がメインかのような特定化は行わない。 交差項を構成する二つの変数の係数を従来の回帰分析結果と同じように解釈してはいけない。 説明変数独立項の係数の意味について注意が必要。 9.2.1 傾きダミー 先述の注意点について、具体例を見ながら確認していく。まず、量的変数とダミー変数の交差項について考える。ダミー変数と量的変数の交差項を作ることによって、傾きがグループによって変わるか否かを捉えることができる。交差項を含む回帰モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2D_i+\\beta_3(x_i\\times D_i)+u_i \\] \\(\\small x_i\\) が \\(\\small y_i\\) に与える影響を \\(\\small \\Delta y_i/\\Delta x_i\\) とすると、それぞれ以下のように示される。 \\[ \\begin{cases} D_i=1\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3\\\\ D_i=0\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1 \\end{cases} \\] つまり、\\(\\small x\\) が \\(\\small y\\) に与える影響（傾き）が、\\(\\small \\beta_3\\) の分だけ、ダミー変数のカテゴリーによって変化することが伺える。\\(\\small \\beta_1\\) は \\(\\small D=0\\) の際の x の効果であり、 \\(\\small D=1\\) の際の効果は、\\(\\small \\beta_1+\\beta_3\\) で表される。 そのため、このようなダミー変数の使い方を「傾きダミー（Slope dummy）」と呼ぶことも多い。 先程の firmdara19 にこの分析モデルを適応してみる。Rにおける交差項の導入は、lm(y ~ x * d) のように指定すれば、xとdの交差項とそれぞれの独立項を自動でモデルに含めてくれる。 fit.d3 &lt;- lm(op ~ mkexp * retail, data = firmdata19) summary(fit.d3) ## ## Call: ## lm(formula = op ~ mkexp * retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.096735 -0.028600 -0.005523 0.018005 0.258568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.108430 0.009217 11.764 &lt; 2e-16 *** ## mkexp -0.105662 0.027505 -3.842 0.000183 *** ## retail -0.088147 0.023231 -3.794 0.000218 *** ## mkexp:retail 0.191432 0.063025 3.037 0.002837 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05305 on 143 degrees of freedom ## Multiple R-squared: 0.1585, Adjusted R-squared: 0.1409 ## F-statistic: 8.979 on 3 and 143 DF, p-value: 1.73e-05 分析の結果、マーケティング支出と小売ダミーは負に有意である一方で、両変数の交差項（`mkexp:retail）は正に有意であることが示された。このとき、mkexp の単独項は、retail が0のとき、つまりその他グループにおけるマーケティング支出と利益率の関係を示しており、それが負に有意であると理解できる。そして、mkexp:retail の項が正に有意であることから、その他グループの傾きと、小売グループの傾きは有意に異なることが伺える。もう少し詳細にこの結果を整理すると、本分析による利益率の予測値 \\(\\hat{y}\\)は以下のように示すことができる。 \\[ \\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i - 0.088147\\text{retail}_i + 0.191432\\text{mkexp}_i * \\text{retail}_i \\] \\[ \\begin{cases} \\text{Retail}: &amp;\\hat{y}_i = 0.020283 + 0.08577\\text{mkexp}_i\\\\ \\text{Others}: &amp;\\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i \\end{cases} \\] 交差項を用いた分析を実行する場合、分析結果をより詳細に理解するため、結果を図示化することが大切になる。具体的には以下のように、分析結果からカテゴリ別に予測値をそれぞれ計算し、図示化することで、含意を得ることができる。ここでは、sjPlotというパッケージの plot_model() 関数を用いて、分析結果に基づく被説明変数の予測値を図示化する。まず、パッケージをインストールして欲しい。 install.packages(&quot;sjPlot&quot;) 今回の図示化のために、plot_model() 関数内では、type = \"pred\" と引数を指定した上で、terms において着目する変数名を特定する。また、この関数では、ggplot2のように、図示化に関する凡例や軸ラベルなどの様々な情報を書き足していくことができる。 library(sjPlot) pred &lt;- plot_model(fit.d3, type = &quot;pred&quot;, terms = c(&quot;mkexp&quot;,&quot;retail&quot;), ci.lvl = .95) + labs(title = &quot;Slope Analysis&quot;, subtitle = &quot;(Predicted Values of Profitability with 95% Confdence Intervals)&quot;, x = &quot;Marketing Expense&quot;, y = &quot;Profitability&quot;) + scale_color_discrete(name = &quot;Retail Dummy&quot;) pred 分析の結果、小売企業においてはマーケティング支出と利益率の関係は右上がりである一方で、その他の企業では右下がりである。回帰分析における交差項の係数がこの傾きの違いを示している。また、実線を比較すると マーケティング支出が 0.5 辺りを境に小売企業の利益率の予測値のほうがその他グループよりも高くなっているように見える12。しかしながら、両直線の95%信頼区間は重なっており、統計的に有意な差があるとは言えない。そのため、マーケティング支出が高いとき、小売企業の利益率（の予測値）のほうが有意に高いとは言えない。 9.2.2 量的変数同士の交差項 交差項を用いた分析は量的変数同士にも応用できる。しかしながら、交差項を用いた分析結果の解釈には注意が必要であり、分析の実行においても工夫が求められる。量的変数同士の交差項モデルとして、以下の回帰式を考える。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2z_i+\\beta_3(x_i\\times z_i)+u_i \\] 上記における x と z はどちらも量的変数であり、x が y に与えるパーシャル効果は、以下のように表すことができる。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] 同様に、z が y に与えるパーシャル効果は、 \\[ \\frac{\\Delta y_i}{\\Delta z_i}=\\beta_2+\\beta_3x_i \\] となる。したがって、x と z が y に与える影響は、互いに依存しあっていることがわかる。上式の \\(\\small \\beta_3\\) は調整効果や相互作用効果と呼ばれる。 交差項を用いた回帰モデルでは、説明変数の独立項に関する解釈に注意が必要となる。例えば上式の \\(\\small \\beta_1\\)（\\(\\small \\beta_2\\)）はどのような条件で x（z）が y に与える影響だと解釈できるだろうか。例えば、上記のモデルにおける y が体重、 x と z がそれぞれ筋肉量と身長だったとする。このとき、筋肉量が体重に与える影響は、 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] であり、\\(\\small \\beta_1\\) は「身長（z）が0」という条件下で筋肉量が体重に与える影響を表している。しかしながら、身長が0のときという非現実的な条件下での結果は、我々にとって解釈が難しく、また情報としても有益でないかもしれない。この問題への対策のひとつが「平均値での中心化（mean-centering）」である。これは、交差項に用いる説明変数に関して、平均からの偏差を用いる方法である。中心化された変数による交差項モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 (x_i-\\bar{x})+\\beta_2(z_i-\\bar{z})+\\beta_3(x_i-\\bar{x})\\times (z_i-\\bar{z})+u_i \\] これにより、x の効果は以下のように示される。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3(z_i-\\bar{z}) \\] そのため、中心化されたモデルにおける\\(\\small \\beta_1\\) は、\\(\\small z_i-\\bar{z}=0\\)、つまり「z が平均値」の際の、x 効果だと解釈できる。また、平均値以外の値を用いた中心化も当然可能であるため、研究・実務上関心の強い値（何らかの閾値など）がある場合には、それを基準とした中心化もできる。 ここで、量的変数を用いた交差項モデルをRで分析する。分析の実行方法は、前節の傾きダミーと同様である。ここでは、“headphone07.csv” という、ある年のヘッドフォン製品の売上を捉えたデータセット（人工架空データ）を用いる。このデータは、以下の変数を含んでいる。 売上（百万円） プロモーション投資額（百万円） R&amp;D投資額（百万円） このデータセットに対して、本分析ではR&amp;Dによって向上する（と仮定する）製品品質とプロモーションの相互作用が売上に与える影響を分析する。具体的には、たとえ良いものを作っても、きちんとその情報を消費者に伝達しないといけないのではないかという課題を捉える。そのため本分析を通じて、製品品質への投資（R&amp;D投資）が売上に与える影響が、プロモーション量に応じて変化するのではないかという研究課題に答える。まず以下のようにデータを読み込み、データフレームを確認する。 Headphone07 &lt;- readr::read_csv(&quot;data/headphone07.csv&quot;, na = &quot;.&quot;) #データフレームの確認 glimpse(Headphone07) ## Rows: 221 ## Columns: 4 ## $ ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ sales &lt;dbl&gt; 118.8377, 548.6312, 197.3075, 104.2657, 748.8251, 947.8850, … ## $ rd &lt;dbl&gt; 404.0893, 252.1270, 444.3374, 407.5876, 841.7605, 336.8744, … ## $ promotion &lt;dbl&gt; 75.63163, 102.74572, 97.98040, 83.46613, 105.69250, 80.17476… このデータに対して、まずは以下の通り中心化していない変数を用いて分析を行う。 fit_int &lt;- lm(sales ~ rd*promotion, data = Headphone07) summary(fit_int) ## ## Call: ## lm(formula = sales ~ rd * promotion, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.033e+04 1.090e+02 186.5 &lt;2e-16 *** ## rd -5.187e+01 2.844e-01 -182.4 &lt;2e-16 *** ## promotion -1.914e+02 1.052e+00 -181.9 &lt;2e-16 *** ## rd:promotion 4.979e-01 2.730e-03 182.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、rdとpromotion の交差項は正に有意だが、どちらの独立項も負に有意であった。また、先述の通りこれらの独立項の係数は、もう一方の変数が0のときのそれぞれの効果を表しており、現実的には解釈が難しい結果になっている。したがって、以下の様に中心化変数を作成し、回帰分析を実行する。 Headphone07 &lt;- Headphone07 %&gt;% mutate(promotion_c = promotion - mean(promotion, na.rm = TRUE), rd_c = rd - mean(rd, na.rm = TRUE)) fit_int_c &lt;- lm(sales ~ rd_c*promotion_c , data = Headphone07) summary(fit_int_c) ## ## Call: ## lm(formula = sales ~ rd_c * promotion_c, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 382.27812 1.51459 252.40 &lt;2e-16 *** ## rd_c -0.91767 0.01196 -76.75 &lt;2e-16 *** ## promotion_c 0.69039 0.03972 17.38 &lt;2e-16 *** ## rd_c:promotion_c 0.49792 0.00273 182.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、交差項の結果については非中心化モデルと同じであることが伺える。一方で独立項については、rdは負に、promotionは正に有意であることが明らかになった。つまり、プロモーションが平均値である場合、rdにコストを掛けても売上には繋がらない一方で、rdが平均値の場合、プロモーションによって売上が伸びることが伺える。これをまとめると、品質が平均的ならプロモーションで売上は上がるが、プロモーションが平均的で品質投資をしても売上に逆効果となることが伺える。R&amp;D投資による負の効果については価格との関係もあるかもしれない。品質が向上すると通常価格も上がるため、プロモーションがあまり高くない状態においては、価格の向上によって売上を損ねるかもしれない。この点については、価格も含めたさらなる調査、分析が必要になる。 量的変数同士の交差項分析結果の図示化においては、二種類の方法がある。一つ目は調整変数として捉える変数を高水準（例、平均 \\(+\\) 1標準偏差）と低水準（例、平均 \\(-\\) 1標準偏差）に二分し、前節の傾きダミーの図示化のようにそれぞれの場合でメイン変数による被説明変数への傾きを描画する方法である。これは、前節で使用したsjPlotパッケージで実行できる。 二つ目は、メインの変数が被説明変数へ与える影響を縦軸に、調整変数の値を横軸に取ることによって、メイン変数が持つ効果が調整変数によって変化する様子を連続的に描画する方法である。これは、marginaleffectsというパッケージを用いる。 まず、一つ目の方法は、以下の通り実行することができる。分析の結果、やはりプロモーション水準が低い場合にはR&amp;D投資が売上に与える影響は負であるものの、プロモーション水準が高い場合には正に転じることが伺える。 leg = c(&quot;Mean - 1sd&quot;, &quot;Mean&quot;, &quot;Mean + 1sd&quot;) int_fig1 &lt;- plot_model(fit_int_c, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;, ci.lvl = .9999999999) + labs(title = &quot;Predicted values of Sales (R&amp;D * Promotion)&quot;, x = &quot;R&amp;D Investment&quot;, y = &quot;Sales&quot;)+ scale_color_discrete(name = &quot;Promotion level&quot;, labels=leg) ## Scale for colour is already present. ## Adding another scale for colour, which will replace the existing ## scale. int_fig1 続いて、二つ目の方法を実行するために、以下の要領でパッケージをインストールしてほしい。marginaleffects は、margins や emtrends の機能を引き継ぎ、限界効果に関する計算結果を図示化するためのパッケージである13。 install.packages(&quot;marginaleffects&quot;) R&amp;D が 売上に与える影響（傾き）がどのように変化するかは、plot_slopes() という関数を用いる。以下は、傾きの変化に関する図の出力結果である。 library(marginaleffects) int_fig2 &lt;- plot_slopes(fit_int_c, variables = &quot;rd_c&quot;, condition = &quot;promotion_c&quot;, conf_level = .99999999) + labs(title = &quot;Marginal effects of R&amp;D on Sales&quot;, x = &quot;Promotion&quot;, y = &quot;Slope of R&amp;D on Sales&quot;) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig2 分析の結果、promotionが0（平均）である条件を基準に、R&amp;Dの傾きが正に転じていることが伺える。なお、よく見ると、直線の周りに灰色の影が描画されていることがわかる。これは、99%信頼区間を示している。今回は人工的に作成したデータなので、非常に当てはまりがよく信頼区間がとても狭くなっているが、実際のデータを用いて同様の図示化をすればもう少し明確に信頼区間を視認できる。このように、交差項を利用した回帰モデルを分析する際は、事後的な図示化を行うことを心がけると良い。この作業により実務的・学術的により有益な含意につながることがある。 以下は、plot_slopes() に関するおまけである。図示化の確認のために、firmdata19を使って、マーケティング支出と総資産との交差項により、利益率を説明する回帰モデルを考える。以下では、中心化と回帰分析の実行、図の出力を実施している（回帰分析結果は省略）。 firmdata19 &lt;- firmdata19 %&gt;% mutate(mkexp_c = mkexp - mean(mkexp), asset_c = total_assets - mean(total_assets)) fit_int2 &lt;- lm(op ~ mkexp_c * asset_c, data = firmdata19) int_fig3 &lt;- plot_slopes(fit_int2, variables = &quot;mkexp_c&quot;, condition = &quot;asset_c&quot;, conf_level = 0.99) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig3 図を見ると、Headphoneデータよりもはっきりと信頼区間が視認できる。マーケティング支出の効果は資産によって低下するように見えるが、信頼区間を考慮すると、資産額が平均値（asset_c \\(=0\\)）付近の値を取るときのみマーケティング支出は負に有意の影響を持つが、それ以外の区間では有意でない（信頼区間に0を含む）ことが伺える。このように、信頼区間の出力は結果の解釈を有意義なものにしてくれる。 試しに、plot_model 内の信頼区間に関する引数を ci.lvl = NAとし、実線のみの比較を確認して見て欲しい↩︎ 本講義では、詳細は割愛するが以下から詳細を確認できる https://vincentarelbundock.github.io/marginaleffects/dev/↩︎ "],["係数比較.html", "9.3 係数比較", " 9.3 係数比較 マーケティング領域においては、異なる説明変数のうちどちらの係数のほうが大きいのかを比較するような議論を行う研究がしばしば見られる。しかし、その多くの場合において、（1）係数の推定値をそのまま比較することや、（2） 片方の検定結果が有意でありもう一方は有意でないというような検定結果の比較をもとに大小関係を論じている。しかしながら、たとえ説明変数の単位を（標準化などで）統一していたとしても、これら二つのような比較によって大小関係を結論づけるのは不十分である。係数の大小比較に関する現実的な方法のひとつは、説明変数の単位を統一した上で信頼区間を計測することである。以下では、もうひとつの方法として、大小比較に関する統計的検定を実行するための工夫を紹介する。 まず、以下のような被説明変数を\\(Y\\)、説明変数を \\(X_1,~X_2\\)とする重回帰モデルを考える。 \\[ Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + u \\] 係数の大小比較において重要となるのは、上式内の \\(\\alpha_1\\) と \\(\\alpha_2\\) の差である。つまり、\\(\\alpha_1 - \\alpha_2 = 0\\) であれば両者に差がないことになる。ここで、\\(\\theta = \\alpha_1 - \\alpha_2\\) と定義し、回帰モデルを以下のような修正版モデルに書き換える。 \\[ Y = \\alpha_0 +(\\theta - \\alpha_2) X_1 + \\alpha_2 X_2 + u \\\\ = \\alpha_0 +\\theta X_1 + \\alpha_2 (X_1 + X_2) + u \\] つまり、この式のように \\(X_1\\) と \\((X_1 + X_2)\\) という二つの説明変数を用いた重回帰モデルを推定すると、修正版における\\(X_1\\)の係数は \\(\\theta = \\alpha_1 - \\alpha_2\\) として解釈する事が可能になる。そしてこの修正版モデルにおける \\(X_1\\)の係数を検定することで、\\(\\alpha_1 - \\alpha_2=0\\)を帰無仮説とした検定と同義の結果を得ることができ、大小関係に関する含意を得ることができる。 ここでは例として、企業の広告投資とR&amp;Dへの投資が売上に与える影響について比較する。分析にはfirmdata19を用いる。このデータの広告とR&amp;D変数の単位はともに百万円であり揃っているが、本書では教育的意図から変数を標準化するプロセスを提示し、標準化した変数を用いる。そのため、分析結果の係数解釈には注意が必要になる。本書ではまず、以下の通り変数を作成した後、通常の重回帰モデルを実行する。 firmdata19 &lt;- firmdata19 %&gt;% mutate(adv = (adv -mean(adv))/sd(adv), rd = (rd -mean(rd))/sd(rd), ad_rd = adv +rd) fit_linear &lt;- lm(sales ~ adv + rd, data = firmdata19) summary(fit_linear) ## ## Call: ## lm(formula = sales ~ adv + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4356502 -459524 -238384 126951 2695201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1194514 73839 16.177 &lt;2e-16 *** ## adv 1627341 74544 21.831 &lt;2e-16 *** ## rd 30111 74544 0.404 0.687 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 895200 on 144 degrees of freedom ## Multiple R-squared: 0.7709, Adjusted R-squared: 0.7677 ## F-statistic: 242.3 on 2 and 144 DF, p-value: &lt; 2.2e-16 これを見ると、一見広告投資のほうが係数が大きそうである。では次に、ad+rd を用いた係数比較モデルを分析することでこの差が統計的に有意かを検討する。 fit_comp &lt;- lm(sales ~ adv + ad_rd, data = firmdata19) summary(fit_comp) ## ## Call: ## lm(formula = sales ~ adv + ad_rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4356502 -459524 -238384 126951 2695201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1194514 73839 16.177 &lt;2e-16 *** ## adv 1597230 111072 14.380 &lt;2e-16 *** ## ad_rd 30111 74544 0.404 0.687 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 895200 on 144 degrees of freedom ## Multiple R-squared: 0.7709, Adjusted R-squared: 0.7677 ## F-statistic: 242.3 on 2 and 144 DF, p-value: &lt; 2.2e-16 分析の結果、adv の独立項の係数が有意であったため、二つの係数は有意に異なると理解できる。しかしながら、このような係数の比較を行う際には、その背景にある社会的実務的比較可能性について考慮する必要がある。例えば、特定の産業において、広告への投資もしくはR&amp;Dへの投資をしづらい状況はないだろうか。反対に多くの企業が広告への投資を行っているが、R&amp;Dには投資がされていないという状況は無いだろうか。仮に多くの企業がすでに広告への投資を十分に行っているならば、その中で広告支出額が1単位（1標準偏差）増やすことの意味は非常に大きいはずである。したがって、たとえ分析上変数間の比較が可能であったとしても、その比較がどのような意味をもつのか、もしくはその比較はフェアなものなのかという点については慎重に議論・検討する必要がある。 "],["対数線形.html", "9.4 対数線形", " 9.4 対数線形 ここまでの回帰モデルの特定化では、一次関数を利用していた。しかしながら、以下の図のように傾きの大きさが途中で変化するような非線形の関係には一次関数のモデルは上手くフィットしない。 非線形例 非線形の関係として例えば、以下のような関数形がある。 \\[ Y=AL^\\alpha K^\\beta \\] ここで、Yは生産量、Lは労働投入量、K は資産を仮定すると、上記の式は、経営学分野にも応用される、経済学におけるコブダグラス型の生産関数として知られている。しかしながら、このような関数をそのまま線形回帰分析に当てはめることはできない。そこで、右辺と左辺両側の変数に対して、自然対数を用いた変数変換を行うことで、回帰分析で推定可能なモデルを構築する。このような変換された説明変数（例、\\(\\small \\ln K_i\\)）を用いる定式化であれば、推定される係数については線形の形で扱うことができる。対数による変数変換を伴う線形回帰モデルは、一般に対数線形モデルと呼ばれる。 \\[ \\ln Y_i = \\ln A + \\alpha\\ln K_i+\\beta\\ln L_i + u_i \\] このような回帰式をR上で構築することは難しくない。lm() 関数内のモデル定式化において、log() を用いれば良い。例えば、上の式を firmdata19 に当てはめて、Yを売上、Lを人件費、K を有形固定資産とおいて、以下のようにモデルを推定してみる14。 fit_prod &lt;- lm(log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) summary(fit_prod) ## ## Call: ## lm(formula = log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4058 -0.3725 -0.0108 0.3870 1.6627 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.17985 0.31763 10.01 &lt;2e-16 *** ## log(labor_cost) 0.53793 0.03684 14.60 &lt;2e-16 *** ## log(ppent) 0.34438 0.02797 12.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5176 on 144 degrees of freedom ## Multiple R-squared: 0.8735, Adjusted R-squared: 0.8718 ## F-statistic: 497.2 on 2 and 144 DF, p-value: &lt; 2.2e-16 このような対数線形モデルでは、これまでのような一次関数モデルとは異なる係数の解釈を行う必要がある。両側対数の回帰モデルにおける回帰係数は、「弾力性」を表している。つまり係数は、説明変数が 1% 増加したとき、被説明変数が何%変化するか（変化率）を表している。また、この係数解釈の特徴から、マーケティング分野においても対数線形モデルはよく用いられる。具体的には、対数線形モデルによって需要の価格弾力性（価格が1%増えたときに、需要量が何%減るか）を分析することができる。 需要の価格弾力性（\\(\\eta\\): イータ）は、消費者による価格感度を捉える指標である。通常、他の要素を一定とした場合、価格が低いと需要量は上がる。では、どの程度価格の低下が需要量に影響を与えるのかという問いは、実務的にも学術的にも重要になる。その際に、単位に依存しない価格感度の尺度として最も広く使われているものが、需要の価格弾力性である。また弾力性は、1を重要な閾値として実務的に重要な含意を与える。弾力性値に応じた含意は以下のように整理することができる。 \\(\\small \\eta= 1\\) の場合: 価格が1%下がった時に需要量が1%上がることを意味する。価格と需要の変化率がちょうど釣り合った状態。 \\(\\small \\eta &gt; 1\\) の場合: 価格の上昇によって需要量が著しく減る（価格に敏感に反応する）。 価格の上昇によって売上を損ねる（需要量の損失が価格上昇の便益を上回っている） 。 \\(\\small \\eta &lt; 1\\) の場合: 価格の上昇によって需要量があまり減らない（価格に敏感に反応しない） 価格の上昇によって売上が伸びる（価格上昇の便益が需要量の損失を上回っている） 。以下では、対数線形モデルと需要の価格弾力性との関係に関する補足的な説明を追加している。 9.4.1 需要の価格弾力性追加説明 ここでは、需要の価格弾力性についてもう少し詳細な説明を加える。興味のない場合は読み飛ばしてもらって構わない。需要の価格弾力性は、需要量（q）と価格（p）とし、以下の \\(\\eta\\)のように定義できる。 \\[ \\eta=-\\frac{dq}{dp}\\times \\frac{p}{q} \\] この価格弾力性の定義に基づき、なぜ１が重要な閾値になるのかを説明する。 価格の変化による売上の変化\\(dpq/dp\\) の関係を変形することで \\(\\small \\eta\\) について以下を得る。 \\[ \\eta=\\frac{dpq}{dp}=-p\\cdot\\frac{dpq}{dp}+p=q\\left[1-\\left(-\\frac{dq}{dp}\\cdot\\frac{p}{q}\\right)\\right]=q(1-\\eta) \\] そのため、\\(\\small \\eta\\) が１より小さい時、価格の増加による売上の変化が正（\\(dpq/dp&gt;0\\)）であることが伺える。 9.4.2 対数線形の推定と検定 ここで、需要の価格弾力性に関する分析を実行するために、“price_data.csv” という人工データを用いる。このデータは製品の価格と需要量の情報を含んでいる。以下のようにデータを読み込み、分析を実行してほしい。 price &lt;- readr::read_csv(&quot;data/price_data.csv&quot;) fit_q &lt;- lm(log(q) ~ log(p), data = price) summary(fit_q) ## ## Call: ## lm(formula = log(q) ~ log(p), data = price) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.11220 -0.13668 0.02804 0.16166 0.51039 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.47781 0.08135 116.5 &lt;2e-16 *** ## log(p) -0.18008 0.01354 -13.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2309 on 998 degrees of freedom ## Multiple R-squared: 0.1506, Adjusted R-squared: 0.1497 ## F-statistic: 176.9 on 1 and 998 DF, p-value: &lt; 2.2e-16 分析の結果、log(p) の係数は負に有意であった。しかしここで、デフォルトで出力される検定結果の「帰無仮説」を思い出して欲しい。これは、係数が0か否かの検定を行っている。しかしながら、我々は係数が 1 より高いか、低いかに興味がある。そのため、\\(\\small H_0:\\beta = 1\\)という帰無仮説による検定を採用すべきである。このような検定の実行には、色々とやり方はあるが、本書では carというパッケージのlinearHypothesis() 関数 を用いる方法を紹介する。まず、以下のようにパッケージをインストールして欲しい。 install.packages(&quot;car&quot;) そして、linearHypothesis()において、参照する分析結果と、着目する変数（の係数）とその値について入力することで、検定を行う。 library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some linearHypothesis(fit_q, c(&quot;log(p) = -1&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## log(p) = - 1 ## ## Model 1: restricted model ## Model 2: log(q) ~ log(p) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 999 248.811 ## 2 998 53.225 1 195.59 3667.3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 検定の結果、fit.q による係数の推定値は、有意に -1 とは異なることがわかった。つまり、弾力性は 1 より低く、非弾力的である（価格にあまり敏感でない）ことが伺えた。なお、一つの説名変数の有意性検定にF検定を用いる場合のF分布は対応するt分布の二乗値に等しい。そのため、car による検定では、F値を返しているが、それ自体に問題はない。 最後に、回帰モデルにおいて対数化を右辺（説明変数）と左辺（被説明変数）のどちらに適用するかによって、係数の解釈が変わることを説明する。モデルの特定化と係数解釈の対応関係は、以下のように整理できる。 両側対数（\\(\\ln y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化率を表している。 被説明変数のみ対数（\\(\\ln y_i = \\alpha + \\beta x_i + u_i\\)）: \\(\\beta\\) は x が 1単位変化したときの y の変化率を表している。 説明変数のみ対数（\\(y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化量を表している。 なお、実際に研究において生産関数を推定する場合には、変数の特定化に関して、先行研究に基づきより慎重に検討する必要があるため注意して欲しい↩︎ "],["おまけ分析結果のまとめ出力上の工夫.html", "9.5 （おまけ）分析結果のまとめ・出力上の工夫", " 9.5 （おまけ）分析結果のまとめ・出力上の工夫 これまで本書では、分析結果について、Rで出力されるものをそのまま示していた。しかしながら、Rでの出力結果をレポートや論文にベタ貼りすることは、可読性の低下につながるため好ましくない。論文やレポート執筆の際には、Rで出力された結果から表を作成することが必要になるのだが、Excelなどのスプレッドシートに一つ一つの値をコピー・アンド・ペースト（コピペ）して表を作成していく方法は、操作ミスによる間違いの可能性が高まることと、作業プロセスを記録できない上に、何より面倒くさい。そこで、分析結果を読みやすく見た目も良い表の形式で楽に出力してくれるコマンドをR上で実行するした。そのために用いるパッケージは色々あるが、本書では modelsummary パッケージを 紹介する。以下のようにインストールして欲しい。 install.packages(&quot;modelsummary&quot;) ここでは試しに、本節で実行した fit_intを msummary() 関数で出力する。Rstudio上でmsummary() を実行すると、結果がコンソールではなく、Viewerに表示される。 library(modelsummary) ## `modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing ## backend. Learn more at: https://vincentarelbundock.github.io/tinytable/ ## ## Revert to `kableExtra` for one session: ## ## options(modelsummary_factory_default = &#39;kableExtra&#39;) ## options(modelsummary_factory_latex = &#39;kableExtra&#39;) ## options(modelsummary_factory_html = &#39;kableExtra&#39;) ## ## Silence this message forever: ## ## config_modelsummary(startup_message = FALSE) msummary(fit_int, statistic = &#39;conf.int&#39;) tinytable_q03mvfj38fu9wgcoho9s .table td.tinytable_css_83phy0kdt0miq6dvdrj0, .table th.tinytable_css_83phy0kdt0miq6dvdrj0 { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_wffv3iys844na9kk6488, .table th.tinytable_css_wffv3iys844na9kk6488 { text-align: left; } .table td.tinytable_css_qbemzp6wiflgxlh3fenw, .table th.tinytable_css_qbemzp6wiflgxlh3fenw { text-align: center; } .table td.tinytable_css_4z9dmmlsau139c1a4r6e, .table th.tinytable_css_4z9dmmlsau139c1a4r6e { border-bottom: solid 0.05em black; } (1) (Intercept) 20326.588 [20111.797, 20541.378] rd -51.872 [-52.433, -51.312] promotion -191.431 [-193.505, -189.358] rd × promotion 0.498 [0.493, 0.503] Num.Obs. 221 R2 0.994 R2 Adj. 0.993 AIC 1969.2 BIC 1986.2 Log.Lik. -979.617 F 11110.648 RMSE 20.36 上記のように引数をほぼ指定せずとも（statistic = 'conf.int' は信頼区間を出力する引数。デフォルトでは標準誤差）、このような表形式で結果を出力することができる。引数を用いることで、研究者に都合の良い表なるよう編集可能であることもこの関数の利点である。例えば、不必要な情報があれば、以下のように表から削除することもできる。 msummary(fit_int, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;) tinytable_2okx431ou7we7fnel41r .table td.tinytable_css_pqeyht3kmgfe5eniqa2t, .table th.tinytable_css_pqeyht3kmgfe5eniqa2t { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_o9xksnxrvin8x75zq86x, .table th.tinytable_css_o9xksnxrvin8x75zq86x { text-align: left; } .table td.tinytable_css_a42a1fgntdy32yumypoy, .table th.tinytable_css_a42a1fgntdy32yumypoy { text-align: center; } .table td.tinytable_css_gp7avl2ifi8c7br88lse, .table th.tinytable_css_gp7avl2ifi8c7br88lse { border-bottom: solid 0.05em black; } (1) (Intercept) 20326.588 (108.978) rd -51.872 (0.284) promotion -191.431 (1.052) rd × promotion 0.498 (0.003) Num.Obs. 221 R2 0.994 R2 Adj. 0.993 F 11110.648 また、msummary() は二つ以上のモデルを並べて表示させる場合にも便利である。複数の分析結果を list としてまとめ以下のように出力できる。ここでは、交差項を用いた回帰モデルにおける中心化のありなしを併記し比較可能にする。更に、表示される変数名も調整する。 var_nam &lt;- c(&quot;rd&quot; = &quot;R&amp;D&quot;, &quot;promotion&quot; = &quot;Promotion&quot;, &quot;rd:promotion&quot; = &quot;R&amp;D * Promotion&quot;, &quot;rd_c&quot; = &quot;R&amp;D_c&quot;, &quot;promotion_c&quot; = &quot;Promotion_c&quot;, &quot;rd_c:promotion_c&quot; = &quot;R&amp;D_c * Promotion_c&quot;, &quot;(Intercept)&quot; = &quot;定数項&quot;) Int &lt;- list() Int[[&quot;Without centering&quot;]] &lt;- fit_int Int[[&quot;With centering&quot;]] &lt;- fit_int_c msummary(Int, coef_map = var_nam, title = &quot;Comparing Interaction Models&quot;, notes = &quot;Values in [ ] show 95% confidence intervals&quot;, stars = TRUE, statistic = &#39;conf.int&#39;, conf_level = .95, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;) tinytable_4wn6ixteqn90jen2htmr .table td.tinytable_css_uzq246gxfd2n00v4xd9f, .table th.tinytable_css_uzq246gxfd2n00v4xd9f { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_n9ckjqslxhatkqr9d23s, .table th.tinytable_css_n9ckjqslxhatkqr9d23s { text-align: left; } .table td.tinytable_css_o9n1a6h67uk7vmn3ez2k, .table th.tinytable_css_o9n1a6h67uk7vmn3ez2k { text-align: center; } .table td.tinytable_css_x562f3be0xy751vi2wgw, .table th.tinytable_css_x562f3be0xy751vi2wgw { text-align: center; } .table td.tinytable_css_nuscvgb6q34hm5q5m5th, .table th.tinytable_css_nuscvgb6q34hm5q5m5th { border-bottom: solid 0.05em black; } Comparing Interaction Models Without centering With centering + p Values in [ ] show 95% confidence intervals R&D -51.872*** [-52.433, -51.312] Promotion -191.431*** [-193.505, -189.358] R&D * Promotion 0.498*** [0.493, 0.503] R&D_c -0.918*** [-0.941, -0.894] Promotion_c 0.690*** [0.612, 0.769] R&D_c * Promotion_c 0.498*** [0.493, 0.503] 定数項 20326.588*** 382.278*** [20111.797, 20541.378] [379.293, 385.263] Num.Obs. 221 221 R2 0.994 0.994 R2 Adj. 0.993 0.993 F 11110.648 11110.648 対外的に分析結果を共有する場合には、このような分析結果をViewer上ではなく、文書ファイル等に貼り付けたいと考えることも多いだろう。msummary() 関数では、引数を変更することで、出力形式を変更することも可能である。そのうえで、本書が最もおすすめする方法が、Latex形式での表の出力と、それを用いたLatexによる文書作成である。 Latex（ラテック or レイテック）は、ソースファイルにコマンドと文章を記載していきながらpdfを出力することで文書を作成するためのツールである。Latexは、数式や表の入力が簡単であり、出力結果もきれいであることや、文書フォーマットについて細かく気にせず文書を作成できるという利点がある。そのため、定量的な分析を伴う文書を作成するのに適したツールである。逆に短所としては、ソースコードの入力や、コンピュータ内へのLatex用環境構築に関する手間がかかるという点が挙げられる。しかし近年では、ウェブブラウザ上でLatexを動かせる “Overleaf” というサービスも普及し、環境構築による問題は解決されている。本書では、LatexやOverleafの詳細や設定方法については割愛するが、基本的にはOverleafの活用をおすすめするので、以下のリンクを参照して欲しい（https://dreamer-uma.com/overleaf/）。特に、卒業論文や修士論文に取り組む学生にとって、Overleafによってウェブ上にファイルを保管できることは非常に重要な利点となるだろう（毎年少なくない学生が論文のファイル保管ミスやノートPCの故障などのトラブルに見舞われるのを観察している）。 先程の交差項比較の表について、output = \"latex\" と引数を設定することで、出力形式を変更する。 msummary(Int, title = &quot;Comparing Interaction Models&quot;, notes = &quot;Values in [ ] show 95% confidence intervals&quot;, stars = TRUE, statistic = &#39;conf.int&#39;, conf_level = .95, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;, output= &quot;latex&quot;) tinytable_xx0g3hblibhg447kg8r5 .table td.tinytable_css_lzytcp18bmoxssjr5bvd, .table th.tinytable_css_lzytcp18bmoxssjr5bvd { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_9zpe22kvfowskp73divf, .table th.tinytable_css_9zpe22kvfowskp73divf { text-align: left; } .table td.tinytable_css_4prdrcpmhv8p5icnp4xd, .table th.tinytable_css_4prdrcpmhv8p5icnp4xd { text-align: center; } .table td.tinytable_css_xu9ftb7jeoz60zixb2fs, .table th.tinytable_css_xu9ftb7jeoz60zixb2fs { text-align: center; } .table td.tinytable_css_yk767yz5ysfi98ghmj9l, .table th.tinytable_css_yk767yz5ysfi98ghmj9l { border-bottom: solid 0.05em black; } Comparing Interaction Models Without centering With centering + p Values in [ ] show 95\\% confidence intervals (Intercept) \\num{20326.588}*** \\num{382.278}*** [\\num{20111.797}, \\num{20541.378}] [\\num{379.293}, \\num{385.263}] rd \\num{-51.872}*** [\\num{-52.433}, \\num{-51.312}] promotion \\num{-191.431}*** [\\num{-193.505}, \\num{-189.358}] rd × promotion \\num{0.498}*** [\\num{0.493}, \\num{0.503}] rd\\_c \\num{-0.918}*** [\\num{-0.941}, \\num{-0.894}] promotion\\_c \\num{0.690}*** [\\num{0.612}, \\num{0.769}] rd\\_c × promotion\\_c \\num{0.498}*** [\\num{0.493}, \\num{0.503}] Num.Obs. \\num{221} \\num{221} R2 \\num{0.994} \\num{0.994} R2 Adj. \\num{0.993} \\num{0.993} F \\num{11110.648} \\num{11110.648} これにより出力される結果のうち、\\begin{table} から \\end{table} までのコード内容をそのままlatexにコピペし、コンパイル（文書出力）をすれば、以下のような表が出力される15。 Latex出力例 いくらLatexが便利だと言っても、マーケティング分野においてはマイクロソフトWordによって文書を作成している教員や研究者も多い。そのため、指導教官や共同研究者との兼ね合いでLatexやOverleafを使えないという場合もある。そのため、もしどうしてもスプレッド形式で表を作成・編集したいと考えるのであれば、Googleスプレッドシートの拡張機能を利用するのが良いだろう。Google スプレッドシートに対応した “Spread-Latex”（https://workspace.google.com/marketplace/app/spreadlatex/218144906748） というアドオンによって、Latexコードで記載された表のソースコードをスプレッドシートに変換することができる。この拡張機能を利用すれば、ウェブブラウザで開いたGoogle スプレッドシート上にmsummary() によって出力された Latex コードを貼り付け、いくつかのクリック操作をするだけで、Latexコードからスプレッドシート型の表に変換できる。その後、少しの微調整（消えていないコードの削除など）を行うと、以下のような表を得る。 Google スプレッドシート出力例 もしExcelで作業を行いたい場合には、このGoogleスプレッドシートに変換された表をExcelにコピペすれば良い。しかしながら、2023年現在ではこのアドオンの更新は続いているが、今後デベロッパーがどのように対応するかは不明であるため、これはあくまで一時的な対応策にとどめておくことが賢明かもしれない。 以上のように、既存のパッケージやサービスを用いることで、少ない労力で分析結果を表にまとめ、出力することが可能になる。ここで紹介した以外にも有力なパッケージがあるので、関心のある人は色々と調べて、自分にあったやり方を探してみて欲しい。 なお、Latexにおいては % 記号によってコメントアウトされるため、ソースコード内にある場合には、バックスラッシュを用いる調整が必要になる。↩︎ "],["参考文献-7.html", "9.6 参考文献", " 9.6 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Arel-Bundock V (2023). marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. R package version 0.13.0.9002, https://vincentarelbundock.github.io/marginaleffects/. Wooldridge, J. (2013) Introductory Econometrics A Modern Approach,Cengage Learning. "],["cluster.html", "Chapter 10 セグメントとクラスター分析", " Chapter 10 セグメントとクラスター分析 本章では、マーケティング意思決定に密接に関わる手法として、クラスター分析を紹介する。クラスター分析は、複数の量的変数情報に基づいて、データのサンプルをいくつかのグループに分類する方法である。この手法は、マーケティングにおける「セグメンテーション」に関連する。本章ではまず、マーケティングにおけるセグメンテーションについて説明をした後、クラスター分析のマーケティング領域への応用について説明する。 "],["セグメンテーションとマーケティング意思決定.html", "10.1 セグメンテーションとマーケティング意思決定", " 10.1 セグメンテーションとマーケティング意思決定 セグメンテーションは、セグメンテーション・ターゲティング・ポジショニング（STP）に代表される基本的なマーケティング枠組みを構成する重要な要素である。マーケティングにおいて企業は、製品やサービスを通じて顧客に対し価値を提供することを目指すのだが、そのためには、対象となる市場においてどの部分（セグメント）を狙うのかを特定化する必要がある。市場を構成する消費者は、多様な好みや特徴を持っている。その中から似た好みや特徴を持つ消費者グループを見つけ出し特定化することがセグメンテーション（市場細分化）である。 市場におけるセグメントを明らかにすることは、より効果的に標的への価値提供を行うことにつながると期待できる。セグメンテーションを行わない場合、様々な消費者の好みからバランスを取るように、平均的な特徴をもつような製品・サービスを開発することになる。例えば、ホットティーが好きな人もいれば、アイスティーが好きな人もいる。しかしながら、これらの間を取った「ぬるいお茶」を提供しても、誰の好みにも響かない可能性がある。つまり、万人受けを狙って中途半端な製品・サービスを提供することを避けるためにも、市場を細分化して、共通の好みを持つ消費者グループを判別することはとても重要になる。 セグメンテーションを行うための分類基準として、企業はいろいろな情報を活用すると考えられる。理想的には、企業は消費者が求めている便益や好みに基づき市場を細分化したいと考えるだろう。例えば自動車市場において、ある消費者は技術的革新性を、別の消費者は快適性を求めるかもしれない。このような消費者のニーズに基づき市場を細分化することができれば、より効果的な標的市場を決定できる。しかしながら、消費者のニーズに関するデータを直接的に入手できない場合も多い。そのようなときには、（1）消費者属性、（2）心理的情報、（3）行動的情報についてのデータをニーズにつながる代理指標として用いてセグメンテーションを行うことが多い。 消費者属性としては、人口統計的情報や地理的情報が用いられる。人口統計的情報には、年齢、職業、所得、世帯人数、教育水準などが含まれる。これらの情報が消費者のニーズと連動している場合には、人口統計的情報に基づく市場細分化は効果的であると言える。例えば、高品質だが高価格な製品と低価格だが質の低い製品がある場合、所得による市場細分化は効果的になるかもしれない。また、子供がいる世帯といない世帯では、異なるニーズを持っていることも予想できる。 地理的情報として、消費者の居住地域によって異なる好みを表すことがある。例えば、関東と関西のように、食品の味付けに関する好みについて日本国内の地域間で違いがうかがえる。カップうどんやスナック菓子においては、このような味付けに関する地域差を反映させた製品開発を行っており、製品を販売する地域に応じて味付けを変えている。このような人口統計・地理的情報は、測定可能性・容易性が高いという利点を有している。自社内部データや自治体の人口動態情報などによって、自社顧客や特定地域における消費者特性の情報について得ることができる。これらの情報が消費者のニーズや好みと連動している場合には、非常に有力なデータとなるものの、マーケティングの本来的な目的はこれらの情報に基づくセグメントを発見することではないことには、注意が必要である。あくまで、これらの情報の背後にある消費者のニーズの代理指標として利用していることを理解することが求められる。 心理的情報としては、ライフスタイルや価値観、志向といった、特定の商品への評価そのものではない消費者の心理的特性を捉えた指標が用いられる。これらの情報については、アンケート調査などを用いた心理学的手法を用いることによって把握することができる。例えば、低価格志向や環境意識などは製品・サービスに求める価値観と近い心理的特性であるため、例えば自社の既存顧客や店舗を出店しているエリア内において環境問題への関心に基づきセグメントを確認することは、製品開発上重要な情報になりうる。 また、行動につながる心理的な態度（ブランドに対する好意）を捉えた類型も可能になる。これにより、どのようなセグメントが何を好むかを把握することができる。例えば、英国における “comfortable green” というセグメントは、Fairtrade や Green &amp; Black’s を好むが、Asdaやコカ・コーラは嫌っているだと言われている（Jobber and Elis-Chadwick, 2020）。 行動的情報の代表的なものとしては、購買行動に関する情報が挙げられる。例えば、製品やサービスの購買・利用頻度を顧客によるブランドロイヤルティの代理指標として用いることも多い。セグメンテーションを行うためにはまず、「どの情報に基づきセグメンテーションを行うのか」という基準となる情報の選定が求められる。 セグメンテーションは、図10.1 に示されるように、市場に存在する多様な消費者をいくつかのグループに分類することである。セグメンテーションが可能であることの前提には、（1）市場における消費者の好みが異質であることと、（2）共通した好みを持つ消費者のまとまりは見いだせるが、同時に見出したまとまり間での好みの差異も見いだせることである。つまり、異なる好みを持つ消費者の中から、「グループ内類似性」と「グループ間差異」とを基準に複数のグループを見出すことがセグメンテーションであるといえる。 Figure 10.1: セグメンテーション概要 "],["stpその後.html", "10.2 STPその後", " 10.2 STPその後 セグメンテーションが完了すると企業はターゲティングを行う。ターゲティングにおいては、発見したセグメントの中から標的とする特定のセグメントを選ぶ。その際、自社にとって魅力的なセグメントを選ぶのだが、その魅力は主に以下の要因によって規定される（Jobber and Ellis-CHadwick, 2020）： 市場要因：セグメントのサイズ、成長率、収益性 競争要因：競争の状態（数量的情報だけでなく、質的情報も重要）、新規参入者、差別化可能性 政治・社会・環境要因：政治的・社会的傾向、環境問題、技術的変化 基本的に企業にとっては、規模が大きく、成長性も収益性も高いセグメントは魅力的である。しかし、そのようなセグメントに対しては競争も激しくなると予想できる。そこで、競争の状態を観察するのだが、ここでは競合の数だけではなく競合の持っている強みが自社とどのように異なるかを検討することも重要になる。例えば、自動車メーカーにとってアメリカ市場は非常に魅力的なマーケットである。しかしながら、伝統的に欧米の自動車会社が市場を席巻しており、他地域の企業にとっては厳しい競争環境に見えるかもしれない。しかしながら、欧米の自動車会社が持っていた弱点をうまく利用し、競争を優位に進めたのは日本の自動車会社である（Jobber and Ellis-CHadwick, 2020）。つまり、競争の状態に関する質的な側面も含めてセグメントの魅力を評価すべることが重要となる。 政治・社会・環境要因の例として、近年のジェンダーや性役割に関する消費者の認識の変化が挙げられる。このような変化は、新たなセグメントの成長性へ影響を与えるかもしれない。伝統的な価値観のもとでは、女性が育児の中心を担うと思われる傾向があったが、それとは異なる価値観が台頭することによって、育児に関する製品群において男性も魅力的なセグメントになるかもしれない。例えば、自転車小売業者のサイクルベースあさひは、直線的で無骨なデザインながら、チャイルドシートや荷台を追加しやすい機能を有した「88サイクル（ハチハチサイクル）」という製品を販売している。あさひはこの製品を「パパチャリ」としてフレーミングし、家庭内での役割を果たす男性のに向けた自転車としての市場を捉えようと試みている。 セグメントの選択においては、セグメントの魅力だけでなく、自社の能力も考慮する必要がある。どれだけ魅力的なセグメントがあったとしても、そのセグメントのニーズに応えるための能力がなければ、そのセグメントを狙うことは適切ではない。また、自社能力を評価するときには、既存もしくは潜在的な競合との比較のもとで相対的に能力を評価することが大切になる。例えば、自社として製品品質に自信があったとしても、同程度の価格帯でより品質の高い製品を製造できる競合がいた場合には、相対的に能力は低いことになる。 ターゲットを決めた後企業は、製品や提供物を決定し、市場において特有のポジションを専有しようと試みる。この段階をポジショニングという。ポジショニング段階にある企業は、競争的な優位性を得ることに焦点を合わせるのだが、この時消費者に自社製品が優れていると認識されるような情報伝達も必要になる。ポジショニングと消費者の認識との関係については 11 章にて説明を行う。 次節では、定量的なデータを用いたセグメントの探索・発見方法としてのクラスター分析を紹介する。なお、本資料においてはクラスター分析の概要とRを用いた基礎的な分析方法に着目する。そのため、理論的背景や発展的手法については他の資料や著書を参照してほしい。 "],["セグメンテーションの実行とクラスター分析.html", "10.3 セグメンテーションの実行とクラスター分析", " 10.3 セグメンテーションの実行とクラスター分析 セグメンテーションでは、基準となる情報を選定し、その情報を用いて消費者の類型を発見する。その発見の方法として照井・佐藤（2023）では、（1） 経験、（2）クラスタリング、（3）潜在クラス、の3つの方法を紹介している。経験によるセグメンテーションでは、マーケティング担当者の経験や既存のリサーチによる知見を参考に消費者を類型する。この方法は、十分な知識や経験が蓄積されており、基準となる情報が限定的かつニーズと関連的である場合には有効になる。例えば、消費者の年齢がニーズと関連していることがわかっている場合、年齢に基づくセグメンテーションは特別な分析を介さずとも有効な手法となりうる。しかしながら、経験的な知見が不足している場合には、セグメンテーションの信頼性を損なうことに加えて、考慮すべき情報が複数ある場合には、情報処理が複雑になり、類型化が困難になる。 クラスタリングによるセグメンテーションは、クラスター分析を用いたセグメンテーションである。基準となる情報を（多くの場合複数）選択し、その情報をもとに各観測の類似性を求めることで、類型化を行う。この方法は、多くの情報をセグメンテーションの基準として用いることができる点や、分析方法についての資料やソフトウェアが充実しているため、分析の実現可能性が高いという利点を持つ。本資料ではこのクラスター分析を中心に議論を進める。第三の潜在クラスによるセグメンテーションでは、潜在クラスモデルと呼ばれる統計モデルを用いた方法である。これは発展的な手法であり、セグメントに関する統計的推測も行えるという利点を持っている。また、この方法では消費者が確率的に複数のグループに属することも許容するため、より現実的な手法とも評価されている（照井・佐藤,2023）。しかしながら、この方法には相対的に多くのデータを必要とし、発展的な統計的知識が必要になる。本講義においては潜在クラスモデルを用いた手法は扱わない。 本資料では、消費者や顧客のセグメントを発見するための方法としてクラスター分析を紹介する。クラスター分析は、2つ以上の基準となる情報（変数）に基づいて、対象または人を相互に排他的で網羅的なグループに分類するために使用される統計的手法である。クラスター分析にはいくつかのアプローチが存在するが、それらに共通するのはサンプル間の類似性を確認し、グループとして分割していくというプロセスを有しているということである。ここでは主に階層的クラスター分析と、被階層的クラスター分析を紹介する。階層的クラスター分析は類似する観測同士を段階的にまとめていき、グループ（クラスター）を形成していく方法である。一方で非階層的クラスター分析は、分析者の定めた前提のもと、非階層的にクラスターを形成する方法である。本資料ではおもに、これら2つのアプローチについて紹介する。 10.3.1 階層的クラスター分析 本節では、階層的クラスター分析について説明する。階層的クラスター分析では、データの中から類似している観測値を段階的にクラスターとしてまとめていき、最終的にすべてのデータが1つのクラスターになるまでそれを繰り返す方法である。 このプロセスにおいて類似度は観測値同士の距離として測られる。距離の測定方法には色々とあるが、ここではユークリッド距離とマンハッタン距離を紹介する。ユークリッド距離は、観測値同士の各座標の差の二乗和の平方根であり、マンハッタン距離は観測値同士の各座標の差の絶対値の合計であるといえる。例えば、以下の図10.2 における2点をつなぐ赤い線がユークリッド距離、青い線がマンハッタン距離だといえる。以降では、実際の分析においても頻繁に用いられるユークリッド距離に主に焦点を合わせ、手法を紹介する。 Figure 10.2: 距離定義概要 階層的クラスター分析では、図10.4 で示されているデンドログラム（樹形図）を得ることで、各観測が段階的に集約されていく様子が可視化される。例えば、図 10.4の左（1）では、6個の観測が3つのクラスターにまとめ・分類されている一方で、右（2）では、2つのクラスターに集約されている事がわかる。これらの図を見ると、aはbと最も近く、cはdと最も近いことがうかがえる。また、aとbで構成されたクラスターは、cdクラスターとは近いが、efとは遠いことがうかがえる。デンドログラムの解釈方法については後述するが、まずはデンドログラムを得るプロセスについて説明する。 ここでは、以下の表10.1 で示されている、2つの変数（x, y）に関する6個のデータが与えられた場合を考える。この場合の各観測値は図 10.3のように示される。 Table 10.1: データ例 x y a 1 2 b 2 3 c 2 5 d 3 5 e 7 2 f 6 3 Figure 10.3: 階層的クラスター分析 この時、各データ同士の類似度をユークリッド距離で測るとする。例えば、a と最も近いデータは b である。a と b の距離は、以下のように求まる。 \\[ \\sqrt{(2-3)^2+(1-2)^2}=\\sqrt{2}=1.414 \\] 同様に、c（e）と最も近いデータは d（f）であり、その類似度もユークリッド距離で求めることができる。これによって、各観測点から最も近い観測点同士を結びつける形で、3つのクラスター（図10.4 左に対応）が初期段階のものとして形成される。次に、まとめた3つのクラスターと、他の観測点もしくは他クラスターとの距離を計算し、より大きな（多くの観測点が含まれる）クラスターを形成する。例えば、クラスター（a, b）は、（e, f）よりも（c, d）のほうに近いため、図10.3 に示されているように、（a, b, c, d）というクラスターとしてまとめる（合併する）ことができる（図10.4 右に対応）。このように段階的にデータをまとめていくと、最終的にはすべてのデータを一つのクラスターとしてまとめることができる。これが、階層的クラスター分析の直感的プロセスである。 Figure 10.4: デンドログラム 図 10.4 のようなデンドログラムでは、一番下に全観測値が表示される。横軸と平行の線は、クラスタとしての併合を意味しており、下でのつながりほど初期に併合されたクラスタであることを示す。そのため、最終的には（一番上では）すべてが一つのクラスタにまとまっていることがうかがえる。この時、デンドログラムの高さ（縦軸）は距離を示している。したがってデンドログラムは、どの程度の離れ具合を許容するかによって何組でデータをまとめられるかが変わることを表している。例えば図10.4 では、高さを2に定めれば3つのクラスターにまとめることができ、4を設定すれば2つのクラスターにまとめることができる。 階層的クラスター分析ではクラスター同士を段階的に合併させていくのだが、クラスター同士の類似性を測るための方法もいくつか存在する。ここでは、代表的なものをいくつか紹介する。クラスター同士の距離の決め方として、図 10.5 に示すように、最短距離法（Single linkage）、再遠距離法（Complete linkage）、ウォード法（Ward’s method）がある。この中で、最も用いられる方法がウォード法である。ウォード法は最小分散法とも呼ばれ、クラスター内の平均までの二乗距離を最小化する方法である。 Figure 10.5: クラスター類似性測定手法 ウォード方では、異なるクラスター間の類似性について、まずそれらを構成する観測値を一括にして捉えた仮のクラスターを形成し、そのまとめられたクラスター内の観測値同士の距離が近い（分散が小さい）ものから併合される。図 10.6ではその直感的な概略図を示している。ウォード法ではこのようにクラスター間の類似性を分散の形式で測定することで段階的に併合するグループを決定していく。 Figure 10.6: クラスター類似性測定手法 階層的クラスター分析におけるクラスター数の決定は、分析者の判断に依存し、絶対的な基準は存在しない。しかしながら、多くの場合、分類の「効率性」と「有効性」のバランスから効果的なクラスター数が決定される。効率性は分類によってどれだけ多くの情報を説明できているかを表しており、より少ないクラスター数で多くのデータを説明できたほうが情報の集約による効果が大きいと考えられる。例えば、表 10.1 で示されているような6つのデータを6つのクラスターで説明しても、クラスター分析としては不適切だといえる。一方で、得たクラスター分類がどの程度現実的に含意のある分類をできているか、を捉えたのが有効性である。効率性を意識しすぎて少ないクラスター数による分類を採用しても、あまりに大雑把過ぎる分類だと分析結果が有益にならない。例えば、図 10.3 において、1つのクラスターで6個のデータを説明するよりも、2つか3つのクラスターで説明したほうがより直感的かつ実務的な含意を得ることができるかもしれない。このように、階層的クラスター分析は、クラスターがどのように形成されていくかの段階を示すことによって、おおよそどのようなクラスター分類を行うことが良さそうかを判断することにつながる。 "],["k-means.html", "10.4 非階層的クラスター分析：K-means法", " 10.4 非階層的クラスター分析：K-means法 非階層的クラスター分析では、段階的なクラスターの分類ではなく、事前に決められたクラスター数に基づき、データを分類する。本資料では代表的な非階層的クラスタリング法である、K-means法について紹介する。K-means法の分析では、分析者が事前にクラスター数を決める必要がある。その仮定の下、Sequential threshold method と言われる以下のようなプロセスによってクラスターが決定される。 初期クラスターの形成 クラスターセンターのアップデートと更新 アップデートの収束と最終クラスターの決定 図 10.7と10.8 はK-means法のプロセスに関する直感的説明を段階的に図示化したものである。ここでは、図 10.7 (1) のようなデータに対し、3つのクラスター数を仮定した場合を考える（このプロセスは任意の \\(k\\) 個のクラスターに対して適応可能）。 Figure 10.7: K-means法直感１ まず初めに図10.7 (2) に着目してほしい。ここでは、与えられたデータの空間に対して3つのクラスターセンター初期値（初期シードとも言う）をランダムにとる。次に各観測個体をそれぞれの最も近い初期センターに割り当てる形で3つのクラスターを作成する（図10.7 (3)）。ここで、作成された3つのクラスター（の観測値）と、初期値とを比較し、適切に中心を表しているかを検討する（図10.7 (4)）。例えば、図10.7 (4) における赤いクラスターについてはもう少し右上の方向にセンターを移動したほうがいいかもしれない。このように、各クラスターの観測値に基づき、クラスターセンターが計算され、アップデートされる。 Figure 10.8: K-means法直感２ しかしながら、クラスターセンターがアップデートされると、新たなセンターから各観測個体までの距離も変化する。例えば、図10.8 (5) を見ると、これまで赤クラスターに属していた個体のひとつが、クラスターセンターのアップデートに伴って赤のセンターよりも青のセンターから近くなっている。そのため、この個体は青クラスターに分類される（図10.8 (6)）。各観測個体のクラスターへの分類が変更されたため、クラスターセンターも当然修正されるべきであり、新たな分類に基づくクラスターセンターが再度計算される（図10.8 (7)）。K-means方においては、このようなプロセスをクラスターセンターが動かなくなるまで繰り返すことで、観測個体を分類する。 K-means法によるクラスターセンターの計算等については、ソフトウェアが行ってくれる。しかしながら、分析者は分析実行前にクラスター数の決定を行わなければならない。クラスター数の決定においては、10.3.1節でも述べた通り、分析者の恣意性を含む判断によって決定される。クラスター数の決定においては、伝統的には（1）ハーティガンルールと、（2）エルボー法という方法のいずれかもしくはその両方を用いるｋとが多い。 ハーティガンルールとは、\\(k\\) 個と \\(k+1\\) 個のクラスターにおける内部平方和（クラスター内部の分散）を比較する変量を計算し、その値が10を越えた場合は、\\(k+1\\)個のクラスター数を採用したほうが良いという考え方である。この値が10を越える状況は、\\(k+1\\)個の場合に比べ\\(k\\)個のクラスターを採用したほうが内部平方和（クラスター内部の分散）が大きくなることを意味している。 一方でエルボー法とは、\\(k\\) 個のクラスター数それぞれに対応する内部平方和をプロットすることで、好ましいクラスター数を解釈する方法である。図10.9に示すように、クラスター数の変化に伴い、内部平方和（クラスター内部の分散）がどのように変化するかをチェックすることができる。この図を用いた判断では、変化量（傾き）の変化に着目することも多い。変化量が小さい、つまりクラスター数が増えてもあまり内部平方和が減らない場合、効率性と有効性の観点からより少ないクラスター数を選ぶことが判断されやすい。例えば図10.9 の場合、4つのクラスターまではクラスター数の増加とともに内部平方和が下がっているものの、4を越えてからは傾きがほぼ水平になっていることがうかがえる。そのため、このような結果の場合には、4クラスターを仮定した非階層的クラスター分析を実行することが多い。 Figure 10.9: エルボー法例 ここで、セグメントの発見を目的とするような探索的なクラスター分析に関する手順を紹介する。非階層的クラスター分析は、階層的クラスター分析との組み合わせる形で実行されることも多い。顧客セグメントを探索するような目的で実行されるクラスター分析として、以下のような手順を紹介する： 階層クラスター分析、ハーティガン、エルボーの確認 クラスター数 (k) を決定 k個のクラスター数に対する K-means クラスター分析の実行 \\(\\rightarrow\\) クラスターの図示化と記述による検討 クラスターのプロファイル情報を整理し、各クラスターを評価 次節では、この4つのプロセスに基づき、Rを使ったクラスター分析方法を紹介する。 "],["rによるクラスター分析の実行.html", "10.5 Rによるクラスター分析の実行", " 10.5 Rによるクラスター分析の実行 ここからは、Rを用いたクラスター分析の実行方法について紹介する。なお、本節では、吉田秀雄記念事業財団によって2023年に実施された消費者調査アンケートデータ、研究⽀援消費者調査結果 2023年度下期 のうち、（簡単化のために）特定の変数と回答者（兵庫と東京在住者のみ）を抽出して利用する16。ここでは、以下のリストにある変数を活用する。 q12_4（ブランドロイヤリティ性向）：「たとえ多くのブランドを利用できる状況にあっても、何時も同じブランドを選ぶ。」 q13_3（価格感度）：「大抵、一番安いものを買う。」 性別 年齢 結婚有無 q5: 職業 q7_2: 世帯年収 なお、q12_4とq13_3はどちらも5点リッカート尺度で回答を得ている。その他消費者属性情報の回答項目の詳細はエクセルファイルを参照してほしい。上記の条件に沿うデータは以下のように抽出することができ、その結果、1973件の回答を得た。 df_cons &lt;- readxl::read_xlsx(&quot;data/回答データ【消費者調査2023年度下期調査】.xlsx&quot;, sheet = &quot;回答データ【共通調査2023年度下期】&quot;,na = &quot; &quot;) library(tidyverse) #東京と兵庫の県番号リスト作成 list &lt;- c(13, 28) #回答者と項目を抽出 df_cons &lt;- df_cons %&gt;% select(県番号, q12_4, q13_3, 性別, 年齢, 結婚有無, q5, q7_2) %&gt;% filter(県番号 %in% list, q12_4 != 999, q13_3 != 999) %&gt;% mutate(Pref = ifelse(県番号 == 13, &quot;Tokyo&quot;, &quot;Hyogo&quot;), Gender = case_when(性別 == 1 ~ &quot;Male&quot;, 性別 == 2 ~ &quot;Female&quot;, TRUE ~ &quot;Others&quot;), MaritalSt. = case_when(結婚有無 == 1 ~ &quot;Married&quot;, 結婚有無 == 2 ~ &quot;Not Married&quot;, TRUE ~ &quot;Others&quot;)) 本節でのクラスター分析の実行にあたっては、以下のパッケージをインストールし利用する。なお、クラスター分析自体は cluster パッケージで実行可能だが、factoextra を用いるともう少し洗練された可視化が可能になるため、こちらの紹介も行う。 install.packages(c(&quot;cluster&quot;, &quot;factoextra&quot;,&quot;ggrepel&quot;, &quot;useful&quot;)) library(cluster) library(factoextra) library(ggrepel) library(useful) 本節で我々はdf_consを用いて分析を行いたいのだが、クラスタリングの関数はデータセットに含まれているすべての変量間の類似性・相関を計算してしまうため、必要な変数のみを抽出し、分析に利用する。なお、クラスタリングの関数は文字列にも対応していないため、もしデータセットにそのような変数が含まれている場合には、この段階で取り除く必要がある。 ここではブランドスイッチ（q12_4）と価格志向（q13_3）に関する変数の抽出と、クラスター分析の実行を行うために、以下の手順を経る： 階層的クラスター分析を実行し、デンドログラムを確認 デンドログラムとエルボー法によりクラスター数を決定 2.で決定したクラスター数に従い、K-means法を実行 クラスター情報と元データ（df_cons）を結合し、プロファイル情報の整理と検討 まずは以下の通り、階層的クラスター分析を実行する。なお、階層的クラスター分析の実行においては、agnes() 関数を用いる。その際に用いる距離の計測方法は metric = という引数で設定できる。その後、階層的クラスター分析の結果を用いて pltree() 関数を実行することで、デンドログラムが出力される。 clus_cons &lt;- df_cons %&gt;% select(q12_4, q13_3) Hier1 &lt;- agnes(clus_cons, metric = &quot;euclidian&quot;, method = &quot;ward&quot;, stand = TRUE) pltree(Hier1) Figure 10.10: 消費者デンドログラム 図10.10 の通り、本データは1973件の観測個体を有するため、デンドログラムも下部については識別が難しくなっている。しかしながら、例えば高さを40ぐらいに設定した場合、クラスターは4つか5つに分かれることがうかがえる。例えば4クラスターの場合、どのような分け方になるかについて、直感的に示すために、factoextra パッケージを用いると図10.11のようにデンドログラムを出力できる。この時、fviz_dend() 関数における k = 4 という引数で、ここでのクラスター数を定義している。 alt_Hier &lt;- clus_cons %&gt;% dist(&quot;euclidian&quot;) %&gt;% hclust(&quot;ward.D&quot;) alt_Hier %&gt;% fviz_dend(k = 4, rect = TRUE, rect_border = TRUE) Figure 10.11: 4クラスターデンドログラム 続いて、上記の結果を踏まえ、エルボー法を実施する。 fviz_nbclust(clus_cons, kmeans, method = &quot;wss&quot;) Figure 10.12: エルボー法結果 図 10.12 を確認すると、4もしくは5クラスターで傾きが小さくなっているように見える。ちなみに、5クラスターを採用したケースは、デンドログラムでは図10.13のように示すことができる。 alt_Hier %&gt;% fviz_dend(k = 5, rect = TRUE, rect_border = TRUE) Figure 10.13: 5クラスターデンドログラム 4クラスターモデルと5クラスターモデルのどちらがいいのかについての判断は難しいが、ここでは便宜的に5つのクラスターを仮定する。K-means法の実施には、clusterパッケージの kmeans()関数を用いる。なお、初期クラスターセンターの割り振りはランダムで行うため、set.seed() を用いる。set.seed()のカッコの中に数値を入力することで、その数値を使う場合には常に同じ乱数を発生させることが可能になり、初期値の固定と再現性の確保ができる。 set.seed(343) K_shopping &lt;- kmeans(clus_cons,5) K_shopping ## K-means clustering with 5 clusters of sizes 80, 298, 760, 592, 243 ## ## Cluster means: ## q12_4 q13_3 ## 1 5.000000 2.150000 ## 2 3.261745 1.781879 ## 3 2.582895 3.207895 ## 4 3.447635 4.123311 ## 5 1.699588 1.777778 ## ## Clustering vector: ## [1] 5 5 5 4 3 3 3 4 2 3 3 1 3 3 3 3 4 3 3 4 2 4 5 3 2 4 3 4 5 4 3 3 3 4 3 4 3 ## [38] 4 3 3 4 5 4 3 3 4 3 3 4 2 4 2 2 4 4 2 3 3 3 3 4 3 4 3 2 1 2 1 3 4 3 4 2 4 ## [75] 2 3 5 3 4 4 4 3 4 3 2 4 3 2 3 3 3 3 4 2 3 3 3 4 4 3 3 3 4 3 3 3 4 4 3 1 3 ## [112] 3 3 3 4 4 3 4 5 4 4 5 3 3 4 3 4 4 4 3 3 3 4 1 2 4 3 2 3 1 4 3 2 4 3 3 4 4 ## [149] 3 2 5 2 4 2 3 3 3 4 3 3 4 3 4 4 3 4 4 3 3 4 4 3 2 4 5 3 2 4 3 4 3 3 3 3 3 ## [186] 4 3 3 3 3 3 2 4 3 3 2 4 4 4 3 2 3 3 4 4 1 3 3 2 4 3 5 3 3 3 4 4 5 3 2 1 4 ## [223] 5 3 5 3 4 4 5 3 4 5 3 4 3 3 4 4 5 3 3 3 5 4 3 4 2 3 2 4 3 4 3 3 4 5 4 2 3 ## [260] 3 3 3 5 2 5 4 3 2 3 5 1 4 2 3 4 3 4 3 4 3 2 1 3 3 3 2 3 4 4 2 3 4 3 4 4 3 ## [297] 3 3 2 4 3 5 4 2 2 4 4 4 4 5 3 1 3 4 4 4 3 3 5 4 5 4 4 3 2 2 4 4 5 4 3 4 5 ## [334] 3 3 4 4 3 3 3 2 1 4 3 4 3 4 3 5 5 2 3 1 2 1 3 3 3 4 2 3 4 4 2 3 3 2 2 3 3 ## [371] 4 3 3 3 3 3 3 5 3 3 4 4 3 4 3 2 4 3 4 5 3 2 4 2 3 3 4 3 3 3 3 4 4 4 1 4 3 ## [408] 4 3 2 3 3 3 3 3 4 3 1 2 4 2 5 2 3 2 3 2 4 2 3 2 3 4 3 3 3 3 3 2 3 4 3 3 3 ## [445] 3 3 3 5 3 3 3 1 3 3 4 4 3 2 3 5 3 4 1 5 4 4 3 2 3 3 4 3 5 4 3 4 4 3 3 4 5 ## [482] 4 3 4 2 3 5 4 4 4 3 4 3 4 3 3 4 5 5 3 3 3 5 3 3 1 5 3 4 3 4 5 3 2 2 5 5 3 ## [519] 2 3 3 4 2 4 3 3 3 2 5 2 4 5 5 3 3 5 3 4 2 3 3 3 3 3 3 4 4 1 4 3 3 4 2 5 4 ## [556] 3 4 3 3 2 5 4 5 3 3 4 5 4 3 2 2 4 4 3 2 4 2 3 2 3 3 2 3 3 3 4 5 3 4 3 3 4 ## [593] 2 4 3 3 4 3 4 3 4 3 3 4 3 2 2 3 3 3 2 4 3 4 2 5 5 3 5 3 3 5 3 4 3 1 3 2 3 ## [630] 3 5 1 4 3 5 3 2 3 2 4 5 2 3 1 4 4 3 2 4 3 4 3 4 3 2 4 3 4 4 3 4 3 5 4 3 3 ## [667] 3 3 3 5 2 3 2 3 4 4 3 5 3 3 4 1 3 4 1 3 5 3 5 4 5 3 3 4 2 3 3 5 2 3 3 3 4 ## [704] 3 3 4 3 3 3 3 3 5 4 1 2 3 2 4 2 2 3 5 3 3 3 5 3 4 3 5 3 2 3 4 4 5 4 3 1 3 ## [741] 4 3 5 4 3 1 3 3 4 4 3 5 4 3 4 4 4 2 3 4 3 5 3 3 3 4 4 3 1 3 5 2 4 1 2 3 2 ## [778] 3 2 3 4 3 5 5 4 3 4 4 3 2 3 3 3 1 3 3 3 3 2 4 4 3 3 2 3 4 4 3 3 4 3 5 4 3 ## [815] 4 3 4 1 4 1 4 3 4 5 2 4 2 3 4 3 2 5 4 3 4 4 3 4 2 5 3 2 2 4 3 4 4 4 4 3 5 ## [852] 3 2 4 5 4 4 5 2 2 3 1 3 4 5 5 3 2 3 3 5 2 4 3 3 2 4 2 3 1 3 2 2 2 3 2 5 1 ## [889] 4 4 5 5 5 4 2 5 2 3 4 1 4 3 5 5 3 2 4 4 4 3 2 2 4 3 3 4 3 3 3 3 3 2 3 2 3 ## [926] 5 5 4 3 4 3 4 4 3 1 2 4 5 4 3 5 1 2 4 5 4 4 5 3 3 4 3 4 4 3 4 2 4 2 4 5 4 ## [963] 3 2 4 5 4 2 4 2 3 4 3 4 3 2 4 2 3 2 3 4 2 4 2 3 3 3 2 2 4 1 3 4 1 4 3 4 4 ## [1000] 3 5 4 2 5 4 5 2 3 3 3 4 3 4 3 3 4 3 3 4 4 2 3 3 5 3 2 5 3 3 3 3 4 3 4 4 5 ## [1037] 2 4 3 4 2 3 2 2 4 3 3 3 4 4 2 5 3 3 3 4 3 3 5 4 4 4 2 3 4 5 1 4 3 1 2 5 4 ## [1074] 4 3 4 5 4 4 3 4 3 3 4 4 4 3 3 3 4 2 4 2 4 3 2 4 4 3 3 2 3 5 5 4 3 3 5 3 4 ## [1111] 2 2 3 4 5 3 4 5 4 4 3 3 3 3 3 3 4 4 3 3 4 4 3 3 4 3 2 4 5 5 5 5 2 4 3 4 3 ## [1148] 4 4 2 5 4 3 2 2 3 2 4 3 3 4 4 5 3 2 2 3 4 4 2 4 5 3 5 1 4 3 2 5 3 4 4 3 3 ## [1185] 4 1 5 2 4 5 3 3 4 4 3 4 3 5 5 2 3 2 3 4 4 3 2 4 4 3 2 4 3 3 3 2 5 4 1 3 5 ## [1222] 4 4 3 3 3 3 4 4 2 3 4 2 5 5 4 5 4 5 2 5 2 5 4 3 3 4 3 5 3 4 4 3 4 4 4 3 3 ## [1259] 2 4 2 3 4 4 4 3 4 2 4 2 4 3 4 3 2 2 4 5 5 4 3 3 3 4 4 2 3 3 4 4 3 3 5 1 4 ## [1296] 3 4 3 2 3 4 3 3 4 5 3 3 4 3 4 3 2 4 4 3 3 3 5 3 4 3 5 4 4 3 4 3 5 1 4 3 1 ## [1333] 2 4 4 5 3 4 4 3 3 3 4 3 3 5 4 3 1 3 3 5 3 3 3 4 2 3 5 3 2 4 3 4 1 3 4 1 5 ## [1370] 3 4 3 3 3 4 3 3 5 4 5 4 3 4 2 4 5 2 4 3 3 5 5 3 2 4 5 4 4 3 3 2 3 2 2 3 3 ## [1407] 4 4 5 3 4 2 3 2 3 1 2 1 2 3 2 3 3 3 4 4 1 4 2 5 3 5 2 4 4 3 4 4 4 5 3 2 3 ## [1444] 1 5 3 4 3 3 3 2 4 3 4 4 3 4 3 2 4 3 4 2 4 3 3 5 4 3 5 4 2 5 4 4 2 3 2 3 3 ## [1481] 3 2 5 5 5 4 4 1 3 4 3 4 3 2 5 3 4 3 3 5 1 4 4 4 3 3 3 3 4 3 4 3 4 2 3 3 3 ## [1518] 3 5 4 5 5 3 2 4 3 4 4 3 3 4 3 4 3 3 2 4 4 4 5 2 4 4 1 3 3 3 5 3 3 5 3 3 3 ## [1555] 3 5 4 3 2 2 4 3 3 2 3 3 3 3 3 4 4 5 5 4 2 4 3 2 3 4 4 3 1 4 2 1 5 4 2 4 3 ## [1592] 3 2 3 3 4 5 3 2 2 4 1 3 2 5 3 3 3 2 4 5 5 5 4 3 5 5 4 2 4 3 5 3 1 4 5 3 2 ## [1629] 4 5 5 3 2 3 3 4 1 3 5 2 3 3 2 4 3 2 3 2 3 5 3 2 2 5 3 2 4 4 4 3 4 2 4 2 3 ## [1666] 2 4 2 5 5 1 3 4 2 3 5 3 3 5 5 3 4 4 2 2 4 2 4 3 4 4 4 3 3 2 2 2 4 5 2 3 3 ## [1703] 5 1 3 4 5 2 3 5 5 3 2 2 4 3 3 5 3 4 4 3 3 5 3 4 4 4 3 3 4 4 5 4 4 3 3 3 3 ## [1740] 4 3 3 4 2 2 5 3 4 4 2 3 4 4 2 4 3 3 2 3 2 4 2 4 4 4 4 5 2 2 3 4 3 2 4 2 4 ## [1777] 4 4 4 4 3 2 4 4 3 4 4 5 4 5 2 2 2 2 3 5 4 5 2 2 1 4 4 2 4 1 3 5 2 4 5 3 1 ## [1814] 3 5 4 2 3 3 3 3 2 4 3 5 3 5 3 4 1 5 5 2 3 4 2 5 4 3 5 4 5 3 4 3 4 5 3 4 4 ## [1851] 4 3 3 5 1 2 5 3 3 1 2 4 3 4 3 3 4 4 1 5 3 3 3 4 1 2 3 5 5 2 4 3 2 4 4 2 3 ## [1888] 2 4 5 4 3 2 3 5 3 1 3 2 3 1 2 3 2 4 3 3 3 3 4 4 3 4 4 3 4 2 4 2 3 3 5 3 4 ## [1925] 5 3 4 3 4 4 5 4 1 4 3 3 3 2 2 5 1 4 4 3 3 3 4 4 4 4 2 4 3 4 1 4 5 2 4 3 5 ## [1962] 1 3 3 3 4 5 2 3 3 4 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 54.2000 108.4060 401.9303 622.3750 127.0700 ## (between_SS / total_SS = 66.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; 分析の結果表示される Cluster means:では、分析に用いた変数に関する各クラスターごとの平均値が出力される。例えば、クラスター1は、ブランドスイッチ（q12_4）については高いが、価格志向（q13_3）については低いことがうかがえる。結果の下部で表示される、Within cluster sum of squares by cluster:には、各クラスターの内部平方和（分散）が出力されている。その下の between_SS / total_SS は、全体の平方和に占めるクラスター間平方和の比率であり、全体の分散の何%を5つのクラスターが説明しているかを示している。 以下では、上述の結果について、factoextra::fviz_cluster()によって図示化する。図10.14 では、ブランドロイヤリティと価格志向のどちらも高い（低い）グループや、どちらか一方のみ高いグループに加え、価格志向は低いがブランドスイッチについては平均的というグループも確認できた。このように図示化することで、クラスター分析の結果についての解釈が容易になる。なお、4クラスターを採用した場合の結果については、10.15で示されている。また、4クラスターでの分析を実行すると、between_SS / total_SS = 65.6 % という結果を得るはずなので、興味のある読者は自身で実行してみてほしい。 fviz_cluster(K_shopping, data = clus_cons, geom = &quot;point&quot;) + labs(title = &quot;k = 5&quot;, x = &quot;Brand switching (Std. score)&quot;, y = &quot;Price orientation (Std. score)&quot;) Figure 10.14: 消費者価値観クラスター Figure 10.15: 4クラスターモデル図 続いて、各クラスターに属する消費者についての情報を整理、検討する。ここでは、5クラスターモデルの結果に基づき、クラスター情報と元データとを結合する。結合した結果として以下の表 10.2 に、元データにクラスター番号に関する変数 cluster_id が追加されていることがわかる。 df_cons$cluster_id &lt;- factor(K_shopping$cluster) knitr::kable(head(df_cons),caption = &quot;結合データ&quot;) Table 10.2: 結合データ 県番号 q12_4 q13_3 性別 年齢 結婚有無 q5 q7_2 Pref Gender MaritalSt. cluster_id 13 2 2 2 38 2 8 4 Tokyo Female Not Married 5 13 2 1 1 42 2 2 9 Tokyo Male Not Married 5 13 1 2 1 30 1 3 NA Tokyo Male Married 5 13 4 5 1 33 1 3 NA Tokyo Male Married 4 28 3 3 1 25 1 3 10 Hyogo Male Married 3 28 3 3 1 32 1 3 NA Hyogo Male Married 3 以下では、各クラスターの平均的な消費者像について理解するために、個人属性情報をまとめる。その作業方法と結果は、以下のコードと表10.3 のとおりである。ここでは、表10.3に基づき、いくつかのクラスターに絞り、それらの特徴を整理し解釈を行う。なお、本データでは東京在住者の観測数（1465）が兵庫在住者の観測数（508）を大きく上回っているため、居住エリアの比率については本データの比率（\\(1465/1973\\approx 0.742\\)）を基準に、この比率と同等の東京在住者がいれば1を、それより多ければ1より大きい値を取るような比率で示している。 表10.3におけるクラスター１は最も人数が少なく、価格志向は高くスイッチはやや低いことがうかがえる。そのため、このグループに属する消費者は、価格に基づいて選んだブランドを買い続ける傾向があるのかもしれない。また、このクラスターは他のクラスターよりも東京在住者の比率が高いこともうかがえる。クラスター6は、価格志向もスイッチもどちらも低いことがうかがえる。そのため、このクラスターに属する消費者は価格以外の属性に基づいて選んだブランドを買い続ける傾向があるかもしれない。また、このクラスターは既婚者率が高く、このような属性の特徴も価値観に影響しているのかもしれない。他のクラスターに関する解釈はここでは割愛するが、ぜひ読者においてもそれぞれのクラスターについての解釈を展開してみてほしい。 clus_summary &lt;- df_cons %&gt;% group_by(cluster_id) %&gt;% summarize(N = n(), Loyalty_m = mean(q12_4), Price_m = mean(q13_3), Age_m = mean(年齢), Male_r = sum(Gender == &quot;Male&quot;)/n(), Tokyo_r = (sum(Pref == &quot;Tokyo&quot;)/n())/(1465/1973), Married_r = sum(MaritalSt. == &quot;Married&quot;)/n()) knitr::kable(clus_summary, caption = &quot;クラスターサマリー&quot;) Table 10.3: クラスターサマリー cluster_id N Loyalty_m Price_m Age_m Male_r Tokyo_r Married_r 1 80 5.000000 2.150000 42.61250 0.5125000 0.8753925 0.5250000 2 298 3.261745 1.781879 38.56040 0.5134228 0.9400188 0.6107383 3 760 2.582895 3.207895 41.88816 0.5065789 1.0100683 0.5618421 4 592 3.447635 4.123311 43.79730 0.4645270 1.0191680 0.4712838 5 243 1.699588 1.777778 34.91770 0.5102881 1.0363938 0.6213992 本節では、実際の消費者アンケートデータを用いてクラスター分析の実行手順を紹介した。クラスター分析を用いて探索的にセグメントを発見するためには、階層的クラスター分析と非階層的クラスター分析を組み合わせることが重要となる。ただし、クラスター分析ではクラスター数の決定や結果の解釈などにおいて、分析者の恣意性に依存することになる。しかしながら、このような限界も理解した上でうまく利用すれば、有益なセグメントを発見することにもつながりうる。 本節では観測数2000件弱のデータを利用したが、実際に我々がこれだけのデータを目視し、セグメントを発見することは困難である。クラスター分析は人間では処理困難な情報量を集約し、解釈可能にしてくれるという強みを持つ。もちろんもっと多量のデータを用いてクラスター分析を実行することも可能であるし、機械学習への応用や潜在クラスモデルの利用など、より発展的な手法も展開されているため、興味・関心のある学生においてはさらなる学習を進めてほしい。 なお、履修生によるデータの読み込みエラーを防ぐために、自由回答設問項目は削除している。↩︎ "],["参考文献-8.html", "10.6 参考文献", " 10.6 参考文献 Jobber, D., &amp; Ellis-Chadwick (2020). Principles and Practices of Marketing 9th Edition, McGraw-Hill Education. 照井伸彦・佐藤忠彦（2022）「現代マーケティング・リサーチ[新版]」,有斐閣. "],["factor.html", "Chapter 11 知覚マップと因子分析", " Chapter 11 知覚マップと因子分析 企業におけるマーケティング諸活動は、差別的優位性を獲得するための戦略に従って一貫性を保って提供される必要がある。そのための具体的な枠組みがSTPである。10章では、主にセグメンテーションとターゲティングに焦点を合わせ、クラスター分析のやり方を紹介した。しかしながら、企業は自社の定めた標的顧客に評価され、競合よりも優位で差別的な地位を獲得するために、具体的なマーケティング活動を調整・遂行する。4Ps（Product, Price, Place, Promotion）に代表されるマーケティングの諸活動は、このSTPの枠組みに基づくマーケティング戦略の実行手段となる。したがって、STPによるマーケティング戦略の策定は、4Psのようなマーケティング活動間の一貫性に貢献する。本章では主にポジショニングに着目し、消費者の知覚に基づく企業（または製品）ポジションの可視化について紹介する。 "],["ポジショニングと知覚マップ概要.html", "11.1 ポジショニングと知覚マップ概要", " 11.1 ポジショニングと知覚マップ概要 ポジショニングとは、セグメンテーションとターゲティングの後、企業の製品や提供物を決定し、市場において特有のポジションを専有するための過程である。なお、この段階においては、製品やサービスを開発するだけではなく、自社が消費者のニーズを満たし、独自性を有するものだということを消費者に伝える必要がある。しかしながら自社の戦略とそれに準ずる製品属性を決定したからと言って、それによって自動的に企業のポジションが確立するわけではない。製品が消費者にどのように知覚されるかが問題になるため、消費者の知覚や評価に基づき、ポジションを理解する事が重要になる。 製品やサービスの特徴について、消費者知覚に焦点を合わせたポジショニングツールを「知覚マップ」という。知覚マップは市場に存在する企業・製品に対する消費者の認識の視覚的表現である。知覚マップの作成は、ある製品に関する（1）競合ブランドを特定する、（2）重要な製品属性を特定化する、（3）消費者の製品属性に関する評価をアンケート調査等から得る、（4）統計的分析を実施しプロットを描く、という段階を経る。例えば、製品の価格と品質の2点に関する消費者からの評価を製品 A, B, C, D について集計しプロットすると、知覚マップは、図11.1 のようなものとして示すことができる。このとき、AからBの各円は消費者による評価の集計結果を示しているとする。また、企業Aの狙いが点線で囲まれた黄色い楕円であったとする。この場合、A社の想定よりも品質が低く評価されていると言える。このような、企業自身想定と消費者からの評価とが一致しない場合もあるため、消費者からの評価について理解する必要がある。 Figure 11.1: 知覚マップ例 では、我々消費者はどのように製品やサービスを知覚・評価しているのだろうか。消費者がある製品を購買する際、通常いくつかのブランドを購買候補として着目し比較を行う。その際に消費者は、それらの各ブランドがどのような特徴を持つものかについて知覚を構成する。例えば、我々が製品を語るとき、「コスパ（コストパフォーマンス）が良い」や「高級感がある」といった漠然とした印象を用いることが多い。しかし、これらの製品に対する印象は通常、製品に関する複数の異なる要素を観察することで形成される。例えば、「コスパが良いノートパソコン」という印象の背後で、消費者は製品のCPU、ディスプレイ画質、操作性、デザインなど、様々な要素を複合的に評価しているはずである。これらの製品に関する要素は属性と呼ばれ、製品は複数の属性の束として考えられる。製品開発を行う企業においても、企業やブランドレベルでのマーケティング戦略や目標を実現するために具体的な製品属性を決定していくことが、マーケティングにおける意思決定の原則である。 複数の製品属性について知覚マップを形成しようとすると、情報が複雑になり、解釈も難しくなる。そのため、各属性の特徴の背後には抽象的な概念（上述では印象という言葉を使った）が存在すると考え、その概念を捉える形で各属性についての情報を集約し、解釈を行うために「因子分析」を用いる。例えば、製品の高級感という印象は抽象的な概念として捉えられ、この概念自体は観察できない潜在的なものである。しかしこの潜在的な概念は、製品の価格や品質、外観といった様々な観察可能な属性に影響しているはずである（図11.2）。このように、製品の各属性に対する評価をある漠然とした概念として集約することに役立つ分析手法が因子分析である。本章では、因子分析の概要とその応用としての知覚マップの作成に焦点を合わせることで、企業が自社のポジションを把握するためのリサーチ手法を紹介する。 Figure 11.2: 概念と属性例（高級感） "],["因子分析概要.html", "11.2 因子分析概要", " 11.2 因子分析概要 11.2.1 因子分析の概要とモデルの構造 本節では、因子分析の概要を説明する。因子分析とは観測された変数の背後に「潜在的」な共通因子 (common factor)が存在することを仮定し、その「潜在因子から観測変数への影響」の度合いを推定することで観測された変数同士の相関（まとまり）を説明しようとする方法である。因子分析は心理学分野で発展した手法であり、人々の心理的特徴（例えば、価格志向やコスモポリタニズム）を多面的に捉えることや、情報の複雑さを削減することができる。 因子分析には大きく分けて1つのアプローチが存在する。第1に、探索的因子分析である。これは、複数の観測変数間の相関関係から、その背後にいくつ潜在的な因子を導入すれば観測変数間の関係をうまく説明できるのかを探索的に調査・分析する方法である。第2に、確認的因子分析である。これは、先行研究などに基づき、因子の数と因子負荷量について仮説的な構造を想定し、その構造をデータに基づき検証する方法である。この手法は、共分散構造分析と呼ばれる分析手法を応用したものである。これら2つのうち本講義では、探索的因子分析に焦点を合わせる。 因子分析では、観測変数の値を規定するような、共通する潜在因子が存在すると考える。図11.3 は、3つの観測変数と1つの潜在因子との関係を表す因子モデルを図示したものである。因子分析では観測変数間の相関を捉えており、観測変数間に高い相関があるということは、その背後に共通する因子があると考える。図11.3 における \\(f\\) は潜在因子、 \\(x_j~(j=1,2,3)\\) は観測変数を表している。また、因子と変数の関連性は \\(a_j\\) という因子負荷量で表現され、 \\(e_j\\) は因子では説明できない変数のばらつきを表す独自因子と呼ばれる。図示化においては、一般的に観測変数は四角形、潜在因子と独自因子は円や楕円で表現される事が多い。 Figure 11.3: 因子分析モデル 図11.3のパス図は、以下のような式で表すことができる。 \\[ x_{1i}=a_1f_i+e_{1i}\\\\ x_{2i}=a_2f_i+e_{2i}\\\\ x_{3i}=a_3f_i+e_{3i}, \\] ただし、\\(i\\) は \\(1,..,n\\) の個人を指す。そのため、因子分析のモデルは、各観測変数それぞれを被説名変数とし、潜在因子を説明変数とした回帰モデルのような形で捉えることができる。このことからも、潜在因子が観測変数を規定しているという考えのもとモデルが構築されていることがうかがえる。因子分析においては、上記の式をもとに、因子負荷量を求める17のだが、ここで潜在因子を式に含めていることの問題が浮上する。説名変数として用いている潜在変数は、本来観察できないものであるため、単位や基準点が存在せず、式が変形可能になってしまい、因子負荷量についての解が定まらない（分寺、2022）。そこで、因子分析の実行においては以下のいずれかの制約をおく： 因子の分散が1、平均が0と仮定する。 1つ目の観測変数の因子負荷量を1に固定する。 このような仮定の下、因子負荷量を計算するのだが、因子負荷量の計算方法はいくつか存在する。詳細についてはぜひ分寺（2022）を参照してほしい。ここでは、分析後に算出されるいくつかの重要な指標について簡単に説明する。まずは、想定した因子モデルがどの程度うまく観測変数のばらつきを説明できているのかを表す共通性と独自性について説明する。 ここで、\\(j\\) 番目の観測変数（\\(j=1,...,J\\)）に対応する以下のような因子モデルを考える。 \\[ x_{ji}=a_jf_i+e_{ji} \\] この時、\\(\\hat{x}_{ji}\\) は \\(x_{ji}\\) の予測値、\\(\\hat{e}_{ji}\\) は 残差とすると、以下のような式を得ることができる。 \\[ x_{ij}=\\hat{x}_{ji}+\\hat{e}_{ji}=\\hat{a}_jf+\\hat{x}_{ji} \\] 上記の式において説明変数に潜在因子が使われていることには注意が必要であるが、\\(x\\) の分散と予測値や残差との関係について、回帰分析の場合と同様に、以下のように捉えることができる。 \\[ \\sum_{i=1}^n(x_{ji}-\\bar{x}_j)^2=\\sum(\\hat{x}_{ji}-\\bar{x}_j)+\\sum(x_{ji}-\\hat{x}_{ji}) \\] この時、\\(\\sum(x_{ji}-\\bar{x}_j)^2\\) をSST（Total Sum of Squares）、\\(\\sum(\\hat{x}_{ji}-\\bar{x}_j)\\) をSSE（Sum of Squares Explained）、\\(\\sum(x_{ji}-\\hat{x}_{ji}\\) をSSR（Residual Sum of Squares）と定義すると、以下のように変形できる。 \\[ 1=\\frac{SSE}{SST}+\\frac{SSR}{SST} \\] この時、右辺第一項は共通性（観測変数全体の変動のうち、因子で説明できる部分）、第二項は独自性（観測変数全体の変動のうち、因子で説明できない部分）と呼ばれる。 これまでの因子モデルにおいて \\(a_j\\)として示された因子負荷量は、因子がそれぞれの観測変数にどの程度影響を与えているかを表しており、因子負荷量が高い程、因子と観測変数の関連が強いことを示す。また、因子分析を実行すると、因子寄与率（分散比率）が算出される。これは、観測項目全体の分散を因子によってどの程度説明しているかを示している。 11.2.2 2因子モデル ここまでの説明では、1つの因子を想定した1因子モデルを紹介したが、因子分析は複数の因子を想定したモデルも採用できる。この場合も、基本的には1因子モデルと同じように計算が可能だが、複数因子モデルにおいては、（1）軸の回転と、（2）因子数の決定、という2点について追加的に考える必要が出てくる。複数因子モデルでは、因子\\(\\times\\)因子負荷量の解の座標の取り方が一意定まらないという性質を持っている。例えば、観測変数を \\(J\\)個、潜在因子を2個含むモデルを行列表記を用いて以下のように示す。 \\[ \\boldsymbol{x}=\\boldsymbol{Af}+\\boldsymbol{e} \\] ここで、上記の式は、任意の（\\(p\\times p\\)）の正則行列（逆行列が存在する正方行列）\\(\\boldsymbol{T}\\)を用いて以下のように示すこともできる。 \\[ \\boldsymbol{x}=\\boldsymbol{Af}(\\boldsymbol{TT}^{-1})+\\boldsymbol{e} \\\\ = \\boldsymbol{A}^*\\boldsymbol{f}^*+\\boldsymbol{e} \\] この\\(\\boldsymbol{A^*}\\)と\\(\\boldsymbol{f^*}\\)もまた2因子モデルの解である。そのため、座標軸の変換についての不定性がうかがえる。因子分析の実行においては、この特徴を逆手にとり、解釈が容易になるよう（単純構造化した）軸を回転させることが可能になる。ここでいう単純構造とは各変数が1つの因子だけから強い影響を受け、他の因子からの影響が0に近くなるように見える構造を意味している。 軸の回転の方法としては主に、直交回転と斜交回転という2つのアプローチが存在する。直交回転とは、因子負荷量行列に直交行列をかけた解のことである。この方法では、因子間に相関がないことを仮定している。直交回転法代表例がバリマックス回転である。一方で斜交回転は、直交ではない回転を表しており、因子間の相関を認める方法である。斜交回転法の代表例はプロマックス回転である。図11.4 は、川端ほか（2019 p.180）を参照した因子軸の回転について直感的に示した概要図である。回転なしの図では、すべての観測変数が \\(f2\\) に同じような負荷を持っていることがうかがえる。一方で、直交回転では、直交である条件は守ったままではあるが、\\(1\\sim 4\\)はf1に高いがf2には低い値を取っていることがうかがえる。さらに斜交回転においては、軸の角度を自由に取ることができ、より単純構造化されていることがうかがえる。 Figure 11.4: 因子軸の回転概要 因子分析では、モデルで採用する因子の数を自由に決定する事ができる。基本的には、より少ない因子数で全体を説明できることが好ましいのだが、説明力があり有力な因子は残すことも好ましい。ここでも、10 章での説明と同様、効率性と有効性のバランスを探ることになる。以下では、因子数を決めるために用いられている基準をいくつか紹介する。 第1に、各因子の固有値（eigen value）に基づく基準である。この基準では、各因子の確認し、1を越えていない因子についてはモデルに含めないと判断する。因子負荷量の行列は、データの相関行列を固有ベクトルと固有値（を持つ行列）にそれぞれ分解することで計算されている（分寺、2022）。この時計算される各因子の固有値はその因子の寄与率にも対応しており、固有値が1ということは、（直感的には）観測変数1項目分の分散を説明していると解釈することも可能である。そのため、固有値が1以上の因子のみをモデルに用いるという方法が慣習的に広く用いられている。しかしながら、この基準に対する批判も存在していることに注意が必要である。 第2の因子数判断基準にスクリープロットの活用がある。これは、因子数を横軸にとり、それに対応する固有値を縦にプロットしたものである。図11.5 はスクリープロットの例である。スクリープロットでは、固有値そのものに加え、因子数を増やすことによる固有値の変化量（傾き）の変化にも着目する。例えば図11.5 については、因子数が2から3への変化では傾きが急であるが、3以上になった点からプロットの傾きが緩やかになっている。この場合、3因子目は説明力が低く、それ以降の因子についても説明力が高くない事が伺える。そのため、11.5 の結果では、2因子モデルを採用することが有力となる。 Figure 11.5: スクリープロット 固有値とスクリープロットによる因子数の決定は伝統的に広く用いられている基準であるが、この他にも、乱数を用いて計算された固有値との比較（平行分析）などがあるので、関心がある人はぜひ学習してみてほしい。 11.2.3 因子スコア 因子分析ではその結果に基づき、各個体が因子に対してどれだけの特性値を持っているのか、因子の得点を割り振ることができる。このように割り振られた因子についての値を因子スコア（因子得点）という。これは、抽出された因子についての予測値として解釈でき、最も基本的なスコアの求め方は回帰推定法と呼ばれる計算方法である。この方法では、個人 \\(i\\) における因子 \\(f\\) に対して（簡単化のために1因子モデルを考える）\\(J\\) 個の観測変数と、係数 \\(b\\) を想定すると、因子スコアの理論モデルは以下のように示すことができる。 \\[ f_i = b_1x_{i1}+ b_2x_{i2}+...+ b_Jx_{iJ}+u_{ij}\\\\ = \\sum^J_{j = 1}b_{j}x_{ij} + u_{i} \\] ただし、\\(u_{i}\\) は、誤差項である。この因子の予測値を \\(\\hat{f}_i\\) とし、以下の \\(G1\\) を最小化するような \\(b\\) を求めるのが、回帰推定法である。 \\[ G1 = \\sum^n_{i = 1} \\left(f_i-\\hat{f}_i\\right) \\] 11.2.4 小括 本節では、因子分析の概要についての説明を行った。因子分析の概念的な理解のためには、観測できる変数の背後に潜在因子というものが存在することを想定する必要がある。しかしながら観察できない潜在因子を測定するためには、観測変数間の共分散を用いる。ある変数同士が似ているということは、その背後に共通する因子が存在し、その因子から影響を受けているのではないかと考えているわけである。分析の結果、複数の変数を一纏めに捉えることができる因子が発見された場合、事後的に、「それはどのような因子か」を解釈し、名前をつける。このように、データに基づく結果を事後的に解釈することで因子を発見しようとするアプローチを「探索的因子分析」という。次節では、Rを用いた因子分析の実行方法を紹介しながら、探索的因子分析の手順や結果の解釈について説明する。 詳細は、補足資料か南風原（2002）を参照。↩︎ "],["faex.html", "11.3 Rによる因子分析の実行", " 11.3 Rによる因子分析の実行 本節では、スマートフォンに関する個人の価値観について捉えた架空のデータセットを用いて、Rでの因子分析実行方法を紹介する。因子分析の実行においては、以下の手順を経ることが多い： データの特徴を確認 因子数の決定（固有値とスクリープロット確認） 因子分析の実行 抽出された因子の解釈 （必要ならば）因子スコアの計算と利用 本節で使用するデータセットでは、「スマートフォンを買う際に、あなた自身がの重視していることについて教えて下さい。」という質問に対応する以下の項目を含んでいる。なお、すべての項目が7点のリッカート尺度で構成されている。 V1: 複数のアプリを同時に快適に使える製品を買うことが大切である。 V2: 見た目が洗練されている製品が好きだ。 V3: スマートフォンは、他の機器との無線通信が快適に使用できるべきである。 V4: 人目を引くデザインをしている製品が好きだ。 V5: 操作の容易性はスマートフォンにおいて重要な要素ではない（逆転項目） V6: スマートフォンを買う上で最も重視することは、形状を含むデザインである。 ただし、V5は値が高いほど重要性が低いことを意味する項目になっていることに注意が必要である。このような質問項目を、逆転項目（Reverse-coded item）と呼ばれ、分析の際に調整する必要がある。 ここからは実際にRを用いて分析を行っていくが、そのために psychとGPArotation というパッケージを新たに使うため、以下の要領でインストールしてほしい。 install.packages(&quot;psych&quot;) install.packages(&quot;GPArotation&quot;) まずは、データとパッケージの読み込みを行う。なお、今回因子分析で用いるpsych::fa() という関数は、指定されているデータセットすべての変数を参照し分析を行ってしまう。そのため、このタイミングで、分析の対象となる変数のみを取り出したデータセットを作成、定義する。そのうえで、以下のコードではV1からV6までの変数間の相関関係を確認している。表11.1 を見ると、V1はV3、V5と正の相関が高く、V2はV4、V6と正の相関が高いことが伺える。 factor_exdata &lt;- readxl::read_excel(&quot;data/factor_ex.xlsx&quot;, na = &quot;.&quot;) library(psych) library(GPArotation) library(tidyverse) factor_exdata$V5 &lt;- 8 - factor_exdata$V5 rownames(factor_exdata) &lt;- factor_exdata$ID factor_exdata2 &lt;- factor_exdata %&gt;% select(-ID) knitr::kable(summary(factor_exdata2)) V1 V2 V3 V4 V5 V6 Min. :1.000 Min. :2.0 Min. :1.0 Min. :2.0 Min. :1.0 Min. :2.000 1st Qu.:2.000 1st Qu.:3.0 1st Qu.:2.0 1st Qu.:3.0 1st Qu.:3.0 1st Qu.:3.000 Median :4.000 Median :4.0 Median :4.0 Median :4.0 Median :4.5 Median :4.000 Mean :3.933 Mean :3.9 Mean :4.1 Mean :4.1 Mean :4.5 Mean :4.167 3rd Qu.:6.000 3rd Qu.:5.0 3rd Qu.:6.0 3rd Qu.:5.0 3rd Qu.:6.0 3rd Qu.:4.750 Max. :7.000 Max. :7.0 Max. :7.0 Max. :7.0 Max. :7.0 Max. :7.000 knitr::kable(cor(factor_exdata2), caption=&quot;相関行列&quot;) Table 11.1: 相関行列 V1 V2 V3 V4 V5 V6 V1 1.0000000 -0.0532178 0.8730902 -0.0861622 0.8576366 0.0041681 V2 -0.0532178 1.0000000 -0.1550200 0.5722121 -0.0197456 0.6404649 V3 0.8730902 -0.1550200 1.0000000 -0.2477879 0.7778480 -0.0180688 V4 -0.0861622 0.5722121 -0.2477879 1.0000000 0.0065819 0.6404649 V5 0.8576366 -0.0197456 0.7778480 0.0065819 1.0000000 0.1364029 V6 0.0041681 0.6404649 -0.0180688 0.6404649 0.1364029 1.0000000 続いて、スクリープロットを描画し、固有値を確認する。図11.6 を確認すると、因子数が2から3へ増える際に大きく固有値が低下しており、3つめの因子の固有値も1を下回っている。そのため、前節で確認した判断基準から、2因子モデルを採用し、因子分析を実行する。 cor.exdata&lt;-cor(factor_exdata2) VSS.scree(cor.exdata) Figure 11.6: スマホデータスクリープロット 因子分析では、psych::fa() という関数を用いるのだが、この関数では引数として因子数（nfactors =）、回転（rotate =）とモデルの推定方法（fm =）を指定する。以下では、斜交（プロマックス）回転を用いた2因子モデルを最尤法（ml）で分析している。 fa&lt;-fa(r = factor_exdata2, nfactors=2, rotate = &quot;promax&quot;,fm=&quot;ml&quot;) fa ## Factor Analysis using method = ml ## Call: fa(r = factor_exdata2, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 h2 u2 com ## V1 0.97 0.00 0.94 0.063 1 ## V2 -0.02 0.75 0.56 0.437 1 ## V3 0.89 -0.12 0.83 0.174 1 ## V4 -0.06 0.78 0.62 0.378 1 ## V5 0.89 0.11 0.79 0.205 1 ## V6 0.08 0.83 0.69 0.309 1 ## ## ML1 ML2 ## SS loadings 2.54 1.89 ## Proportion Var 0.42 0.32 ## Cumulative Var 0.42 0.74 ## Proportion Explained 0.57 0.43 ## Cumulative Proportion 0.57 1.00 ## ## With factor correlations of ## ML1 ML2 ## ML1 1.00 -0.06 ## ML2 -0.06 1.00 ## ## Mean item complexity = 1 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 15 with the objective function = 4.25 with Chi Square = 111.31 ## df of the model are 4 and the objective function was 0.21 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.05 ## ## The harmonic n.obs is 30 with the empirical chi square 0.65 with prob &lt; 0.96 ## The total n.obs was 30 with Likelihood Chi Square = 5.21 with prob &lt; 0.27 ## ## Tucker Lewis Index of factoring reliability = 0.95 ## RMSEA index = 0.095 and the 90 % confidence intervals are 0 0.314 ## BIC = -8.39 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML2 ## Correlation of (regression) scores with factors 0.98 0.92 ## Multiple R square of scores with factors 0.96 0.84 ## Minimum correlation of possible factor scores 0.92 0.68 分析を実行すると、いくつかの数値が表示される。Standardized loadingsは、各変数に対する標準化された因子負荷量を表示している。 ML1（ML2）は、最尤法によって算出された因子1（因子2）の因子負荷量を示している。そのうえでML1（因子1） はV1, 3, 5に対して因子負荷量が大きくかつそれらが正である。また、M2（因子2）はV2, 4, 6に対して因子負荷量が大きくかつそれらが正であることが示されている。また、その下のSS loadings は因子負荷の二乗和、Propotion Var は各因子の寄与率（説明された分散比率）、Cumulative Var は累積寄与率、Proportion Explained は潜在因子で説明されている分散のうち、各因子が占める分散の比率を表している。これらを踏まえると、因子1は「マルチタスクの快適性」、「無線通信」、「操作容易性」に、因子2は「洗練された見た目」、「人目を引くデザイン」、「デザイン」に影響を与えるものであり、2つの因子で全体の約74%を説明していることになる。この結果を踏まえ、これら2つの因子が何を表しているかを解釈することになる。例えば、因子1は「使いやすさ」、因子2は「デザインの良さ」という名前をつけるかたちで、上記の結果を解釈する事ができる。 Rを使うと簡単に、因子、観測変数と、因子負荷量との関係を図示化することもできる。fa.diagram()という関数をつかうのだが、これを用いると、高い因子負荷量を持つ因子からのみ矢印を引くように設定されている。simple = FALSE という引数を設定することで、複数の因子から線を引くことも可能であるが、図が複雑になり見にくくなる。そのため、デフォルトの設定で図示化を行うと、図11.7 の通りの結果を得る。 fa.diagram(fa) Figure 11.7: 図示化された因子モデル 因子スコアは、因子分析の結果から抽出できる。fa でストアされた結果の場合、fa$scores と指示することで、因子スコアにアクセスできる。ここでは、因子スコアを元データに結合してみる。 fs &lt;- data.frame(fa$scores) factor_exdata2$rowname &lt;- rownames(factor_exdata2) fs$rowname &lt;- rownames(fs) factor_exdata2 &lt;- left_join(factor_exdata2, fs, by = &quot;rowname&quot;) knitr::kable(head(factor_exdata2),caption = &quot;結合後データ&quot;) Table 11.2: 結合後データ V1 V2 V3 V4 V5 V6 rowname ML1 ML2 7 3 6 4 6 4 1 1.3106155 -0.2896470 1 3 2 4 3 4 2 -1.2878038 -0.2073067 6 2 7 4 7 3 3 1.1828544 -0.7996332 4 5 4 6 6 5 4 0.1474225 1.0035837 1 2 2 3 2 2 5 -1.3907544 -1.3067260 6 3 6 4 6 4 6 0.9928111 -0.2875982 せっかくなので、今結合したデータを用いて、抽出された因子に基づくクラスター分析を実行してみる。まず、階層的クラスター分析を実施し、図11.8 を参照し、便宜的に3クラスターモデルを仮定する（エルボー法などは省略）。 library(&quot;cluster&quot;) library(&quot;factoextra&quot;) library(&quot;useful&quot;) library(&quot;ggrepel&quot;) ex_cluster &lt;- factor_exdata2 %&gt;% select(ML1, ML2) Hier1 &lt;- agnes(ex_cluster, metric = &quot;euclidian&quot;, stand = TRUE) pltree(Hier1) Figure 11.8: 2因子に基づくデンドログラム ここでは3クラスターを仮定し、K-means法でクラスター分析を実施し、その結果を図示化する。なお、以下では図のみを載せるが、関心のある学生はクラスター分析の結果も出力してみてほしい。図11.9 を見ると、デザインと使いやすさのどちらも求めている顧客層はおらず、使いやすさを重視している人たちはあまりデザインを重視していない事が伺える。一方でデザインと使いやすさの両方で低いスコアを有しているグループもあり、このグループの人たちはデザインと使いやすさの両方に対するスコアが低い。 cl_1&lt;- kmeans(ex_cluster,3) clus_fa &lt;- data.frame(cl_1$cluster) clus_fa$rowname &lt;- rownames(clus_fa) factor_exdata2 &lt;- left_join(factor_exdata2, clus_fa, by = &quot;rowname&quot;) factor_exdata2$cl_1.cluster &lt;- factor(factor_exdata2$cl_1.cluster) ##Visualizing the clusters with 2 factors p1 &lt;- ggplot(data = factor_exdata2, mapping = aes(x = ML1, y = ML2, color = cl_1.cluster)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rownames(factor_exdata2))) + labs(x = &quot;Ease of use&quot;, y = &quot;Design&quot;) Figure 11.9: 2因子に基づくクラスター 以上のように因子分析は、（1）データの確認、（2）因子数の決定、（3）因子分析の実行、（4）解釈と名前付け、（5）事後的な分析、というステップを経て実行される事が多い。特にマーケティングや消費者行動論の分野では、因子分析による因子の解釈で完結するのではなく、その後の分析に因子を活用する事が多い。また、本節では因子スコアを用いた事後分析を紹介したが、マーケティング領域においては高い因子負荷量を持つ項目のみを用いた「尺度得点」を用いることも多い。例えば、「使いやすさ（EoU）」を表す変数を、\\(EoU_i=(V1_i+V3_i+V5_i)/3\\) という対応する変数の平均値によって定義する方法が。同様に、「デザインの良さ（Des）」は 、\\(Des_i=(V2_i+V4_i+V6_i)/3\\) によって捉えられる。 このような変数作成を行うと、平均値算出で用いた変数以外（例えば、使いやすさに対するV2, V4, V6）は影響を与えないと仮定してよいのか、という疑問が残るだろう。実際に、それはとても強い仮定である。このような疑問に対して統計的な判断を与えるのが、確認的因子分析というアプローチである。因子分析において、モデルに含まれていない変数の因子負荷量がゼロであるという仮定のもと、その仮定に基づくモデルとデータとの適合度を調べるような方法を確認的因子分析と呼ぶ。この方法については発展的な内容になるため本講義では割愛するが、2010年代前半ごろまではマーケティング領域で広く用いられていた手法である。 "],["アンケートデータと知覚マップ.html", "11.4 アンケートデータと知覚マップ", " 11.4 アンケートデータと知覚マップ 本節では、消費者の製品に対する知覚を図示化する形で企業のポジショニングを可視化する知覚マップの作成プロセスを紹介する。消費者の知覚について理解するためには、消費者に関する（主にアンケートによる）データが必要である。しかしながら、企業のポジションは企業同士の相対的な位置を表している。そのため、消費者レベルで収集したデータを企業レベルに集計し、企業に関する結果として図示化する必要がある。本節で紹介する知覚マップの作成は以下の手順で構成される。 複数の製品属性を捉えた、いくつかの企業に対する評価アンケートを実施する（ただし、アンケート収集プロセスは本資料では省略）。 消費者をサンプル（行）とするデータセットを、企業レベルの集計データ構造に変換する。 各企業をサンプルとする因子分析を実行し、いくつかの因子を抽出する。 抽出された各因子の因子に関する得点（因子スコアや尺度得点）をサンプル（企業）に割り当てる。 2つの因子に関する得点の関係をプロットする。 ここでは、7つのカフェチェーンに対する5つの項目について関して11人の消費者から回答を得たと想定する演習用データを用いる。ここでは、以下の5つの各項目に対して5点リッカート尺度を用いて回答を得ている。 提供されている飲み物の品質が高い 提供されている食べ物の品質が高い この企業の店舗は気持ちよく過ごせる環境である この企業の店内は楽しい雰囲気に包まれている この企業の店内は魅力的である 11.4.1 データ構造の変更 多くのテキスト等で紹介、共有されている知覚マップの作成方法やそこで利用されているデータは、すでに企業レベルで集計されている事が多い。しかしながら、消費者から複数の企業に対する複数属性についてのアンケートを取ると、多くの場合表11.3 のような構造を持つだろう。表11.3では、“y1_1” と “y2_1” はそれぞれ企業1への質問1、企業1への質問2を表している。そのため、例えば、“y1_2” は企業2への質問1を表している。なお、照井・佐藤（2022）のように、新聞等ですでに公表されている企業レベルで集計された調査結果を利用する場合、本節で紹介するデータの変換作業は不要である。 df_cafe &lt;- readxl::read_excel(&quot;data/2021_cafeSurvey.xlsx&quot;) knitr::kable(head(df_cafe), caption = &quot;カフェデータ概要&quot;) Table 11.3: カフェデータ概要 ID y1_1 y2_1 y3_1 y4_1 y5_1 y1_2 y2_2 y3_2 y4_2 y5_2 y1_3 y2_3 y3_3 y4_3 y5_3 y1_4 y2_4 y3_4 y4_4 y5_4 y1_5 y2_5 y3_5 y4_5 y5_5 y1_6 y2_6 y3_6 y4_6 y5_6 y1_7 y2_7 y3_7 y4_7 y5_7 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 3 3 5 5 2 2 2 3 4 3 2 1 4 5 5 3 5 2 4 4 5 3 4 3 4 2 2 2 4 3 4 2 3 3 4 3 2 3 4 4 4 2 4 2 4 4 2 4 4 4 4 2 4 3 4 4 3 5 5 2 2 3 3 4 2 2 3 3 3 2 4 3 3 3 3 3 3 3 3 2 4 2 4 2 4 4 5 4 5 4 4 4 3 4 4 4 4 4 4 5 3 3 2 3 2 3 3 5 3 4 2 2 4 5 3 5 3 3 3 4 3 4 4 4 4 5 4 4 3 4 3 3 3 4 2 3 3 3 2 2 3 4 4 4 2 3 3 4 2 2 2 3 5 3 2 4 3 4 3 3 3 6 4 3 4 3 4 4 3 4 4 4 2 3 3 3 3 3 3 2 3 2 4 3 1 2 2 4 5 4 4 4 4 3 3 3 3 本資料では、知覚マップの作成についてデータ構造の変更から説明する。慣習としては、消費者についての平均値や合計をとって、製品\\(\\times\\)項目の構造を持ったデータに修正する。本節で紹介する方法は、一般的に 「wide型データ」から「long型データ」への変換と呼ばれる作業である。以下では、合計を使って集計する形でデータ構造の変換作業を行っている。 #IDについての情報は含まない形で、データをlong型に変える。 #&quot;names_to&quot; と &quot;values_to&quot; によって列名が定義される。 data_reshaped &lt;- df_cafe %&gt;% pivot_longer(-ID, names_to = &quot;company&quot;, values_to = &quot;y&quot;) #企業番号と変数をそれぞれ &quot;company&quot; と &quot;k&quot; として割り振る。 ##&quot;str_extract&quot;によって正規表現を抽出 ##&quot;parse_number&quot;によって特定の値を抽出 data_reshaped &lt;- data_reshaped %&gt;% mutate(k = str_extract(company, &quot;y[0-9]&quot;), company = str_extract(company, &quot;_[0-9]&quot;) %&gt;% parse_number()) #&quot;k&quot;をwide型に変換し、変数の列を作成する。 data_reshaped &lt;- data_reshaped %&gt;% pivot_wider(names_from = &quot;k&quot;, values_from = &quot;y&quot;) #企業名の割振りと、企業レベルデータへの集計 data_renamed &lt;- data_reshaped %&gt;% mutate(company = case_when( company ==1 ~ &quot;A&quot;, company ==2 ~ &quot;B&quot;, company ==3 ~ &quot;C&quot;, company ==4 ~ &quot;D&quot;, company ==5 ~ &quot;E&quot;, company ==6 ~ &quot;F&quot;, company ==7 ~ &quot;G&quot;, TRUE~&quot;else&quot; )) brand_based &lt;- data_renamed %&gt;% group_by(company) %&gt;% summarize(q1_sum = sum(y1), q2_sum = sum(y2), q3_sum = sum(y3), q4_sum = sum(y4), q5_sum = sum(y5)) %&gt;% tibble::column_to_rownames(var = &quot;company&quot;) knitr::kable(brand_based, caption = &quot;変換後データ&quot;) Table 11.4: 変換後データ q1_sum q2_sum q3_sum q4_sum q5_sum A 45 41 46 45 45 B 35 34 37 37 38 C 32 33 38 37 34 D 38 40 39 32 36 E 39 39 36 31 35 F 37 44 39 35 39 G 43 45 44 35 44 11.4.2 因子分析の実行 前述の通りデータ構造が企業レベルに変換できたら次は因子分析と図示化テクニックを使って知覚マップを作成する。以下では、簡単にそのプロセスを紹介する。まずはスクリープロットと固有値によって因子数を検討する。図11.10 によると、傾きの変化、固有値どちらの観点からも2因子を採用することにする。 cor_b &lt;- cor(brand_based) VSS.scree(cor_b) Figure 11.10: カフェデータスクリープロット 続いて、2因子モデルで因子分析を実行し、因子を解釈する。分析の結果、1つ目の因子は問1、2, 3, 5に高い因子負荷量を持っている。一方で、2つ目の因子は、問4に高い因子負荷量を持っている。また、因子1よりは低いものの、問3についても高い因子負荷量を有している。そこで、因子1を製品と環境、因子2を店舗の雰囲気と名付けることとする。また着目すべきは、因子1は問4（この企業の店内は楽しい雰囲気に包まれている）に、因子2は問2（提供されている食べ物の品質が高い）に対して負の因子負荷量を持っていることである。そのため、因子1と2は、因子の値が高まると、これらそれぞれの観測変数が低くなると考えられる。消費者の知覚・評価の中で、これらの因子や項目の間になんらかのトレードオフ構造があるかもしれない。 Perc_cafe &lt;- fa(r = brand_based, nfactors = 2, rotate = &quot;promax&quot;) Perc_cafe ## Factor Analysis using method = minres ## Call: fa(r = brand_based, nfactors = 2, rotate = &quot;promax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## MR1 MR2 h2 u2 com ## q1_sum 0.84 0.12 0.82 0.1825 1.0 ## q2_sum 1.04 -0.38 0.83 0.1694 1.3 ## q3_sum 0.58 0.52 0.91 0.0892 2.0 ## q4_sum -0.25 1.10 1.00 0.0023 1.1 ## q5_sum 0.67 0.44 0.93 0.0654 1.7 ## ## MR1 MR2 ## SS loadings 2.64 1.86 ## Proportion Var 0.53 0.37 ## Cumulative Var 0.53 0.90 ## Proportion Explained 0.59 0.41 ## Cumulative Proportion 0.59 1.00 ## ## With factor correlations of ## MR1 MR2 ## MR1 1.0 0.5 ## MR2 0.5 1.0 ## ## Mean item complexity = 1.4 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 10 with the objective function = 5.54 with Chi Square = 19.39 ## df of the model are 1 and the objective function was 0.02 ## ## The root mean square of the residuals (RMSR) is 0 ## The df corrected root mean square of the residuals is 0.01 ## ## The harmonic n.obs is 7 with the empirical chi square 0 with prob &lt; 0.97 ## The total n.obs was 7 with Likelihood Chi Square = 0.04 with prob &lt; 0.84 ## ## Tucker Lewis Index of factoring reliability = 5.793 ## RMSEA index = 0 and the 90 % confidence intervals are 0 0.618 ## BIC = -1.91 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## MR1 MR2 ## Correlation of (regression) scores with factors 0.98 1.00 ## Multiple R square of scores with factors 0.96 1.00 ## Minimum correlation of possible factor scores 0.92 0.99 上記の因子を反映した知覚マップは図11.11 で表されている。これを見ると、企業Aはどちらの因子についても高い評価を得ている事がわかる。一方で企業Cは、製品や環境については標準的な評価を得ているものの、雰囲気については評価がとても低くなっている。反対に、企業DやEのように、製品や環境について悪い評価を得ている企業も存在する。本節では簡単に図11.11 についての紹介を行っているが、実際にレポートを書く際には、このような結果から得られる含意や実務的示唆について詳しく論じる必要があることに注意してほしい。 fs2 &lt;- data.frame(Perc_cafe$scores) brand_based &lt;- brand_based %&gt;% rownames_to_column(var = &quot;rowname&quot;) fs2 &lt;- fs2 %&gt;% rownames_to_column(var = &quot;rowname&quot;) brand_based2 &lt;- left_join(brand_based, fs2, by = &quot;rowname&quot;) p1 &lt;- ggplot(data = brand_based2, mapping = aes(x = MR1, y = MR2)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rowname)) + labs(x = &quot;Atmosphere&quot;, y = &quot;Product+environment&quot;) Figure 11.11: カフェ知覚マップ 本節では、データ構造の変換も含む知覚マップの作成手順について紹介した。このような方法により、「消費者が企業をどのように評価しているか」という観点から、自社や競合のポジションを把握する事ができる。また、この企業レベルでのデータに対して因子分析とクラスター分析を組み合わせることによって、図11.12 のような図を描画できる。この演習データの場合には少し意義が見出しづらいかもしれないが、図11.12のような可視化はより明確な実務的含意につながりうる。具体的には、2つの因子間において類似する企業を特定することができ、市場の競争環境（特に誰が競合となりうるかなど）について可視化できると考えられる。 cafe_cluster &lt;- brand_based2 %&gt;% column_to_rownames(var = &quot;rowname&quot;) %&gt;% select(MR1, MR2) #事前の階層的クラスター分析等は省略 #便宜的に3クラスターを想定している cl_cafe&lt;- kmeans(cafe_cluster,3) clus_fa_cafe &lt;- data.frame(cl_cafe$cluster) clus_fa_cafe$rowname &lt;- rownames(clus_fa_cafe) brand_based2 &lt;- left_join(brand_based2, clus_fa_cafe, by = &quot;rowname&quot;) brand_based2$cl_cafe.cluster &lt;- factor(brand_based2$cl_cafe.cluster) ##Visualizing the clusters with 2 factors p1 &lt;- ggplot(data = brand_based2, mapping = aes(x = MR1, y = MR2, color = cl_cafe.cluster)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rownames(brand_based2))) + labs(x = &quot;Atmosphere&quot;, y = &quot;Product+environment&quot;, color = &quot;Cluster&quot;,shape = &quot;Cluster&quot;) Figure 11.12: 知覚マップ &amp; クラスター 因子分析は、複数の観測変数の背後にある潜在的な因子を捉えようとする手法である。マーケティングや消費者行動では、この潜在的な因子に基づく議論も多く、因子分析は非常に重要な手法の1つである。特に、因子分析を行って因子を特定するだけではなく、それを知覚マップとして図示化したり、クラスター分析と組み合わせた分析を行うことで実務的含意を含む発見につながることが期待される。 "],["pcafa.html", "11.5 （おまけ）主成分分析との違い", " 11.5 （おまけ）主成分分析との違い 因子分析とよく対比される手法として主成分分析（Principal component analysis）がある。主成分分析は、複数の変数間の類似性を分析し、これらの変数をできるだけ少ない数の合成変数（主成分）にまとめることを目的とした分析手法である。そのため、この主成分はできるだけ高い寄与率を持つことが求められる。主成分分析は、変数が多いとき情報の損失を最小限に押さえながらできるだけ少ない（理想的にはひとつの）主成分に縮約する方法であるといえる。一方で因子分析は、観測変数を「メリハリのある分かれ方（単純構造）」にするように因子数の決定や負荷量の計算が行われる（分寺, 2022, p.55）。主成分の計算方法は本資料では省略するが、興味のある人は田中・脇本（1998）や、渡辺（2017）を参考にしてほしい。 主成分分析のモデルは、図11.13 のように示される。因子分析のパス図（11.3）と比べると、矢印の向きに違いがあることがわかる。 Figure 11.13: 主成分分析モデル 同様に、主成分モデルを式で表すと以下のようになる： \\[ z_i=b_1x_{1i}+b_2x_{2i}+b_3x_{3i} \\] 主成分分析と因子分析とではモデル化のコンセプトが異なる。主成分分析では、変数から主成分を導き出すという想定でモデル化されていることが伺える。一方で因子分析はあくまで背後にある潜在因子が観測変数を規定すると想定している。そのため因子分析を用いる場合には、因子と変数の前後関係が想定できるような現象を捉えたデータにのみ有効である。また、誤差項（独自因子）を含まないということも主成分分析モデルの特徴である。 主成分分析は計算方法においても因子分析とは異なる点を有する。結論から述べると、主成分分析は因子分析の特殊形と考えられる（渡辺, 2017 参照）。計算上では、主成分分析は因子分析における主因子法というアプローチの特殊な場合と位置づけられる。主因子法とは、ある制約のもとで、各因子が観測変数の分散を説明する寄与率を最大にするように因子を求める計算方法である。この方法では、共通性を最初に計算し、因子負荷量を算出、因子数を決定した後再度共通性を計算する。そして新しく得られた共通性をもとに再度因子負荷量を算出するというプロセスを繰り返す。計算における共通性の初期値は1や、重相関係数の2乗値、相関行列の行の相関の最大絶対値などが使われる。 これに対して主成分分析は、共通性初期値を1とし、繰り返し計算を行わない主因子法といえる。より細かく述べれば、主成分分析によって求められる主成分係数（上式の \\(b_1, b_2, b_3\\)）は項目間分散共分散行列の固有ベクトルとして計算される一方で、因子負荷量は対角要素に共通性を代入した項目間相関行列の固有ベクトルに固有値の平方根を乗じたものだといえる（渡辺, 2017）。そのため、標準化された項目を用いた主成分分析によって得る主成分係数（項目間相関行列の固有ベクトル）に固有値の平方根を乗じると、対角要素を1とした項目間相関行列の因子負荷量（ただし、繰り返し計算は行わない）と等しくなる。 Rで主成分分析を行う際は、psych::pca() を用いる。ここで、11.3 節で用いた演習用データを用いて、主成分分析を実行し、それが、「項目間相関行列の固有ベクトルに固有値の平方根を乗じたもの」と等しくなるかを検討する。まず主成分分析を実行するが、デフォルトでは軸の回転を行ってしまうので、ここでは rotate = \"none\"とする。 factor_exdata3 &lt;- factor_exdata %&gt;% select(-ID) pca_res &lt;- pca(factor_exdata3, nfactors = 2, rotate = &quot;none&quot;) pca_res ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## V1 0.93 0.25 0.93 0.074 1.1 ## V2 -0.30 0.80 0.72 0.277 1.3 ## V3 0.94 0.13 0.89 0.106 1.0 ## V4 -0.34 0.79 0.74 0.261 1.4 ## V5 0.87 0.35 0.88 0.122 1.3 ## V6 -0.18 0.87 0.79 0.210 1.1 ## ## PC1 PC2 ## SS loadings 2.73 2.22 ## Proportion Var 0.46 0.37 ## Cumulative Var 0.46 0.82 ## Proportion Explained 0.55 0.45 ## Cumulative Proportion 0.55 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 3.94 with prob &lt; 0.41 ## ## Fit based upon off diagonal values = 0.98 続いて、項目間相関係数の固有ベクトルを計算し、それに固有値の平方根を乗じる。その結果、表11.5 は（PC2については符号は異なるものの）上記の主成分分析の結果と一致している事がわかる。 #データの相関行列から固有値と固有ベクトルを抽出 cor_ex &lt;- cor(factor_exdata3) eigen_list &lt;- eigen(cor_ex) #固有ベクトルについての情報をオブジェクトとして定義 pc1_eig &lt;- eigen_list$vectors[, 1] pc2_eig &lt;- eigen_list$vectors[, 2] #固有ベクトルに、固有値の平方根を乗じる PC1 &lt;- pc1_eig* sqrt(eigen_list$values[1]) PC2 &lt;- pc2_eig* sqrt(eigen_list$values[2]) knitr::kable(data.frame(cbind(PC1,PC2)), caption=&quot;計算結果&quot;) Table 11.5: 計算結果 PC1 PC2 0.9283425 -0.2532285 -0.3005297 -0.7952496 0.9361812 -0.1308894 -0.3415817 -0.7889663 0.8687553 -0.3507939 -0.1766389 -0.8711581 "],["参考文献-9.html", "11.6 参考文献", " 11.6 参考文献 川端一光, 岩間徳兼, 鈴木雅之（2019）Rによる多変量解析入門, オーム社. 田中豊・脇本和昌（1998）多変量統計解析法，現代数学社. 照井伸彦・佐藤忠彦（2022）現代マーケティング・リサーチ[新版],有斐閣. 南風原朝和（2002）心理統計学の基礎（有斐閣アルマ），有斐閣. 分寺杏介（2022）「Chapter 6 因子分析」，統計的方法論特殊研究(多 変量解析)講義資料. 渡辺利夫（2017）Rで多変量解析, ナカニシヤ出版. "],["psm.html", "Chapter 12 価格感度測定 (PSM)", " Chapter 12 価格感度測定 (PSM) 本章では、より直接的にマーケティング意思決定に関わるマーケティング調査法について紹介する。特に、本講義では「消費者に受け入れられる価格」について洞察を得るための方法として、価格感度測定（Price Sensitivity Meter; PSM）という手法について説明する。PSMは、マーケティング戦術決定に役立つ調査・分析手法でありながら、高度な分析手法を用いない。この手法はアンケート調査による簡便なアプローチとして広く実務的に用いられている。本章では、PSM の方法と結果の解釈に加えて、PSM に関する学術的背景についても説明することで、「PSM が何を明らかにしようとしているのか」を直感的に理解できるようになることを目的とする。そのために本章では、以下の順でPSMについての説明を行う。まず次節では、PSMでどのような情報を得ることができるのかを概観するため、PSMで用いる調査方法と、Rでの分析方法を紹介する。続いて、質問の背景に存在する価格概念について説明した後、PSMの背景に存在する学術的議論について簡単に触れる。 "],["psmrunning.html", "12.1 PSMの実行と結果", " 12.1 PSMの実行と結果 価格は、マーケティング意思決定者が調整する重要なマーケティング戦術要因（4Ps）のひとつである。価格は4Psの中で唯一企業にとっての収入を生む要因であり、製品の需要量やイメージにも関連する重要な意思決定項目である。消費者の視点にたてば、価格は自身の予算制約のもとその製品が購入に値するかを判断する定量的かつ認知的な情報であると同時に、品質や高級感を推測するための心理的影響を持つ情報としても機能する。マーケティング意思決定者は、製品のマーケティング戦略や目標と一貫するように価格を設定する必要があるが、どれぐらいの価格で販売すれば、消費者が安い/高いと感じるのかを事前に把握したいと考えるだろう。このような背景のもと、PSMは、消費者の心理的側面を踏まえた価格に関する含意の提示を目的としている。 PSM では、4つの質問項目を用いて消費者の価格に対する認識を捉える。なお、質問においては、消費者の価格に関する知覚について知りたい対象（製品やサービス）を特定化する必要がある。例えば、具体的な設問の前に、製品の画像や情報を提示することで、回答者が参照する製品やサービスを特定化することも有効になる。その上で、PSMでは以下の4つの質問によって調査を行う。 （ある製品に対して）安すぎて買わない価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくらより低いと、買うには安すぎて品質に不安を感じますか？; （回答）〇〇円」 安いと感じる価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以下だと、安いと感じますか？; （回答）〇〇円」 高いと感じる価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以上だと、高いと感じますか？; （回答）〇〇円」 高すぎて買わない価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以上だと、買うには高すぎると感じますか？; （回答）〇〇円」 なお、これらの回答の回答によって得た４つ価格は、\\(4 &gt; 3 &gt; 2 &gt; 1\\) という関係が成り立っている必要がある。これらの質問への回答により得たデータは、以下の図のように回答者ベースの構造をもつはずである。 アンケート元データ構造 PSMでは、上記のデータから各価格レンジ（以下の例では、x円以下形式）に対して、安すぎる、安い、高い、高すぎる、と感じる人がどの程度いるかを集計する。ある価格水準に対し、安い（高い）や安すぎる（高すぎる）と感じる回答者がどれだけいるのかという（相対）頻度を求めることで分析を行う。以下の表はPSMでの集計を簡単に示したデータ構造である。 PSM用データ構造 Rを用いた分析においては、自身でデータ構造の変換を行う必要はない。Rでは、“pricesensitivitymeter” というパッケージの psm_analysis()という関数を用いることで分析を実行できる。なお、PSM用データ構造への集計作業もこのパッケージにおいて行ってくれる。 ここからは、実際にRを用いたPSMの実行手順を紹介する。まず分析の実行に際し、このパッケージをインストールし、起動する。 install.packages(&quot;pricesensitivitymeter&quot;) library(pricesensitivitymeter) library(tidyverse) 続いて、“psm_ex.csv” という演習用データを読み込み、psm_ex として定義する。 psm_ex &lt;- read.csv(&quot;data/psm_ex.csv&quot;, na = &quot;.&quot;) head(psm_ex) ## tch ch ex tex ## 1 963 1016 1242 1318 ## 2 866 1082 1272 1303 ## 3 795 824 1290 1297 ## 4 812 933 1211 1335 ## 5 1026 818 1228 1289 ## 6 912 987 1195 1395 このデータは、ある製品への価格について、安すぎる（tch）、安い（ch）、高い（ex）、高すぎる（tex）という上記4つの価格認識について、250人の消費者から回答を得た（と仮定する）人工データである。分析の実行においては以下のように、psm_analysis() という関数において、toocheap、cheap、expensive、tooexpensiveという引数を用いて、データセットにおけるどの変数がPSMで用いる価格情報に対応するのかを指示する必要がある。 output_psm &lt;- psm_analysis( toocheap = &quot;tch&quot;, cheap = &quot;ch&quot;, expensive = &quot;ex&quot;, tooexpensive = &quot;tex&quot;, data = psm_ex ) summary(output_psm) ## Van Westendorp Price Sensitivity Meter Analysis ## ## Accepted Price Range: 923 - 1253 ## Indifference Price Point: 1101 ## Optimal Price Point: 1058 ## ## --- ## 157 cases with individual price preferences were analyzed (unweighted data). ## Total data set consists of 250 cases. Analysis was limited to cases with transitive price preferences. ## (Removed: n = 93 / 37% of data) psm_analysis() の実行結果として、Accepted price range（受容価格域）、Indifference price point（無差別価格）、Optimal price point（最適価格）が提示される。今回の分析の結果を見れば、受容価格域は 923-1253、無差別価格は 1101、最適価格は 1058 である。受容価格域とは、安すぎると感じる人と高すぎると感じる人が少ない価格域を表している。 また無差別価格とは、高いと感じる人と安いと感じる人がつりあっていて、高くもなく安くもない価格である。一方で最適価格とは、高すぎたり安すぎたりして買わないという人が最も少ない（受容する人が最も多い）価格である。 また、PSMでは分析結果を図示化する慣習もあるが、これも psm_plot() という関数を用いて、ggplot 形式のコマンドで簡単に出力できる。 psm_plot(output_psm) + labs( x = &quot;Price&quot;, y = &quot;Share of Respondents (0-1)&quot;, title = &quot;Price Sensitivity Meter Plot&quot;, caption = &quot;Shaded area: range of acceptable prices\\nData: Randomly generated&quot;) + theme_minimal() Figure 12.1: PSM Plot 上記の図では、受容価格価格域、無差別価格、最適価格というそれぞれの結果が、“too cheap”（安すぎる）曲線、“too expensive”（高すぎる）曲線、“not cheap”（安いと感じない）曲線、“not expensive”（高いと感じない）曲線の交点によって導かれていることがうかがえる。次節以降では、これらの結果の背景にはどのような理論があるのかについて詳しく述べる。 "],["price_concept.html", "12.2 マーケティングにおける価格概念", " 12.2 マーケティングにおける価格概念 本節では、PSMの結果への理解を深めるために、マーケティングにおけるいくつかの価格概念を紹介する。PSMは消費者の内的な価格に対する認識や基準を集計する方法である。消費者の購買に関わる最も基本的な価格概念に、支払い意思額（Willingness to Pay: WTP） がある。WTPは、消費者が特定の製品に対して「最大いくらまで払ってもいい」と考えているかを捉えた価格である。単純化された理論的世界観のもとでは、消費者はある製品の価格が、自身の WTP を下回る場合には、その製品を購入すると考えられる。 しかしながら、消費者の心理的側面を考慮すると、価格が安ければいいというわけでもない。我々の日常的な買い物においては、ある製品が自身のWTPより高いので買えないということに加えて、安すぎる価格によって、（製品品質などに）不安を抱き買いたくないと考えることもあるだろう。製品やサービスの品質が事前にわからない場合、人々はなんらかの情報に基づいて品質を類推するが、価格がその手がかりになると考えられる。このように、製品の価格が安すぎても、高すぎても買ってもらえず、どうやら消費者にとって、ちょうどいい価格幅がありそうだといえる。そして、「安すぎて不安だから買いたくない」という価格から「高くて買えない」という価格で示される、消費者が受容する（製品を買う）価格帯のことを、受容価格帯（Accepted price range）という。 もう一つの心理的側面を捉えた価格概念に、内的参照価格がある。我々の日常的な買い物を振り返ると、自身が買える価格の範囲内であっても、その価格が高いもしくは安いと感じることはあるだろう。内的参照価格は、買い手がその価格が妥当かどうか判断する際の基準となる、個人の価値観を反映した価格である。つまり内的参照価格は、「高すぎて買えない」や「安すぎて買いたくない」というほど極端な価格ではないが、自身の持っている基準から消費者自身が「これは安いな」とか「これは高いな」と感じることを捉えた価格概念である。ただし、内的参照価格は対象となる製品やサービスの価値だけではなく、消費者個人の経験や他の関連製品の価格にも影響を受けることに注意が必要である。PSM では主に、受容価格と内的参照価格を中心に分析・図示化を行う。 "],["psmに関する既存研究.html", "12.3 PSMに関する既存研究", " 12.3 PSMに関する既存研究 12.3.1 受容価格に関する知見 本節ではまず、PSMのもとになっている、受容価格帯に基づく価格分析として Gabor and Grager (1966) について紹介する。PSMは主にマーケティング実務上での活用を前提提示された手法であり、学術研究分野ではあまり言及されることはない。しかしながら、PSMは前節で述べた受容価格帯と内的参照価格に関する学術的知見に基づき提唱されたという経緯がある。 受容価格帯に関する知見としては、Gabor and Granger (1966) による購買反応曲線（Buy Response Curve；以下、BRC）の知見が活かされている。本節では、この論文の知見を簡単に紹介する。詳細について関心がある場合にはぜひ論文を読んで見てほしい。逆に、PSMに関する理論的背景に関心がない場合には、「実務的に広く使われているPSMにも、学術的な蓄積がある」ということさえ頭に留めてくれれば、そのまま次節へ進んでもらっても構わない。 Gabor and Granger (1966)では価格に対する需要量の変化ではなく、ある価格においてその製品が購買される可能性（Plausibility）について捉えた議論を展開している。この研究では受容価格の上限と下限に着目し、ある価格Pにおいて、消費者がそれを安すぎると感じる確率を\\(L(P)\\)、価格Pにおいて、消費者がそれを高すぎると感じる確率を \\(H(P)\\) とした（ただし、\\(0\\leq L(P)\\leq1\\)かつ\\(0\\leq H(P)\\leq1\\)）。これらの確率は、下図で示されるように、\\(H(P)\\)が単調増加である一方で、\\(L(P)\\) は単調減少であり、\\(L(P)\\) ではなく \\(1-L(P)\\) が累積分布関数として捉えられる。 Figure 12.2: L(P)とH(P) これらを用いて、価格Pが消費者の受容価格範囲に入っている確率を \\(B(P) = 1- L(P) - H(P)\\) と定義した。ただし、\\(H(P)\\) と \\(L(P)\\) はすべての \\(P\\) に対して微分可能であり、\\(l(P)=dL(P)/dP\\)、\\(h(P)=dH(P)/dP\\)とする。\\(B(P)\\)は、ある価格Pを消費者が高すぎるとも安すぎるとも感じない確率と言い換えることができ、これを購買反応曲線（BRC）と呼んだ。BRCの形状は以下のように示すことができる。この形状から、\\(P&#39;\\) という価格水準を基準とし、それより高いないし低い価格において、購買される可能性が低くなることが予想される。 Figure 12.3: 購買反応曲線 Gabor and Granger (1966)は思いつきや恣意的に曲線の形状を決めたのではなく、いくつかの仮説に基づき、BRCの形状について議論を発展させた。これが、彼らの研究の一番の理論的貢献である。特に、ここで想定されている仮説を確認することで、BRCの議論においてどのような消費者像が想定されているのかについて理解が可能になる。 例えば論文内では、１つ目の仮説（H1）として、先述の \\(l(P)\\) と \\(h(P)\\) をそれぞれ、低すぎ頻度関数と高すぎ頻度関数と呼び、これらが対数正規の密度関数に従うという仮説を述べた。つまり、\\(p = logP\\) としたとき、\\(l(p)\\) と \\(h(p)\\) は正規分布に従うと考えられる。ここでは、Weber-Fechner law という、「ある刺激とそれに対する個人の知覚の関係は対数的である」とする理論に基づき、対数線形を用いた形状を仮定している。言い換えると、Gabor and Granger (1966) によるBRCは、価格の増加という刺激を感覚（お得感や割高感等）として知覚する消費者像を想定したモデルだと解釈できる。 仮説2（H2）では、ある価格と消費者グループにおいては、\\(l(p)\\) と \\(h(p)\\) の標準偏差は近似的にほぼ等しいと述べた。詳細は割愛するが、H1とH2を含む計4つの仮説と整合的な結果を得ることで、BRCの形状についての知見を獲得した。\\(1-L(p)\\) と \\(H(p)\\) が正規の確率分布関数に従うとするならば、\\(B(p) = 1- L(p) - H(p)\\) の理論的な形状も推察できる。ただし、\\(B(p)\\) 自体は確率密度関数ではない。Gabor and Granger (1966) では、\\(m_1\\) と \\(m_2\\) をそれぞれ、\\(l(p)\\) と \\(h(p)\\) の期待値とし、\\(b(p)=B(p)/(m_2-m_1)\\) を確率密度関数と捉えた。単純化のために H2 を仮定すると、\\(b(p)\\) は \\((m_1+m_2)/2\\) を中心に左右対称である。これらの仮説および議論のもと、\\(l(p)\\)、\\(h(p)\\)と、\\(b(p)\\) は以下のように図示できる。このように、Gabor and Granger (1966)は、いくつかの仮定を満たすような状況では、消費者の価格に対する反応を数量的に分析することが可能になることを示した。 Figure 12.4: 購買反応密度関数 12.3.2 内的参照価格に関する知見 受容価格に関するBRCに加え、内的参照価格についての知見もPSMには重要である。詳しくは兼子（2014）を参照してほしいが、内的参照価格にも幅が存在すると考えられる。つまり、受容価格帯内において安いか高いかを知覚する価格は特定の水準だけでなく、価格幅として構成されると考えられる。ある製品の価格が消費者にとって高いと感じる価格帯よりも低く、安いと感じる価格帯よりも高い価格幅に含まれている場合、その価格は「安くも高くもない」価格として知覚される。言い換えると、この安くも高くもない価格幅に基づき、その上限よりも高い価格については高いと感じ、下限よりも低い価格については安いと感じると考えられる。以下では、兼子（2014）を参考に、受容価格帯（\\(P_4-P_1\\)）と内的参照価格帯（\\(P_3-P_2\\)）を図示する。 Figure 12.5: 内的参照価格帯 上図における、\\(P_1\\) を安すぎる価格の上限、\\(P_2\\) を安いと感じる価格の上限、\\(P_3\\)を高いと感じる価格の下限、\\(P_4\\) を高すぎる価格の下限と考える。そのため、\\(P_1\\) より低い価格や、\\(P_4\\) より高い価格の場合には消費者は購買しないと仮定する。一方で、\\(P_2-P_1\\) の範囲では、安いと感じるが買うと考えられるし、\\(P_4-P_3\\) の範囲では高いと感じるが買うと考えられる。このように、受容価格帯と内的参照価格帯の関係を捉えることで、「買う買わない」に加えた消費者の価格に対する感じ方も捉えた含意を得ることができる。 "],["受容価格帯と内的参照価格帯の応用とpsm.html", "12.4 受容価格帯と内的参照価格帯の応用とPSM", " 12.4 受容価格帯と内的参照価格帯の応用とPSM PSM は、受容価格帯と内的参照価格帯に関わる知見に基づき提案されている価格決定手法である。PSMの特徴は、「安すぎる価格」、「安い価格」、「高い価格」と「高すぎる価格」の4つの質問に対する個人の価格水準を回答してもらう点にある。PSMでは特定の価格を提示しその価格に対する消費者の評価を得るという方法ではないことにも注意が必要である。つまり、「この製品が〇〇円だったらどのように感じますか？：1. 安い,…,7.高い」のような質問形態を取らない。 PSMでは、上記の4つの質問から得た回答をもとに、価格の変化に対する安い（または高い）と感じる人と安すぎる（高すぎる）と感じる人の累積比率を計算し図示化していくのだが、もう一点注意するべき点がある。それは、「高いと感じる人」の比率と「安いと感じる人」の比率を１（または100%）から引き、「高いと感じない人」と「安いと感じない人」についての比率を示す曲線を描画するということである。これらの高い（安い）と感じない人の比率を示した曲線は高い（安い）と感じる人の比率を反転させたもの（兼子，2014）や余事象（岸など，1999）と呼ばれている。以下の図は、「高い（安い）と感じない」という形に反転させた状態でのPSM分析結果例を表現した比率曲線である。下図のうち、“Too expensive” （青い実線）と “Too cheap”（赤い実線） 曲線はそれぞれ高すぎると安すぎる（と感じる人の）比率を表しており、“Not expensive” （青い破線）と “Not cheap”（赤い破線）曲線は高いと感じないと安いと感じない人の比率を示している。 Figure 12.6: PSM結果例 PSMでは、上図内の各曲線の交点から、最適価格（\\(P^*\\)）、無差別価格（\\(\\tilde{P}\\)）、最高価格（\\(\\bar{P}\\)）、最低価格（\\(\\underline{P}\\)）を算出する。\\(P^*\\) は、受容価格帯に基づく価格点である。\\(P^*\\)は高すぎると感じる人と安すぎると感じる人の比率が最も少なくなり、受容不可能な消費者が最小となる価格である。\\(\\tilde{P}\\) は内的参照価格に基づく価格点である。\\(\\tilde{P}\\)は、「安いと感じない」曲線と「高いと感じない」曲線の交点であり、安いとも高いとも感じない消費者が最多となる価格帯であり、前節 12.5 の図における \\(P_3-P_2\\)の範囲内だと感じている消費者が最も多い価格だと解釈できる。 それ以外の価格点の解釈は比率の余事象（例、高いと感じない、安いと感じない）を求めることで容易になっている。\\(\\bar{P}\\) は、高すぎると高いと感じないの曲線の交点である。この点は、「高いとは思うが高すぎるわけではない」と感じる消費者が多い点である。この点よりも低い価格では高いと感じる人が少ない（高くないと感じる人が多い）が、この点よりも高い価格では高すぎると感じる人が多くなる。言い換えると、前節における \\(P_4-P_3\\) の範囲内だと感じている消費者が多い価格だと解釈できる。一方で \\(\\underline{P}\\) は安すぎると安いと感じないという二つの曲線の交点である。この点は、「安いとは思うが安すぎるわけではない」と感じる消費者が多い点であり、前節における \\(P_2-P_1\\) の範囲内だと感じている消費者が多い価格だと解釈できる。 本節で述べた通り、12.1 節でも確認した PSM の分析結果では受容価格と内的参照価格に基づいて提示される価格水準や価格帯である。PSMは調査方法も（Rによる）分析方法も簡便であるが、それだけに結果の解釈については注意が必要である。例えば、PSMの分析結果において「最適価格」という言葉が用いられるが、この価格水準は、受容する人が最も多いことを示しており、他の文脈で用いられうる「最適」を意味しないことに注意が必要である。同様に、その他の価格帯・水準についても受容価格帯と内的参照価格帯との関連から解釈することが可能になる。PSMを利用する際には、その前提となる知識に基づき、結果に対する適切な解釈を行うことが重要になる。 "],["集計方法に関する注意点.html", "12.5 集計方法に関する注意点", " 12.5 集計方法に関する注意点 これまで紹介したPSMアプローチは、Westendorp (1976) によって提唱された方式であるが、、Newton et al. (1993) によって提唱された別のアプローチも広く採用されている。Newton et al. (1993) による方式（NMS型）でも、これまで説明した方式と同様の4つの質問を用いて最適価格、無差別価格、最高価格、最低価格を提示する。しかしながら、NMS型においては「高いと感じる」と「安いと感じる」消費者の比率の余事象は取らずに分析を行う。NMS型は広く用いられているものの、Newton et al. (1993) が誤った方法を紹介したことを起点に広まったとされている（兼子，2014）。 NMS型の PSM を用いると、最適価格と無差別価格は Westendorp 型の方式と等しい結果を得るが、最低価格と最高価格については異なる結果を得る。どちらの数値が正しいのかという点については議論があるものの、NMSは各曲線の交点についての論理的意味が見出しにくいという欠点を有している（兼子，2014）。 NMS型の考え方を簡易的に図示化する。安いおよび高いと感じる曲線を 1 (もしくは100%) から引く形で反転させない場合、最低価格（\\(\\underline{P}\\)）は「高い」曲線（青い破線）と「安すぎる」曲線（赤い実線）との交点で求められる。これは、先述の図12.5 で示されている \\(P_4-P_2\\) の範囲に感じている回答者が多くなる価格を表している。 同様に、最高価格（\\(\\bar{P}\\)）は安い曲線（赤い破線）と高すぎる曲線（青い実線）の交点で求められる。これは、\\(P_3-P_1\\) の範囲に感じている回答者が多くなる価格を表している。これらのことから、NMS型では、最高価格と最低価格の解釈が困難になると考えられる（兼子，2014）。NMS型でも前節までに説明したものと似た結果を得ることはできるのだが、その結果が既存の価格概念と整合的かつ直感的かという点に関して、問題を有している。 Figure 12.7: NMS型概要図 オンライン上では、NMS型のPSMを紹介している記事も散見される。実務的にもNMS型の方法を用いる場合もあるが、PSMの背後にある受容価格と内的参照価格に関する理論的基盤を鑑みると、Westendorp (1976) 方式のPSMアプローチのほうが、より実務的な含意を得やすい明示的な方法であると考えられる。PSMは使用の容易さから広く用いられているアプローチではあるが、本資料を読んだ学生においては、その背後に想定されている「受容価格」と「内的参照価格」という価格概念を理解したうえで活用してほしい。 "],["参考文献-10.html", "12.6 参考文献", " 12.6 参考文献 Gabor, A., &amp; Granger, C. W. J. (1966). Price as an Indicator of Quality: Report on an Enquiry. Economica, 33(129) 43-70. 兼子良久 (2014). 「Westendorpのprice sensitivity Meterに関する考察-活用上の注意点と粗問題-」, 鹿児島経済論集, 54(1-3), 29-60. 岸邦宏・内田賢悦・佐藤馨一 (1999). 「航空運賃に対する利用者の価格感度に関する研究」, 土木計画学研究・論文集, 16, 187-194. Van Westendorp, P (1976) NSS-Price Sensitivity Meter (PSM) – A new approach to study consumer perception of price. Proceedings of the 29th ESOMAR Congress, 139-167. Online available at https://www.researchworld.com/a-new-approach-to-study-consumer-perception-of-price/. Newton, D, Miller, J, Smith, P, (1993) A market acceptance extension to traditional price sensitivity measurement. Proceedings of the American Marketing Association Advanced Research Techniques Forum. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
