[["index.html", "マーケティングリサーチ講義ノート Rを使ったリサーチ基礎固め Chapter 1 はじめに（本資料について）", " マーケティングリサーチ講義ノート Rを使ったリサーチ基礎固め Takumi Tagashira 2023-07-17 Chapter 1 はじめに（本資料について） 本資料は一橋大学で開講している「マーケティングリサーチ」の講義ノートである。本講義では、定量的な分析手法のマーケティングリサーチ文脈への応用と、R (R studio) を用いた分析の実行法を紹介する。これにより、読者が本書を通じてリサーチに従事するための基礎を固めることを期待する。なお、本資料は基礎的な統計的・計量経済学的分析手法をマーケティング領域に応用することに焦点を合わせているため、統計学、計量経済学の理論そのものや、より発展的な分析手法については他の講義や著書で学習することを求める。 本資料は、経営学・商学分野の大学院修士および大学学部 3, 4 年生レベルの学生を主な読者層として想定し作成している。特に、これまで講義で基礎的な統計学・計量経済学の講義を履修しており、卒業論文や修士論文執筆のために、マーケティング分野にてこれらの知識を応用したいと考えている人たちの助力になれば幸いである。そのため、マーケティング領域で頻繁に用いられる手法の紹介だけではなく、学術的な研究を進めるための考え方についても言及している。このような特徴を持つ本資料の目的は以下のようにまとめることができる。 マーケティング領域における研究の全体像を理解する。 基本的な分析手法についての結果を適切に解釈できるようになる。 基礎的な分析手法を実行する能力を習得する。 私は、これらの目的が研究者として定量的な実証分析を実行し論文を書くことに加え、実務的な観点からも重要になると期待している。上記の目標を達成することで、リサーチプロセスをブラックボックスとして捉え、「調査会社がこう言ってたからこれが正しい」という態度で調査・分析を依頼、理解することを避けられると考えている。これを読んでいる読者の中には、現在または将来管理者としてマーケティングリサーチプロジェクトに関わる人もいるかもしれない。その場合リサーチの全体像を理解し、各プロセスについても理解する必要がある。また、リサーチそのものは他社にアウトソースすることもあるだろう。その場合であっても、調査会社が提出した結果をきちんと理解する必要がある。したがって、統計的な分析手法についての理解が必要になるのである。加えて、もしあなたが小規模な組織に所属しているのであれば、分析作業も自分でできたほうが良いかもしれない。その場合実際にソフトウェア上でデータを扱って分析作業をこなす能力も必要になる。上記の目的は、これらの実務的重要性も有していると考える。 本資料の構成は以下の通りである。まずRおよび R studioの概要及び基本的な操作方法について説明する。本資料ではフリーソフトウェアである R (R studio環境)を用いるため、実際の分析作業に移る前に各自操作に慣れてほしい。続いて前半部（Chapter 3, 4）では、研究プロセスや論文の構成について学ぶ。特にChapter 3は本資料の特色を表しており、分析方法やRの操作方法だけでなく「なんのために分析を実行するのか？」という問いに着目する。前半部では、リサーチ過程の全体像と論文の構成を始点とし、研究課題の設定や理論・仮説の役割に加えて、アンケート調査票設計について学ぶ。一方後半部分（Chapter 5 以降）ではR studioを用いて、データ処理や統計的仮説検定、回帰分析について学ぶ。その後、マーケティングで広く用いられている手法として、クラスター分析、因子分析等を学ぶ。 "],["r-と-r-studioに慣れる.html", "Chapter 2 R と R studioに慣れる", " Chapter 2 R と R studioに慣れる R（あーる）は統計、データ解析、統計グラフ作成のためのオープンソースソフトウェアである。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし、インストールが可能である。ソフトウェアには、Windows用、Mac用、Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。Rを用いる際には、多くの場合) R studio、Jupyter notebookや、Rコマンダーのようなユーザーインターフェイスが利用される。そしてRを使用する際には (Rコマンダーを使わないかぎりは) 基本的にソースコードを入力し計算、分析を行う。しかし、本講義においては後述する Cloud環境を利用するため、個人の意思でデスクトップ版をインストールする場合を除き、Rのインストールについては気にしなくて良い。 Rを用いる際に最もよく使われる環境（アプリケーション）のひとつがR studioである。そのため、本講義においても基本的にはR studioを用いることを前提とするが、R studioをデスクトップにインストールし利用する場合には、Rそのものもインストールしておく必要があることに注意が必要である。R studioは現在、Positとも呼ばれており、以下のサイトからアクセスが可能である（https://posit.co/）。 本講義では、Posit Cloudという、アカウント登録を行うことでブラウザ上でR studioを利用できる環境を勧める。R studio Desktop版 の利用においては、ディレクトリ設定などによってエラーが生じることが多々あり、個別のPC環境に合わせて対応、設定を行う必要がある。そのため、まとまった人数に対応する必要がある本講義においてはクラウド版を利用する。本講義を通じてRおよびR studioの使い方に慣れ、自身の研究や仕事等でデータ処理や分析を行う場合にはR studio デスクトップ版（通常はR studio IDEのフリーバージョンで十分）をインストールし、利用してほしい。もちろんはじめからデスクトップ版を利用してもらっても構わないが、その際には環境設定について色々と注意してほしい。「（補足）デスクトップ版の利用」という節にて、デスクトップ版の利用についての説明をしているため、関心のある読者は参照してみて欲しい。 "],["posit-cloudを始める.html", "2.1 Posit cloudを始める", " 2.1 Posit cloudを始める R studioは、Rを利用するためのアプリケーションである。R単体で使うよりも便利な機能が搭載されており、R studioを使うことでプログラミング作業を容易にすることが可能になる。最も大きな特徴としては、Rでの操作、分析を実行するための「コンソール画面」と、実行したい操作、分析のコードを記述しておく「Rスクリプト」と呼ばれるテキストファイルを一つの画面内に同時に表示できることである。そのため、Rに実行してほしいコマンドをテキストデータのように記述、修正し書き溜めておける一方で、その実行もスムーズに行え、結果も同画面内で確認することができる。 R studioをより手軽に利用できるサービスがPosit Cloudである。Posit Cloudはブラウザを通じてR studio環境を利用できるサービスであり、アカウント登録をするだけでよく、コンピュータへのRおよびR studioのダウンロードとインストールが不要である。 Posit Cloudの利用方法はとても簡単である。大まかな利用までの流れは以下のとおりである。 以下のリンク（https://posit.co/）からサイトへアクセスし、ProductsタブからPosit Cloudを選択する。 Posit Cloud 画面 その後、進んだ画面で “Get Started” \\(\\rightarrow\\) （特別な理由がなければ）Free planを選択し “Sign up” \\(\\rightarrow\\) 好きな方法でアカウントを作成する。 Sign up 画面 登録が完了すると、自身のアカウントのホーム画面へ移動する。新しいR studio セッションを開始するためには、画面右上の New projectボタンを押し、“New Rstudio Project” を選択する。 New project画面 New projectのセットアップが完了すると、Studio環境画面が表示される。 新しいRstudio 画面 Rstudioは、上記の図のような画面構成をしている。Rstudioの画面を構成する主なウィンドウはペインと呼ばれ、(1) RスクリプトでRコードの入力・編集に用いる”Source”、(2) Rの命令を直接入力し結果も表示される”Console”がなどが主な要素としてある。また、その他利用しているデータ情報、パッケージ、履歴など様々なタブが存在する。Rstudioの初回起動時にはSourceのペインは収納されているため、 Rスクリプトファイルを作成する必要がある。Rstudioは基本的に4分割画面で表示され、各ペインの配置については、Tools \\(\\rightarrow\\) Global option \\(\\rightarrow\\) Pane Layoutより変更が可能になる。Rstudioを操作する上で、基本的に重要となる情報は、(1) Source、(2) Console、(3) データやプロットに関する環境情報の3点であるので、以下のような配置がおすすめである。 左上 or 下: Source 左下 or 上: History (ただし、さほど重要ではないので畳んだ状態にしておく) 右上 or 下: Console 右下 or 上: 複数タブをまとめ 配置の目的はあくまで、必要な情報を同一画面上に表示することであるため、自身のやりやすい配置を考えてアレンジしてほしい。なお、本講義ノート内に掲載している R studio 操作画面のキャプション画像では、Posit cloudではなくデスクトップ版の画面を用いている場合もあるが、ご容赦いただきたい。 "],["補足r-studio-デスクトップ版の利用.html", "2.2 （補足）R studio デスクトップ版の利用", " 2.2 （補足）R studio デスクトップ版の利用 Posit cloud のフリーアカウントには、利用可能な時間やデータ容量に制限が存在する。自身の利用スタイルを鑑みて、Posit cloudのフリープランでは不十分である場合には、有料版へのアップグレードやDesktop版のインストールによって対応する必要がある。ここでは、 R studio Desktop の利用について紹介する。 R studioをオフライン環境で使う場合には、R と R studioの両方をインストールする必要がある。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし,インストールが可能である。ソフトウェアには、Windows用，Mac用，Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。 R studio のインストールは、以下のリンクから “Download” ボタンをクリックすることで始まる（https://posit.co/downloads/）。なお、特別な事情がない限り、無料版で十分分析が可能になる。 無料版のダウンロードが完了したら、指示に従いインストールを実施する。その際の設定はすべてデフォルトで構わない。 Rstudio Desktop インストール ただし、WindowsでのRおよびR studioのインストールには注意が必要である。特に、Rを用いる講義を受け持っていると、新たなパッケージのインストールで問題が生じるなどのトラブルが頻発する。これらの問題点に調べると、(1) 文字コードによる文字化けの問題、(2) ユーザーアカウントのホームディレクトリ名に日本語（全角）が利用されていること、(3) Rのライブラリが(勝手に) One drive 上に作成されることが原因であることが多かった。これに対して、 R の version 4.20以降からは、UTF-8の文字コードに対応したり、デフォルトでのRのインストール場所の変更（One drive上でない）が行われたりと、問題の改善が図られている。自身のホームディレクトリの名前が全角文字であるときは、ホームディレクトリ以外のローカルディレクトリを設定したほうが良い。この点に関する対応には、三重大学の奥村先生によって以下のウェブサイトに説明が記載されている（https://okumuralab.org/~okumura/stat/R-win.html）。 "],["補足デスクトップ版の利用とプロジェクト機能.html", "2.3 （補足）デスクトップ版の利用とプロジェクト機能", " 2.3 （補足）デスクトップ版の利用とプロジェクト機能 R および R studio のインストールが完了したら、アプリケーションを起動する。R studio の利用方法については基本的に Posit cloudの説明と同様である。ただし、デスクトップ版で R studio を利用する際には、「プロジェクト」機能を使うことを勧める。プロジェクトは、互いに関連し合ったファイルの集まりを指す。Rを通じた分析では、たくさんのファイルを扱うことになる。例えば、複数のRスクリプトやデータセット、加工したデータセットの保存、分析結果、出力された図表などがある。これらのファイルを手作業で一括管理することは困難である。むしろそのような管理作業に認知的な負担を費やしたくないというのが分析者の本音である。プロジェクト機能を使うことにより、作業ディレクトリとファイルの保存先をひとまとまりに指定できるため、ファイル管理の手間がなくなる。 新しいプロジェクトを作成するシンプルな方法が、Fileから作成する方法である。具体的には、File -&gt; New Project -&gt; New Directory -&gt; Create New Project -&gt;Directory nameの指定 -&gt; プロジェクトの設置場所（ディレクト）の指定、という手順で作成する。 プロジェクト作成手順1 プロジェクト作成手順2 R をデスクトップ上で利用する際には、基本的には自身のPC内にある（もしくはディレクトリにアクセス可能である）データの所在地（ディレクトリ）を特定することでデータの操作や分析を行う。これに対してプロジェクト機能を利用することでそのプロジェクトを実行している際に参照するワーキングディレクトリを固定することが可能になる。この機能によってR studioを通じたデータ処理や分析作業が容易になり、不要なトラブルを避けることが可能になるため、デスクトップでR studioを使う場合には可能な限りプロジェクト機能を利用してほしい。 "],["rの基本操作.html", "2.4 Rの基本操作", " 2.4 Rの基本操作 ここでは、Rを使用する上での基本的な操作方法を紹介する。Rはコマンド（命令）をconsoleを通じて実行することで動かすことができる。例えば四則演算であれば、以下のように命令し、計算が実行できる。 1 + 2 ## [1] 3 5 - 10 ## [1] -5 3 * 8 ## [1] 24 1/2 ## [1] 0.5 基本的に一つのコマンドは1行に書き、数字、演算記号、スペースは半角で入力する。以下は、べき乗、平方根、自然対数を計算するためのコマンドで計算できる。 2^3 ## [1] 8 sqrt(2) ## [1] 1.414214 log(2) ## [1] 0.6931472 Rは、ベクトルや行列の計算も可能である。c() という関数を用いると、ベクトルを作成できる。例えば、c(1, 3, 5) というコマンドによって(1, 3, 5)というベクトルが作成できる。作成したベクトルを使って以下のような計算も可能である。 c(1, 3, 5) + 1 ## [1] 2 4 6 ベクトルは、連続した数字の列を生成するための演算子である : を用いても作成することができる。例えば、1から100の整数を要素とするベクトルは以下のように作成することが可能である。 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 また、ベクトルの要素は文字列でも構わない。 cities &lt;- c(&quot;Tokyo&quot;,&quot;Osaka&quot;,&quot;Kobe&quot;) cities ## [1] &quot;Tokyo&quot; &quot;Osaka&quot; &quot;Kobe&quot; 上記の計算方法に加え、Rが持つ重要な特徴に、オブジェクトの定義がある。Rでは、任意の行列、ベクトル、数値などに名前をつけ定義したうえで、それを用いた計算を行うことができる。なお、Console上で以下のように定義（実行）したオブジェクトはenvironmentタブ内に表示されるため、各自確認をしてほしい。なお、定義したオブジェクトの確認・出力も簡単に行えるが、大文字と小文字は区別されるため、注意が必要である。 a &lt;- 1 b &lt;- 2 a ## [1] 1 A ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found また、定義したオブジェクトを用いた計算も実行できるため、各自以下の計算を実行し、結果を確認してほしい。 a + b a / b a ^ b なお、先程のベクトル操作と組み合わせ、ベクトル名 [i] とすることで、ベクトルの i 番目の要素にアクセスすることができる。例えば、以下のaとbというベクトルから特定の要素を取り出すことを考える。なおこの場合、同時に複数の要素を取り出すこともできる。 a &lt;- seq(10, 100, length = 10) b &lt;- 10:1 aの2番目の要素 a[2] ## [1] 20 bの2番目の要素 b[2] ## [1] 9 aの3-5番目の要素 a[3:5] ## [1] 30 40 50 aの1,3,5番目の要素 a[c(1,3,5)] ## [1] 10 30 50 分析で繰り返し必要になる機能がRで使えないときは、function()関数を使って、新たな関数を作成できる。例えば、最大値と最小値を並べて表示したい場合を考える。そのために、ここでは ‘mm’ という新たな(オブジェクトxの最小値と最大値で構成されるベクトルを返す)関数を作ってみる。 mm &lt;- function(x){ c(min(x), max(x)) } そして上記の関数を利用して、以下のオブジェクト a, b の最小値と最大値を出力する。 a &lt;- c(1, 5, 100, 2, -8, 7) b &lt;- c(1, 6, 8, 0, 120) mm(a) ## [1] -8 100 mm(b) ## [1] 0 120 しかし、すべての関数を自作するのは難しい。Rでは様々な計算を実行するための関数が用意されており、多くのマーケティング研究においては既存の関数を用いることで対応が可能である。実は上記のmm関数の中で使っている “min”や”max”も、それぞれ最小値と最大値を返す関数である。他にも例えば、meanや median があり、これらはそれぞれ平均値と中央値を計算するための関数である。 関数の利用においては例えば、’f()’のように関数名’f’のあとにカッコをつけて表記する。()の中には、引数（arguments）を用い、計算に必要な情報を指定することが必要となる。例えば、seq()という関数を用いて、2以上20以下の偶数の数列(sequence)を作ることが可能である。以下では、二通りの表記を提示するが、どちらも同じ結果を返す。 seq(from = 2, to = 20, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 特定の関数に対する引数を確認したい場合は ‘?関数名’とconsoleに命令することで確認が可能になる。例えば、seq関数について知りたければ、’?seq’ で確認できる。 Rに元から含まれている関数以外にも他者が開発してくれた関数も存在する。そしていくつかの関数をまとめたpackagesが多数存在する。これまで世界中の開発者たちが作成したパッケージを公開してくれている。パッケージは何らかの目的や課題を達成することを目的に構成されたコードライブラリであり、それらをインストールし、各セッションごとに起動することで利用できる。 CRANで公開されているパッケージは、install.packages()でインストール可能である。また、Rstudio の場合、ペインからpackagesタブ\\(\\rightarrow\\)Install\\(\\rightarrow\\)パッケージ名の入力という手順でもインストールが可能である。そしてインストール済パッケージは、library() によって起動することで、活用可能にある。ここで注意しておきたいのは、library()によるパッケージの起動はセッションごとに実行しないといけないという点である。まずは以下の通り、本講義で用いる “tidyverse” パッケージをインストールし起動してみる。“tidyverse”は、複数のパッケージから構成されているパッケージであり、データの整形・分析を行うために役立つ複数のパッケージをまとめてインストール・起動できる。 install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) "],["r-スクリプトのすゝめ.html", "2.5 R スクリプトのすゝめ", " 2.5 R スクリプトのすゝめ Rstudio環境で作業を行う際には、Consoleに直接コマンドを入力するのではなく、‘.R’ という拡張子のファイルを使って、「Rスクリプト」を作成することを勧める。RスクリプトはRでの分析に対応しRコマンドの集まりとして記されるファイルであり、SourceエディタからRスクリプトに記載されたコマンドを、Consoleを通じて実行する。 Rスクリプトを用いることの重要性は、コマンドの修正可能性と、分析の再現性という二点から理解できる。まず修正可能性として、そもそもコマンドを書くうえでは大小様々な誤りが付き物である。このような間違いに対応し適宜修正を加えていくためには、実行するコマンドを一つ一つ console に直接記載するのではなく、Rスクリプトとして分析過程を記録し、その内容に基づき記載、修正を加えることが好ましい。 第二に再現性においては、Rスクリプトとしてデータ整形・分析のプロセスを文書ファイルとして保存しておくことで研究者自身もしくは第三者が分析を再現することが可能になる。これは研究プロセスの客観性を高め、分析結果の信頼性を高めるために非常に重要な要素である。 これらに加え、Rスクリプトの利用は研究者個人の研究遂行上の利点もある。自身が行った研究であっても分析の細部に関しては時間経過とともに忘れてしまうものである。その際に、Rスクリプトによる分析プロセスの追跡可能性が役に立つ。また、同様の分析を再度別データで実施する場合も、既存のRスクリプトを応用することで効率的に分析が可能になる。これらに加え、共同研究において他の研究者と分析プロセスを共有する場合にもRスクリプトが役に立つ。 2.5.1 Rスクリプトを書く 先述のRスクリプトの利点を活かすために、Rスクリプトの作成においては、いつ作成されたなんのためのファイルなのか、そしてファイル内に記載されているコマンドがどのような意図によるものなのかがわかるように書くべきである。そのための重要になるのがコメント機能である。一つの行の中で#記号よりも後ろの部分はコメントとして処理（コメントアウト）される。この機能を使い、コメントで自然言語による説明を加えることで、コマンドの説明や意図等、自分や他人がスクリプトを見返して内容を理解できるようにする。例えば、Xという変数の平均値を求める場合、以下のようにRスクリプトを書くようにする。 #Xの平均値を求める。 mean(X) Rスクリプトは、Rstudioの左上にある+ボタンから新規作成可能である。ここでは試しに、新規Rスクリプトを作成し、“mktg01.R” という名前で保存してほしい。保存したRスクリプトはファイルから開くことができる。“mktg01.R”ファイルを作成したら、試しに以下のコマンドを書き込み、実行してほしい。Rスクリプトからコマンドを実行する際には、コマンド記入後、Rスクリプト上で実行したい行にカーソルを合わせた状態でcommand (control) + Return (Enter)を入力する。もう一度同じキーを押すと、2行目のコマンドが実行される。これらを実行することで、Rコンソール上に、下記と同じ結果が出ていることを確認してほしい。なお、コマンドを記入の際には、こまめに command (control) + s により保存することを心がけるようにしてほしい。 a &lt;- 9 sqrt(a) ## [1] 3 また、Rスクリプトを作成する際には、ファイルの冒頭に以下の説明を書き込む習慣をつけると後々見返すときに便利である。 ファイル名 目的 作成者 作成日 最新更新日 例えば、上記の内容とコマンドを含めた “mktg01.R” ファイルは、以下のようになる。 Rスクリプト例 "],["本章のまとめ.html", "2.6 本章のまとめ", " 2.6 本章のまとめ Rはデータの管理、分析、図表の作成を行うことができる統計分析プログラミング言語である。 Rを動かすには、コマンドと呼ばれる命令をコンソールを通じて実行する。 コマンドは基本的にRスクリプトに書き込んでからcommand (control) + Return (Enter)で実行する。 Rスクリプトにはコマンドだけでなくコメントを使った説明も追加する。 分析には既存の関数やパッケージを使うことが多い。 "],["参考文献.html", "2.7 参考文献", " 2.7 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. ランダージャレド（2015）「みんなのR 第2版」，高柳新市・牧山幸史・蓑田高志訳，マイナビ. "],["リサーチデザイン.html", "Chapter 3 リサーチデザイン", " Chapter 3 リサーチデザイン 本講義は定量的マーケティングリサーチ過程の全体像を理解し、それを監督・実行できるようになることを目的とする。そのためには、基本的な分析方法およびその結果を適切に解釈できるようになることに加え、実証的なリサーチプロジェクトを管理・実行する前提知識が必要になる。ただ思いつきのままにデータを取り、それを羅列しても適切な定量的研究にはならない。そこで本章では、(1) 研究（リサーチプロジェクト）によって何を明らかにするのか、(2) 調査分析の結果見いだされたものはなにを意味するか、を中心に社会科学における研究についての基礎的な考え方を整理する。本章の内容は研究者志望の大学院生にとって重要であることはもちろんだが、企業においてマーケティングリサーチに従事する人たちにとっても重要であると期待する。その理由については次節で説明する。 "],["マネジメントとリサーチの分離.html", "3.1 マネジメントとリサーチの分離", " 3.1 マネジメントとリサーチの分離 経営学部や商学部などに在籍している学生であれば、「マーケティング」に関する様々な講義や教科書に触れているだろう。しかしながら、その際にそれらの講義や教科書がどのような視点でマーケティングを捉え、議論しているのかに注意することが重要になる。マーケティングを議論する際には、(1) マーケティング実行者の視点と、(2) マーケティングリサーチャーの視点が存在する。マーケティング実行者の視点からは、マーケティングに関する意思決定を行い、実行することを目的に議論が行われる。「マーケティング・マネジメント」はこの視点の典型的な領域である。ここでは、意思決定のサポートするための既存のフレームワークを紹介することが多い。例えば、マーケティングで紹介される4Ps（Product, Price, Promotion, Place）や、STP（Segmentation, Targeting, Positioning）などは、意思決定や実行を手助けするための指針となることを目的としている。そのために、提示される議論や枠組みの精緻さよりも実務的有用性が優先されることも多い。 第二に、マーケティングリサーチャーの視点では、マーケティングや消費者、顧客に関連する信頼度の高い情報や知識を得ることを目的に議論が行われる。「マーケティングリサーチ」や「マーケティングサイエンス」はこの視点の典型的な領域である。マーケティングに関する研究では、既存の知識を疑ったり、不足している知識を発見し補う事を求める。そのうえで、学術的研究の場合には発見物による一般化可能性を、企業での研究では自社顧客への深い理解を求めることが多い。そのため、この視点では議論されている内容の精緻さや文書内の論理的一貫性が優先される。 企業内におけるマーケティング実務においては当然マーケティング実行者の視点が優先されるわけだが、マーケティングリサーチもこれと無関係ではない。実務的な意思決定を助けるために、マーケティング研究を実施する場合もある。マーケティング研究を通じた実務的意思決定への知見の提供では、同一企業内で調査・分析を実施する場合もあれば、調査会社等にそれらをアウトソースする場合もある。いずれの場合においても、企業活動のためにマーケティング研究を実施する際には、「マネジメントとリサーチの分離」が重要になる。マーケティング実務に関する調査分析を行う場合、研究を発注する主体でありマネジメントを主な仕事とする「クライアント」と、それを受け研究に従事する「リサーチャー」が存在する。リサーチャーは、クライアントが直面している実務的問題に基づき研究課題を設計し、調査や分析を実行する。そしてその研究成果をレポートにまとめ、クライアントに報告する必要がある。 リサーチャーは、レポート執筆において「ありのままを書く」必要がある。具体的には研究方法、結果、結論について正確に記述し情報を提供しなければならない。読み手であるクライアントが期待する、または望んでいる結果にレポートの内容を書き換えてはいけない。例えば、クライアントは特定のマーケティング戦略（例、モバイル広告）を実行し成果をあげることに自身の出世や昇給がかかっており、モバイル広告がその企業にとって有益であるというエビデンスを「欲しがっている」かもしれない。その上で、リサーチャーによる調査と分析の結果、モバイル広告による成果の向上が確認されなかった場合を仮定する。このとき、クライアントとリサーチャーの間に利害関係がなければ、「ありのまま」を伝えるのは難しくないかもしれない。しかしながら、もしこのクライアントがリサーチャーの直属の上司であるならばどうだろうか。ひどい場合であれば、クライアントの不利になる結果を提示したとしてリサーチャーが悪い評価を受けるかもしれないし、客観的な評価に影響がなくても、個人的な反感を買うかもしれない。もしくは、リサーチャー自身がこのような可能性を危惧し、「ありのまま」を伝えることに躊躇してしまうことも十分に考えられる。大前提として、研究結果に基づき（クライアントとリサーチャーどちらも）従業員の評価を行うことは避けるべき問題であり、研究結果は研究結果として受け入れることが重要である。しかしながら、人間である以上このような正論では割り切れない部分も出てくるかもしれない。そのため、クライアントとは利害関係のないリサーチャーに研究を発注することが何より重要であり、クライアント側の組織においても研究結果に基づき人事的評価を行わないという前提やルール作りが必要になる。これらが満たされていないと、クライアントにとって都合の良い結果を求めることで適切な研究が設計されなかったり、不利な研究結果受け入れないといった行動を取る誘因がクライアント側に存在することになる。 上記の例は、企業内における利害関係を捉えたものであるが、学術的研究においても注意が必要である。自身の経験や知識に基づいて、「このマーケティング戦略は効果があるはずだ」や「この戦略の有用性を示したい」と考えている場合、意識的もしくは無意識的に自身の都合の良い方法や結果を選別してしまうかもしれない。また、研究者が企業との共同研究に従事している場合には、先述のクライアントとリサーチャーの関係と同様の問題に直面するかもしれない。そのため、あなたが企業、学術どちらの研究に従事するとしても、マネジメントとリサーチとを分離し、研究結果をありのままに記述、報告できる状況を整えることが何より重要である。 "],["リサーチデザインと論文の構成.html", "3.2 リサーチデザインと論文の構成", " 3.2 リサーチデザインと論文の構成 良い研究を遂行するためには、自身の立てた研究上の問い、議論される仮説、調査・分析手法と、結論間の一貫性を保った形でリサーチデザインを決定する必要がある。リサーチの全体像を把握・理解するために本章ではまず学術論文の構成を整理しつつ、それぞれの構成要素の役割について把握する。マーケティングに関する学術論文は（様々な形態があるが、一般的には）以下の構成として整理できる。なお、以下の内容はマーケティングに関する定量的な実証研究についての形式であるため、アプローチが異なれば構成は変わると考えられる。例えば、定性的なアプローチを用いたテキストとしてはベルクほか（2016）による著書が挙げられる。また、項目の順番も論文の特性によって前後することがあるため、以下の提示内容はあくまで参考程度のものである。 イントロダクション 現実的（substantive）トピックの文献レビュー 理論的（theoretical）基盤の文献レビュー 仮説の提示 調査分析手法 分析結果 まとめと議論 イントロダクションの役割は論文としての「問い」と、その問いにどのように回答するかを簡潔かつ明示的に説明することである。そのうえで、その問いについて答えることがなぜ重要なのか、どのような価値があるのかについて、論文の読み手に理解させる必要がある。そのためには、（１） 論文として着目する問題（および問題の背景）の明示、（２） 研究課題の提示、（３） どのような研究群にどんな貢献を与えるのかについての説明、（４） 論文の概要（実際にどんな結果を得たかなど）の説明、が必要になる。 第二に、現実的（substantive）なトピックの文献レビューについてだが、ここでは着目する現象や実務的課題に対応した先行研究のまとめを記述する。マーケティング研究における文献レビューは主に、（1） substantive なトピックベースで、先行研究の潮流をまとめ、課題を見出すレビューと、（2） theoretical （理論的）な議論を整理したり、理論的課題を見出すレビューとに分けることができる。例えばあなたが、「ソーシャルメディアでの炎上における消費者行動を、Fluency theory （Schwarz, 2004; Schwarz et al., 2021） を用いて研究をする」という研究目的を持っていたと考える。このとき、「ソーシャルメディアでの炎上」に関する研究群を網羅的にレビューし、批判的検討を行うことは、substantive な文献レビューであると考えられる。多くの場合、substantiveな文献レビューによって、「そのトピックにおいて今までなにがわかっていて、何がわかっていないのか」を明確にし、研究課題を導出することが多い。一方で、Fluency theory とはどういうものであり、これまでどのような研究蓄積があるのかを整理し議論するような文献レビューは、theoretical な文献レビューであるといえる。また、可能であれば、theoretical なレビューにおいてどのような理論の精緻化もしくは拡張可能性があるのかについて議論することが好ましい。 マーケティング領域の研究では、substantiveな文献レビューによって研究課題の明確化、提示している論文が多いものの、理論の精緻化の必要性から課題を明確化することももちろん可能である。つまり、文献レビューと言ってもその役割は大きく分けて二つあり（substantive vs. theoretical）、自身がどのような目的を持ってレビューを行っているのかについてその都度自覚的になる必要がある。その上でこのパートでは、「今までなにがわかっていて、何がわかっていないのか」を明示的に伝える必要がある。このパートではイントロダクションでは説明しきれないより詳細な既存研究の整理を行う。そのためには、著者自身が着目している研究領域や潮流を明確化し、提示した研究課題およびそれへの回答が、どのような貢献をその研究領域に与えるのかについて説明することが求められる。 第三の理論的基盤の文献レビューは先述の theoretical な文献レビューに相当する。このパートにおいては、著者の研究課題を解明するために依拠する理論やメカニズムを提示することが求められる。ここでは主に、研究の文脈に依存しない抽象度の高い理論的枠組みを定義、整理し、議論する。そのため、理論的基盤の参照のためにはマーケティングや経営領域に限らない分野の研究を参照することになる。例えば、先述のFluency theoryとは、人々の情報処理や思考過程を捉えたメタ認知理論であり、主観的な情報処理の容易さ（流暢性）により、態度形成や意思決定に影響を与えることが知られている（Schwarz, 2004）。Fluency theoryはソーシャルメディアにおける炎上固有の理論ではなく、様々な異なる文脈において議論、分析されてきた（Schwarz et al. 2021）。理論的基盤の議論ではこのような抽象度の高い議論について、説明的な記述をすることで、どのような見方で研究対象を論じるのかを明確にすることが求められる。また、理論の定義や潮流の整理に加えて、着目している理論の限界や拡張可能性などを見出した場合には、それも議論し、精緻化されたモデルを提示することが好ましい。 第四の仮説の提示においては、先述の理論的枠組みに基づき具体的な変数1に対応する仮説を提示する。なお、「理論」と「仮説」は、「問いと分析をつなぐ理論・仮説」という節で説明するように、根本的には同じものだと考えられる。しかしここでは理解のしやすさを優先し、便宜上研究文脈に依存しない抽象的な記載を理論、それよりも具体的で文脈依存的な予想を仮説と仮定する。仮説は通常、「H1: 〇〇が高まると、〜〜が高まる。」というような説明として提示される事が多い。なお、この仮説で用いられる〇〇などは、分析で扱う具体的な変数と一致していることが求められる。 これに加えて、このパートでは仮説のみではなく、「なぜ、もしくはどのようにして仮説のような予想が可能なのか」という、仮説導出に関わる論拠や理屈を説明することが求められる。論文内で複数の仮説を提示する場合には、各仮説ごとにその論拠も含めて明示的に提示するこが求められる。また、例外や対抗仮説（別の有力な理論）がある場合にはそれも提示し、どのような結果になれば、対抗仮説ではなく著者が依拠する理論仮説が支持されることになるかについても論じるとよい。 第五の調査分析手法の説明において研究者は、自身仮説を検証するために使う変数をどのように測定、観察（データとして収集）するのかについて、具体的に説明する必要がある。例えば、データ収集方法として、仮説検証のための証拠となるデータを、誰が、どこで、どのような方法で集めた、どんなデータなのかを説明することが求められる。マーケティングにおけるデータ収集では、二次データ（企業のIR情報、政府統計など）、質問紙調査（郵送、オンライン）などを活用することが多い。データの種類についての詳細は「データの種類とアンケートデザイン」節で説明する。これに加えてこのパートでは、どのような分析手法を用いるのかについても説明する必要がある。統計的な分析を実施する場合には、その分析モデルや手法についても明示することが求められる。 第六の分析結果では、分析の結果をありのまま提示し、それが論文内で主張している仮説と整合的な（仮説を支持する）結果なのかを評価し、説明する。統計的な分析を行う場合には、統計学的な原則と自身が提示する結果の解釈が整合的であるかどうかに注意が必要となる。また、ここでは必要な情報を網羅しながらも、表などを用いながら簡潔に読みやすく構成することが求められる。 最後に議論パートでは、この研究がどのような理論的貢献と、実務的含意を有しているのかについて説明する。 理論的貢献では、本研究が既存の知識体系にどのような新たな知見を提供したのかを明示的かつ簡潔に説明する。実務的含意では、研究結果に基づく解釈として、マーケティング実務者へ具体的にどのような行動指針を提示できるかを述べる必要がある。また、このパートでは、研究の限界（手法上の限界など）についても説明する必要がある。なお、限界を説明する場合にはその限界に基づき、どのような将来的な研究機会を見いだせるのかについても説明することが好ましい。 上記で紹介した構成は、研究を行い、それを文章化する上で必要な情報を捉えている。そのため、研究者は、現実的なトピック、理論、手法の三つの側面について知識を蓄積し研究に望むことが求められる。本章の以降の節では、より良いリサーチデザインの策定のために必要となる、マーケティング研究に関する知識や考え方を紹介する。具体的には、(1) リサーチの目的、(2) 研究課題の設定、(3) 理論と仮説に着目する。 変数の定義については色々とあるが、ここではデータにおける観測対象となっている情報とする（倉田・星野、2011）。↩︎ "],["研究を行うことの目的.html", "3.3 研究を行うことの目的", " 3.3 研究を行うことの目的 ここでは、研究を行うことそのものの目的について考える。マーケティング研究を実施することの目的は、「既存の知見では明らかにされていない何らかのマーケティングに関する問題に対して、信頼性が高く客観的な発見物を提供する」ことであるといえる。もし、既存の知見で問題に対する回答が提示できるのであれば、研究を実施する必要はないはずである。そのために学術的な研究でも、企業における研究でも、未知の問題を明らかにしようと試みる場合には、科学的な考え方や方法を応用し、知識を得ようとする。しかしながら、マーケティングにおいてどのようなプロセスを採用し研究することが「科学的である」と考えられるのかという点に関しては、様々な考え方（主義）が存在する。 「科学的であるということはどういうことか。」この問題について考える領域は科学哲学と呼ばれており、以下の内容は社会科学における科学哲学（吉田, 2021）とマーケティング研究におけるパラダイム議論（宇野ほか, 2022）の内容を要約したものである。そのため、この議論に関心のある場合には、これらの文献およびそこで提示されている関連図書を参照してほしい。科学哲学の潮流においては、論理実証主義、反証主義、解釈主義、などいくつかの主要な考え方が存在する。宇野ほか（2022）によると、マーケティング領域においては、と実証主義と解釈主義という二つの主義が主流のパラダイムとして存在する。実証主義では研究者は現象から独立した存在という立場のもと、普遍的な法則見出すことで知識を形成すること目指す。一方で解釈主義では、研究者が現実から独立し事象を観察することは困難であるという立場をとる。 その上で本書では、より広範な社会科学における科学哲学との関連で議論が可能になるように、吉田（2021）の議論に従い、主に自然主義と解釈主義という二つの考え方に大別し議論を整理する。第一に、自然主義は現在の社会科学研究における有力な立場であり、社会現象は自然現象と同じように研究できると考える。これは、特に数学や物理学を中心に自然科学として蓄積されてきた方法論や考え方を採用することで社会現象に関する客観的かつ信頼性の高い知識の蓄積を試みる考え方である。上述の実証主義はこの自然主義的潮流に含まれる、19世紀に論じられた哲学である。その後様々な批判や社会変化を受け修正されることで、20世紀前半には論理実証主義という形で論じられるようになった。論理実証主義では、論理的に推測されたものが客観的な経験的事実に基づいて証明されたとき、それを科学的に正しいと捉える。そのうえでこのパラダイムでは、再現可能性が重視された。しかし現実には、科学的に正しいとされた検証結果が後に覆ることもある。その際、元々正しいとされた結論をどのように受け止めるべきなのかという問題が残る。また、論理実証主義的立場からは、観察・検証できないものは科学でないと切り捨てざるを得ない。 これらの問題を克服すべく議論されたのが、ポパーによる反証主義である（ポパー, 1980; 吉田, 2021）。反証主義においては、科学的であることの条件として「反証可能性」を重視する。これは、ある主張が科学的であるためにはその主張が間違っていることを証明される可能性を有している必要があるとする考え方である。反証可能性の詳しい説明と具体例については後述するが、このように自然科学的科学観に基づき社会現象を研究しようとする考え方を総じて自然主義と呼び、現代の社会科学研究の主流となっている（吉田,2021）。なお、宇野ほか（2022）によると、2016年から2021年に出版されたマーケティング領域のトップジャーナル8誌2に掲載された論文のうち、96%以上が実証主義的アプローチに基づくものであったとされている。 第二の解釈主義は、社会現象の研究には独自の手法が必要であるとする立場である。特に社会現象においては因果的説明だけでなく、社会現象を構成する人々への内的な理解も必要であると考えられている。また、解釈主義的立場は反実証主義的立場を取り、社会科学研究においては研究者や研究対象者の意図を排除することはできないので、自然科学のように客観的かつ中立的な研究を実施することはできないと主張する。社会科学は自然科学とは異なり、社会現象を研究対象としている。この社会現象の特殊性が、解釈主義者が自然主義を批判する根拠となる。特に社会現象においては、「意図せざる結果」の影響が自然科学に比べて強いということが言われている。この意図せざる結果は主に「自己成就的予言」と「自己破壊的予言」に分けて説明することができる。自己成就的予言とは、たとえ事実に基づかない予言であっても、何らかの発言（予言）を行うことでそれが実現してしまうことである。例えば社会的不安を煽るデマ情報が流布され、その情報を目にした民衆がその不安を信じ込んでしまうことで、本来は根拠ないデマであった予言が実現してしまうことである。一方で自己破壊的予言は、予言されたことによってそれが実現されなくなることである。例えば、根拠があるものの楽観的な予想が提示されたことによって、当事者が油断しまいその予言が実現しなくなることがある。 解釈主義者が主張するように社会現象と自然現象には違いがある。違いがあること自体は事実であるため、社会科学がただ暗黙的に自然科学と全く同じ手法を踏襲するということが好ましくない場合もあるかもしれない。しかしながら、だからといって自然科学で蓄積されてきた客観的かつ信頼性の高い手法を放棄すべきだということはあまりに極端な主張であるとも考えられる。これらの主義については唯一絶対の正解が存在するわけではなく、継続的な議論や批判による発展が必要になる。しかしながら吉田（2021）は、現時点において社会科学分野で科学的研究を行うためには、研究プロセスは最低限、推測と反駁の方法として「反証可能性」を有している必要があると主張する。この反証可能性は、先述の通り反証主義的考え方であり、漸進的な発展を想定した科学観を反映している。反証可能性はテスト可能性などとも呼ばれ、科学的理論それ自体が正しいのか誤っているのかを確認することができる可能性を表す。反証可能性はポパーによる反証主義において科学的基本として捉えられており、科学的な理論や主張はそれを反証する余地を有する必要があると議論されている。つまり、科学とは何らかの真理に至るための方法論であり、何らかの絶対的な真理を前提とすることは科学的ではないと考える。そしてこの反証可能性が科学的か非科学的かを分ける決定的な違いであると捉えられている。 反証可能性を有していない議論の例としては、フロイトによる精神分析が有名である。精神分析は常に正しい理論であるため、科学的ではないと言われている。この点について伊勢田（2003）は、以下のように説明している。精神分析の枠組みでは、人間の心は自我（意識的欲求）、超自我（社会的・道徳的行動統制）と、イド（無意識の欲求）によって構成されている。この理論において、研究者が潜在的な無意識の欲求が本当に存在するのかという問いに関心をもったと想定する。研究者による調査・分析によって、その潜在的な欲求を示唆する行動が観察されれば潜在的欲求仮説は支持される。しかし、そのような行動が観察できない場合、どのように結論付けられるのか？フロイトの理論では、「無意識の欲求は存在するが、超自我によって統制されて顕在化しない」と説明される。すなわち、無意識の欲求の存在についてこの理論は反証可能性を有していないことになる。このように反証されるリスクを背負っていない主張は、反証可能性を軸とした科学哲学に基づくと科学的ではないということになる。 例えば、「AはBを高める」という予想があったとしよう。この予想が科学的である場合、特定の手続きを経て調査・分析を行い、ある結果が出ればこの予想を受けいれ、それ以外の結果であればこの予想が誤っていると結論づけることができる。このように、ある理論が科学的であるためには、絶えず反証による理論の修正や、新たな理論の提案を行うことが可能であることが重要となる（ポパー, 1980; 吉田, 2021）。その上で本書では自然主義的な立場を取りつつも、社会現象が持つ独自性をもつことも認める。そのうえで吉田（2021）の議論の通り反証可能性をもつことが科学的知識の条件として考える。そのため、調査・分析の結果も唯一絶対の真理ではなく常に反証される可能性を有している、という理解のもと本書の内容が構成されていることを理解して欲しい。本節では、我々が調査によって明らかにする知見・理論が持つ特性や目的について、主に反証可能性という概念の重要性を論じてきた。次節では、マーケティング研究を進める上での最初のステップとなる問いの設定について説明を行う。 Journal of Consumer Psychology，Journal of Consumer Research，Journal of Marketing，Journal of Marketing Research，Journal of the Academy of Marketing Science，Marketing Science，International Journal of Research in Marketing，Journal of Retailing↩︎ "],["研究課題問いの設定.html", "3.4 研究課題（問い）の設定", " 3.4 研究課題（問い）の設定 研究は、「問いを立てる」ことから始まる。「問いを立てる」というと、自身の疑問を提示することであり「そんなのは簡単だ」と感じる人もいるかもしれない。しかし、本書は研究課題（研究上の問い）として、実証的に検証可能な問いに着目し、「問いを立てるのはなかなか難しい」という立場を取る。研究者は問いをただ闇雲に思いつくままに述べればいいわけではない。なぜならば、研究課題はその後のリサーチデザイン設計にも深く関わることになるためである。言い換えると、リサーチデザインは、自身の立てた研究課題にきちんと回答できるように設計すべきである。自身の立てた研究課題とその後のリサーチデザインや議論との一貫性を保つことは存外難しく、多くの人にとっては何度か失敗を繰り返しながら学ぶものになる。 また、マーケティング研究における問いを適切に立てるためには、「実務的課題」と「研究課題」という2つの異なる課題のタイプが存在することを理解すべきである。この2つの課題弁別は、「マーケティングとマーケティングリサーチ」という節で説明した2つの視点に準拠するものである。実務的課題とは、マーケティングに関する意思決定についての課題であり、主に意思決定者が何をすべきなのかを捉えている。例えば、ある企業における製品の市場シェアが減少していたとする。ここで、「どうればよいのか？」という問いは典型的な実務的課題だと考える。また、たとえ具体的な方策に着目した問いを立てたとしても、例えば、「モバイル広告を実施すべきか？」や「どうすればオムニチャネル化を推進できるか？」といった問いも実務的課題だといえる。マーケティングに興味を持つ学生の場合、この実務的課題や、実務的な問題を解決するための手段やアイデアを扱うことに慣れているかもしれない。しかし、これら問いに直接的に答えることが必ずしも研究にはならないということを理解する必要がある。 一方で研究課題（ここでは特に実証的な研究課題）とは、実証的に検証可能なものであり、現実社会で何が起きているのかについて、定量/定性的調査を通じて得た情報を用いて結論を提示できる問いを指す。例えば、「ファストリテイリングは何年から有明倉庫を稼働させたか？」という問いは、事実を調べることで回答できる。また、「新しいパッケージデザインは以前のものよりも消費者の購買意図を高めるか？」という問いも研究課題の例であり、消費者を対象とした実験調査によって回答可能である。多くの場合前者のような問いは単純すぎて研究課題として利用されない。その問いに答えることでどのような含意が得られるかによって問いの価値が評価される。 ここで強調したいのは、研究の実行においては研究課題の提示が絶対に必要なのだが、我々が経営学という応用学問領域に属している以上、実務的課題も重要だという点である。マーケティングにおいては多くの場合、社会や実務で起こっている問題に対し何らかの示唆を与える研究を行うことが求められる。そのため、研究のための研究ではなく、社会や実務への含意を見いだせるような研究が重視される傾向にある。このことから、我々にとっては実務的課題も重要になるものの、先述の通り実務的課題のままでは研究課題として機能しない。そこで、実務的課題を実証的研究課題に変換することが求められる。問いの変換方法として、ここでは浅野・矢内（2018）で提示されている2つの方法について取り上げる3。 第一の方法が、「参照枠組みを変える」という方法である。具体的には、実務的課題から議論の対象となる主体を特定し、彼/彼女らの評価について情報を収集する形に問いを変換するような方法だと言える。例えば、「企業は環境負荷に配慮されたチョコ製品を販売すべきか？」という実務的課題を考える。このとき例えば、企業が販売したチョコ製品を購入する主体である既存顧客を、問いの中心となる主体として設定することで、「既存顧客のチョコ製品選択に対し、企業による環境対応の有無は影響を与えるか？」という問いに変換することができる。この変換後の問いであれば、既存顧客へのアンケート等でデータを集め実証可能であるため、研究課題として機能すると考えられる。同様の問いを特定の企業活動に限定しない形で提示するならば、例えば「企業の環境、社会・ガバナンス（ESG）活動はチョコレート菓子市場における消費者の購買意図を向上させるのか？」という問いも設定可能である。 第二の方法は、「背後に想定されている暗黙の前提を問う」というものである。これは、実務的問いの背後に暗黙的に仮定されている理屈やメカニズムに自覚的になり、それ自体を問うものである。例えば、「どうすればオムニチャネル化を推進できるか（or すべきか）？」という実務的課題があったと考える。この問いの背後には、「オムニチャネル化が企業成果に好ましい影響を与えるはずだ」という暗黙の前提が置かれているかもしれない。オムニチャネルに限らず、多くのビジネス書などで話題になる戦略については、このような成果に対する暗黙の前提が置かれ、その用語だけが独り歩きして流行ることも散見される。では「そもそもオムニチャネル化は本当に企業成果に影響があるのかだろうか」、もしあるのだとすれば「どのような成果に対して影響があるのか」という問いは、暗黙の前提を問うものであり、非常に素朴だが重要な研究課題である。これに関連する研究として、「オムニチャネル化（チャネル間統合）は小売企業の売上に影響するのか？」（Cao and Li, 2015）や「オムニチャネル化（チャネル間統合）は小売企業の費用効率性に影響するのか？」（Tagashira and Minami, 2019）といった研究課題が実際に扱われ、論文化されている。 暗黙の前提を捉えた別の具体的な研究例として、Lim et al. （2020） による、顧客満足度が成果へ与える影響について捉えた論文が挙げられる。様々な教科書において、顧客満足度が高まることによって顧客による再購買が増え、企業のマーケティング費用が効率化される、というロジックによって顧客満足の重要性が説明されてきた。しかしながらそれは本当だろうか、というのが Lim et al. （2020） の研究課題である。このように暗黙の前提を問うような研究課題の設定は、非常に素朴な問いになるがそれだけに、もしそれが既存研究で未解決である場合には大きな理論的貢献につながる可能性を持つ。 本節では、研究課題の設定について説明した。本講義で扱う研究課題は、検証可能であり実務的課題とは異なるものであるという点を理解してほしい。また、本設は実務的課題から研究課題への変換についても説明を行った。次節では自ら立てた問いと整合的な議論を提示するための理論や仮説構築について説明する。 浅野・矢内（2018）では、規範的議論と実証的議論との対比で以下の内容を提示している。規範的議論に関心がある場合は、浅野・矢内（2018）を参照してほしい。↩︎ "],["研究における理論の利用と精緻化.html", "3.5 研究における理論の利用と精緻化", " 3.5 研究における理論の利用と精緻化 実証的なマーケティング研究では、結果や議論との間の論理的整合性に寄与する「理論」が重要になる。本講義では理論を、何らかの客観的真理を探求するための主張だとする、実在論的な立場に基づき捉える。しかしながらここで強調したいのは、科学的な理論が「絶対的かつ不変の真理を意味しない」ということである。言い換えると、理論はあくまで暫定的な仮説に過ぎず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくことが求められる。なお、実在論と相反する立場として道具主義という考え方も存在する（吉田, 2021）。道具主義では、科学理論を予想や説明のための便利な道具として捉えるため、用いている理論の経験的正しさは問題視しない。これらの異なる立場に対する優劣を決めるのは困難である。しかしいずれの立場においても、可謬性という、「知識についての主張は原理的に誤りうる」という性質を受け入れ、自己批判と相互批判に基づきより良い理論を目指すことが重要ではないかと言われている（吉田, 2021）。 本書では、理論を事象の原因と結果に関する一般的（抽象度の高い）理屈であると考える（浅野・矢内, 2018）。一方で先程は、理論を現在広く受け入れられている仮説だと述べた。そういった意味で、「理論」と「仮説」の間に本質的な違いはないといえる。しかしながら、マーケティング領域では、様々な具体的変数に応用できる「抽象度の高い構成概念同士の関係」を理論と呼び、「より具体的な変数間の関係」を仮説と呼ぶことが多い。ここでいう構成概念とは心理学的研究アプローチにおいて用いられる、直接観測することはできないがその存在を仮定することで測定や観測を可能にするために構成された抽象的概念のことである。例えばマーケティング分野では、「顧客満足」や「顧客エンゲージメント」といった構成概念が用いられる。 マーケティング研究においては様々な分野から理論を応用することになる。研究における理論の扱いに関して Fisher and Aguinis （2017） は、theory generation（理論生成）, theory testing（理論検証）, theory elaboration（理論精緻化）という3種類のアプローチを紹介している。Theory generationは、ゼロから新たに検証可能な理論や概念を提示することを目的とする。これは、まだ既存の理論がなく説明されていない現象について、新たな構成概念の導出や概念間の関係について論理的かつ十分に納得の行く議論を展開することで新たな知見を提示するアプローチである。一方で theory testing は、既存の理論を検証することを目的とするアプローチである。既存の理論から具体的な仮説を導出し、データを収集し分析することで、提示した仮説的関係が支持されるかを検証するアプローチである。 Fisher and Aguinis （2017） では、theory generation（理論の生成） と theory testing（理論の検証） よりも特に、既存の理論を洗練させ、拡張することに重点を置いたtheory elaboration（理論の精緻化）の重要性が強調された。Theory elaboration は、theory generation や theory testing よりも、incremental な理論のアップデートについて捉えた考え方であり、既存の理論からの積み重ねによって理論を改善していくプロセスとして期待される。Theory elaboration の主な実施アプローチとして Fisher and Aguinis （2017） は、「対比（Contrasting）」「構成要素の特定（Construct specification）」「構造化（Structuring）」の3つを提示した。 「対比」アプローチでは、異なる理論や構成要素を比較し、類似点や相違点を明らかにする。このアプローチに関連する具体的なプロセスとして、水平的対比と垂直的対比がある。水平的対比は、既存の理論を異なる文脈で検討することで、その理論がどのように適合するかを調べるプロセスである。これにより、異なる文脈での理論の有用性や限界が明らかになる。垂直的対比は、同じ文脈内で異なるレベルの抽象度を持つ理論や構成要素を比較することで、新しい洞察を得るプロセスである。これにより、より具体的または抽象的なレベルで理論を分析し、新しい仮説や洞察を導き出すことが期待される。 「構成要素の特定」アプローチでは、構成要素定義の洗練化や明確化によって構成要素の妥当性を向上させることを目的とする。このアプローチに関連する具体的なプロセスとして、新しい構成要素の指定や構成要素の分割が考えられる。新しい構成要素の指定は、既存の理論で考慮されていなかった新しい概念を特定し、それを定義することである。これにより、既存の理論がカバーしていなかった現実世界の側面を捉えることができる。一方で構成要素の分割は、既存の理論内の1つ以上の概念をより明確に区別し分けて捉えることである。これにより、類似した概念間の区別が明確化され、それらが異なる現象を説明するために使用できることを示すことで、理論の妥当性や範囲を向上させる。 「構造化」アプローチでは、構成要素間の関係をより体系的に整理することを目指す。このアプローチに関連する具体的なプロセスとして、「特定の関係の構造化」、連続的関係の構造化、循環的関係の構造化などがある。「特定の関係の構造化」では、2つの概念間の特定の関係を明確に定義する。これにより、異なる概念間の関係性がより明確になり、理論が現実世界にどのように適合するかが明らかになる。「循環的関係の構造化」は、概念やイベントに関する一連や順序的な共起関係を説明することである。これにより、現象やイベントがどのように発生し、その結果として次にどのように進行していくかを説明することができる。 最後に、「再帰的関係の構造化」は、2つ（以上）の概念間での相互作用（双方向の因果）を説明する。これにより、理論が現実世界でどのように適合し、相互作用がどのように進行するかを説明することができる。 本節では、理論を何らかの真理を探求するためのものだと捉えた。しかしながら、理論は唯一絶対の真理を意味せず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくものである。その意味において理論と仮説は本質的には同質的であるものの、マーケティング領域では、抽象的で様々な分野へ応用可能な概念間の関係を理論、より具体的な変数間の関係を仮説と言う事が多い。また本節では、先行研究から理論をただ援用するだけではなく、既存研究へ理論的貢献を与えるための theory elaboration について説明した。Theory generation や theory testing だけでなく、よりincremental な理論のアップデートについてその具体的なアプローチも重ねて理解することで、より発展的な論文執筆に役立つことを期待している。 "],["問題と分析をつなげる仮説の提示.html", "3.6 問題と分析をつなげる仮説の提示", " 3.6 問題と分析をつなげる仮説の提示 マーケティングに関する実証分析は、依拠した理論に基づく仮説を提示し、データを用いた分析によって仮説を検証するという形式が取る事が多い。言い換えると、事象を観察し、論理的な説明としての仮説を提示し、それを客観的・科学的と考えられる手順で検証するというプロセスによって、証明を行う。そのため、マーケティング研究における仮説は、検証可能かつ研究課題や研究が依拠する理論と整合的である必要がある。 先述の通り、説明の容易さのためにここでは理論と仮説という言葉を区別して用いる。本書における理論とは抽象度の高い概念同士の関係を表す。一方で、仮説は作業仮説とも呼ばれ、理論を検証するために引き出された特定の変数間の関係に関する記述を指す（浅野・矢内, 2018）。また、仮説はデータに基づく検証のベースとなる記述であるため、その内容は入手可能な変数間の関係として記述することが大切になる。 ここで、「お金がある人ほど衝動買いをする」という理論があったと仮定して、作業仮説化について考えてみる。作業仮説化においては、この理論と整合的かつ測定（検証）可能な変数を捉えた記述であることが重要だと述べた。それを踏まえ、以下の２つの作業仮説例を考える。 「年収」の高い人ほど「衝動買い性向の程度」が高い 「買い物時の予算」が多い人ほどその買い物における「非計画購買購入額」が高い （1）の例は、年収という個人属性と衝動買い性向という心理尺度を捉えており、個人の特性を表す二変数間の関係を示した仮説である。一方で、（2）はある購買客の入店時予算とその買い物時に発生した非計画購買額を捉えている。この二つの例は、作業仮説化において重要な要素である「分析単位の一貫性」という性質を満たしている。基本的に作業仮説化で捉える変数は同一の分析単位である必要がある。どちらの例も、特定の消費者に関する（1）属性と心理尺度と、（2）買い物時の予算と購買額、という形で測定単位が一致している。これがもし、消費者個人の特徴と店舗での売上との関係を記述した仮説である場合、集計レベルが異なるため、データによる分析と仮説検証が困難になる。したがって、特別な場合を除き分析単位の一貫性を守ることは重要となる。 先述の二つの仮説からは、仮説導出における注意点も見いだせる。具体的には、理論を正確に参照することの重要性である。「お金がある人ほど衝動買いをする」という理論では、「お金」と「衝動買い」という二つの概念間の正の関係が示唆されている。それに対して一つめの仮説では、「年収」という個人属性を示す変数で「お金」という概念を捉えており、個人の心理的傾向としての「衝動買いのしやすさ」を「衝動買い」の変数として扱っている。他方で二つめの仮説では、「特定の購買時点での予算」と、その買い物での「非計画購買の額」を捉えている。「お金がある人ほど衝動買いをする」という理論から導出された仮説としては、どちらもある程度の一貫性がありそうだが、両者は全く違う変数を捉えている。この場合、どちらか一方もしくはどちらも不正確に理論を参照している可能性がある。もしかしたらもとの理論をきちんと参照すれば、消費者の所得と心理的性向を捉えたものであることがわかり、一つめの仮説化が適切であることが判明するかもしれない。しかしながら、「お金」や「衝動買い」という曖昧な言葉のまま参照している状態では、その判断もつかない。また、無意識的にしろ意識的にしろ、研究者にとって都合の良い（測定しやすい）文脈に理論を読み違えて仮説化してしまう場合も散見される。このような問題を避けるためにも、その理論が具体的にどのような視点に基づき議論を展開しているのかについてできるだけ正確に内容を理解することが必要になる。 多くのマーケティング研究では、抽象度の高い理論に基づき仮説を提示し、データを用いて仮説を検証する。仮説の提示においては、着目する研究課題、理論、分析単位の一貫性を保つことが重要になる。本節の後半では特に、理論を正確に参照することの重要性について強調した。 "],["参考文献-1.html", "3.7 参考文献", " 3.7 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. 伊勢田哲治 (2003) 「疑似科学と科学の哲学」, 名古屋大学出版会. 宇野舞・グェン フォン バオ チャウ・趙雅欣・六嶋俊太・福川恭子（2022）『マーケティング研究領域におけるパラダイムの現状と課題 : より活発な理論発展へ向けて』,「一橋商学論叢」, 17(2), 14-30. 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. ポパーカール (1980) 「推測と反駁-科学的知識の発展」, 藤本隆志・石垣壽郎・森博訳, 法政大学出版局. ベルクラッセル・フィッシャーアイリーン・コジネッツロバート（2016）「消費者理解のための定性的マーケティング・リサーチ」，松井剛訳，碩学社. 吉田敬 (2021) 「社会科学の哲学入門」, 勁草書房. Cao, L., &amp; Li, L. (2015). The Impact of Cross-Channel Integration on Retailers’ Sales Growth. Journal of Retailing, 91(2), 198-216. Fisher, G., &amp; Aguinis, H. (2017). Using Theory Elaboration to Make Theoretical Advancements. Organizational Research Methods, 20(3), 438-464. Hui, S. K., Inman, J. J., Huang, Y., &amp; Suher, J. (2013). The Effect of In-Store Travel Distance on Unplanned Spending: Applications to Mobile Promotion Strategies. Journal of Marketing, 77(2), 1-16. Lim, L. G., Tuli, K. R., &amp; Grewal, R. (2020). Customer Satisfaction and Its Impact on the Future Costs of Selling. Journal of Marketing, 84(4), 23-44. Schwarz, N. (2004). Metacognitive Experiences in Consumer Judgment and Decision Making. Journal of Consumer Psychology, 14(4), 332-348. Schwarz, N., Jalbert, M., Noah, T., &amp; Zhang, L. (2021). Metacognitive experiences as information: Processing fluency in consumer judgment and decision making. Consumer Psychology Review, 4(1), 4-25. Tagashira, T., &amp; Minami, C. (2019). The Effect of Cross-Channel Integration on Cost Efficiency. Journal of Interactive Marketing, 47, 68-83. "],["データの種類とアンケートデザイン.html", "Chapter 4 データの種類とアンケートデザイン", " Chapter 4 データの種類とアンケートデザイン 研究においては、変数間の関係について分析、記述することが多い。仮説を提示する際の注意点においても、分析に用いる変数と整合的であることが重要であると述べた。そこで重要なのは、自身が用いる変数がどのような特徴を持つ尺度であり、どのように事象を測定しているかを理解することである。以下の表は、主な尺度のタイプとその特徴をまとめたものである。 Table 4.1: 尺度表 尺度 特徴 例 名義 対象の識別と分類 性別・職種 順序 対象の相対的ポジション 好み順位・ランキング 間隔 対象間の大小関係比較（原点は定まっていない） 態度・指数・気温 比率 連続的関係（原点が定まっており、比率計算も可能） 所得・売上 上表における間隔尺度や比率尺度は一般的に量的尺度と分類される。売上高はマーケティング研究における最も典型的な「比率尺度」の例である。この尺度は大小にも間隔にも意味があり、かつ比率にも意味があるため、四則演算に対応する尺度である。一方で「間隔尺度」は正の整数で表される尺度であり、その値の大小関係と間隔にも意味があるものの、原点が定まっておらず、比率計算に耐えない尺度である。マーケティング研究においては、アンケート調査における質問項目や、質問項目の合計値（およびそれを項目数で割ったもの）である合成変数が間隔尺度の典型的な例である。 一方でマーケティング研究においては、必ずしも量的ではない情報に着目して分析を行うことも多い。そのような場合には、質的尺度を用いて観察対象のカテゴリを分類することで分析可能にする。例えば、消費者の購買行動に関する東京、大阪、北海道という地域（都道府）間の差を分析する場合を考える。このとき、「地域の違い」に数量的な違いは存在しないものの、地域の違いを表すために東京 = 1, 大阪 = 2, 北海道 = 3 のような地域コードを用いる事が多い。しかしながら、この変数がとる数値そのものに本来的な意味はなく、東京が大阪と北海道よりも低い値を取っているという解釈は適切でない。この変数はあくまで異なる地域に分類されることを示しているのみである。このような属性の分類や有無を表すための尺度を「名義尺度」と呼ぶ。また、観測対象が特定の属性に対応する場合（例、男性）には 1 を、そうでない場合には 0 を取るような、1 と 0 で分類された名義尺度のことを特に「ダミー変数」と呼ぶ。ダミー変数は分析結果の解釈が容易になる利点もあるが、詳しくは後述する。質的尺度のもうひとつの例が「順序尺度」である。順序尺度は、観察対象の序列や大小関係を表す尺度であるが、その数値の間隔に意味はない。例えば 1 が最低であり、4 が最高となるような金融商品の等級において、商品 A は ランク 4、商品 B はランク 1 だとする。このとき、 B は A よりも高い評価を受けているということは言えるが、A は B の 4倍優れているという議論は不適切である。このように、各対象間の推移性を表現するときに用いるのが、順序尺度である。 次に、データの収集や取得という観点に基づくデータ分類基準を提示する。研究の遂行においては、研究者の関心や研究課題のために収集されたデータを用いることもあれば、別の目的で収集されたデータを用いることもある。前者のようなデータを「一次データ」、後者を「二次データ」と呼ぶ。一次データは研究上の問いに回答するために実施された調査、実験や観察に基づき収集形成されたデータある。一方で二次データは、業務上蓄積されたデータ、民間リサーチ会社の統計データや、政府統計などに代表される他の目的で収集された、ないし継続して収集されているデータを指す。現在は、様々な二次データがアクセス可能であり、二次データを利用することで研究上の問いに回答できる可能性も十分にある。例えば、企業の視点にたてば、組織内部の二次データ（業務活動で得たデータ: Point of sales (POS) データ、webサイトへのアクセス記録など）と外部の二次データ（民間リサーチ会社の統計データや政府統計など）が存在する。その他にも、オープンソース化されているデータも様々存在する。そのため、本書では一時データを収集する前に、関連する二次データとしてどのようなものが存在しアクセス可能なのか検討することを勧める。しかしながら、二次データではすでに集計や加工をされたデータしか入手できず、raw データ（収集されたまま加工されていないデータ）にアクセスできない場合もある。そのため、入手可能な二次データが本当に自身の研究課題や仮説で議論されている内容および集計レベルと整合的なのか、という点については慎重に検討する必要がある。 データのタイプは、集計レベル（分析単位）に基づいて分類することもできる。この分類では、個人の行動や回答を捉えた「非集計データ」と、非集計データをある単位でまとめ、計算や整理を施した「集計データ」とを捉えることができる。非集計データの例としては、ID-POSデータ（ロイヤルティカード（アプリ）などによる顧客の個人IDと、購買製品、価格、数量などの情報が含まれたPOSデータを結合したもの）や、消費者個人を対象としたアンケートデータなどが挙げられる。一方で企業成果・業績などの財務データは、企業レベルで集計されたデータだと言える。自身の研究課題や仮説がどのような集計レベルのデータに対応するものなのかを考え、研究内容と一貫したデータを用いることが必要になる。 "],["記述的リサーチデザインとアンケート.html", "4.1 記述的リサーチデザインとアンケート", " 4.1 記述的リサーチデザインとアンケート コンピュータやソフトウェアが発展した現代においては、データを収集し、ある変数についての情報を要約、分析すること自体はさほど難しいことではない。しかしながら、実務的ないし学術的に意義のある研究を行うのは容易ではない。そのためには、研究課題について慎重に吟味しつつ、適切なリサーチデザインを決定する必要がある。経営学分野におけるリサーチデザインは、田村(2006) にて検討されている。ここでは、Malhotra (2019) を参照に記述的リサーチデザインを中心に説明する。 Malhotra (2019) は、マーケティング・リサーチのタイプとして、大きく分けて (1) 探索的リサーチ、(2) 決定的 (conclusive) リサーチがあるとし、決定的リサーチの中にさらに (3) 記述的リサーチと (4) 因果リサーチがあるとした。探索的リサーチでは、その研究プロセスが非構造（または半構造）的かつ柔軟であり、一般的に定性的方法が用いられることが多い。研究者は、研究や実務に関するアイディアやインサイトの発見、仮説の構築、未開拓領域における研究課題の定義、等を目的にこのタイプの研究を採用することが多い。一方で決定的リサーチを採用する研究者は、特定の仮説を検証したり、変数同士の関係についての分析を目的とすることが多い。このアプローチでは、リサーチプロセスはより構造化され、定量的な分析手法が用いられることが多い。 リサーチデザイン分類 決定的リサーチの中でも、記述的リサーチは、特定の変数や複数の変数間の関係を明らかにすることで、実証的な問いに回答しようと試みる。記述的リサーチを用いた研究目的の例としては、(1) 関連するグループ（顧客や商圏）の特徴を説明する、 (2) 顧客全体のうち特定の行動を取る顧客の比率はどの程度なのか明らかにする (e.g. ヘビーユーザーの比率)、(3) 企業の操作するマーケティング変数と顧客の購買意図との関係を明らかにする、などが挙げられる。これらはあくまで例であるため、あえて抽象的な表現を用いているが、実際に記述的リサーチ課題を提示する際には、検証可能なレベルにまで焦点を絞ることが必要になる。そして、本書ではおもにこの記述的リサーチに焦点を合わせて、手法を紹介する。 一方で因果リサーチでは、相関関係と因果関係を区別し、先行要因が結果変数へ与える効果について因果関係を想定した形で分析することを指す。相関関係とは、2つの変数XとYの間に、比例や反比例といった共変関係があることを指す。一方で、要因Xを変化させることで要因Yが変化する時、Xを原因でYを結果とする因果関係がある（X\\(\\rightarrow\\)Yと示す）という。つまり相関関係とは異なり、因果関係ではどちらが先行要因であるかという前後関係がはっきりしている。このような因果関係を明らかにする分析手法は一般的に因果推論と言われ、記述的リサーチよりも高度な調査設計・統計的手法が必要となる。本講義は、マーケティングにおける因果推論の応用は扱わないため、別資料を参照してほしい。 マーケティングにおける記述的リサーチアプローチとして広く用いられれているのが、質問紙（アンケート）調査である。マーケティングにおける調査や研究では行動データ等では捉えきれない消費者の態度や知覚について知ることを目的とする場合も多い。このような目的を達成するため、これまでの研究では伝統的に質問紙調査によって情報を得てきた。質問紙調査とは、データ収集のための構造化された手法で、回答者が回答する一連の質問（書面または口頭）から構成される。質問紙調査では、調査者が求める情報を、回答者が回答できる一連の具体的な質問に変換することを目的とする。調査の実施において研究者は、回答者が相互コミュニケーションに参加し、協力し、面接を完了するように意欲を高め、動機を与え、奨励すると同時に、回答誤差を最小化する必要がある。 質問紙調査は、研究者と回答者との間のコミュニケーションとして捉えられる。また、研究者はこの双方向的なコミュニケーションにおいて、下の図のようなノイズが生じることを理解し、それを削減する努力を講じることが求められる。 アンケートノイズ アンケートによるデータ収集は、社会科学における「人を対象にした研究」に該当するため、人を対象にした研究としての研究倫理規則を守る必要がある。例えば、一橋大学における研究倫理規則は以下のリンクから確認できる（https://www.hit-u.ac.jp/academic/research_ethics/index.html）。ここでは、主に調査協力者の身体的・精神的負担を最小化し、彼らに不利益が生じないよう留意する必要がある。また、データを収集する際には、研究対象者から同意（インフォームド・コンセント）を得る必要がある。インフォームド・コンセントにおいては、、調査を行う主体、目的、調査により得た情報の管理方法、公開有無や方法について明示し、この条件について回答者から許諾を得る必要がある。ただし、大規模な無記名アンケート調査など、研究者が、回答者から個人名を記載した形の同意書を得ることが困難な場合がある。その場合は慣習として、調査開始前に上記の点について説明し、調査回答へ進むことで同意を得たことと代替する事が多い。ただし、このような対応方法に関する適切性の議論も、時代の経過とともに変化することが考えられるため、研究倫理については研究者自身が継続的に学習し、適切な手法を採用していくことが求められる。以下は、実際に田頭が使用したアンケートにおけるカバーレターであり、インフォームド・コンセント文の例である。 インフォームド・コンセント例 アンケートのデザインは主に以下のプロセスを経る。 質問トピックを決める 質問と回答方式を決める ワーディングを決める 質問の順番を決める アンケートのレイアウトや装丁を決める プレ調査の実施 アンケート（本調査）の実施 上記のステップに対し、ここではまず質問と回答方式について説明する。なお質問トピックは、研究課題に深く関わるため、本資料にてすでに議論されている内容を参照してほしい。アンケートでは、主に自由回答式質問、選択肢型質問、尺度型質問、といった方式の質問が用いられる。自由回答式質問は、回答者が自身の言葉で自由に回答できる質問であり、回答に関する事前の選択肢は設定されていないタイプの質問である。回答者は、このような質問に対して一言で簡潔に回答するか、詳細に長く回答するかなど、回答方法に裁量を持つことになる。例えば、「出身はどちらですか？」という問いに対して回答者は、「東京都」と都道府県レベルで答えることも、「東京都町田市」と市区町村レベルで答えることも可能である。また、典型的な自由回答式質問の例として、理由や動機を問う設問がある。例えば「なぜ、この航空会社を選びましたか？」といった質問に対して回答者は「機内食」のように一言で回答することも、長い文章を用いて回答することも可能である。選択肢型質問は、回答者に質問に対する回答のための2つ以上の選択肢を与え、その選択肢から択一させるものである。例えば、回答者の特定のサービスの利用経験について「1. はい、2. いいえ」からいずれかを選ばせたり、学歴について複数の選択肢から一つを選ばせるというものがこのタイプの質問に該当する。 一方で尺度型質問は、消費者による企業への態度や満足などを捉えるためにマーケティング分野で広く用いられている質問方式である。ここでは、ある質問に対する点数を回答者に選ばせることで、その質問で捉えようとしている性向の程度を測るものである。尺度の形成方法としては、グラフィックを用いるもの、項目別に評価するものなどが存在するがここでは、マーケティング領域で広く用いられている項目別評価尺度について説明する。項目別評価尺度においてよく用いられる尺度はリッカート尺度とSemantic differential (SD) 法と呼ばれる尺度である。リッカート尺度では、調査者は回答者に文章を提示し、回答者がその文章の内容にどの程度同意するかについて情報を得る。設問では、「全くそう思わない,…,とてもそう思う」や「全くあてはまらない,…,とても良くあてはまる」という選択肢について5点尺度や7点尺度で回答させることが多い。以下は、7点のリッカート尺度による質問の例である。 リッカート尺度例 一方でSD法は、両極の意味を持つ一連の尺度を用いて、回答者の評価を得る方法である。例えば、「冷たい-温かい」や、「弱い-強い」というような対極にある言葉を両極に設定して、5点や7点尺度によって回答を得る方法が一般的である。以下は、7点のSD法による質問の例である。 SD尺度例 マーケティングにおける研究では、アンケートを通じて、顧客満足や企業態度などの抽象度の高い構成概念を捉えようと試みることも多い（南・小野, 2010）。その場合、研究者は複数の質問項目を用いてひとつの概念を捉えるが、このような項目の扱いには、専門的な知識や先行研究による知見が必要になる。例えば、2010年代以降よく用いられる概念に、消費者によるブランドエンゲージメントがある。ブランドエンゲージメントは「消費者が特定のブランドをどの程度自分自身の一部のように大事に捉えているか」を表す概念である (Sprott et al., 2009, p. 92)。このような抽象的な概念は多面的な視点から包括的に捉えるが多く、例えば Sprott et al. (2009) では、以下の8つの質問項目を用いて、ブランドエンゲージメントを捉えていた。 I have a special bond with this brand. 私はこのブランドと特別なつながりを持っている。 I consider this brand to be a part of myself. 私はこのブランドを自分の一部として考える。 I often feel a personal connection between this brand and myself. 私はこのブランドと自分自身の間に個人的な関係を感じる。 Part of me is defined by this brand in my life. 私の人生の一部はこのブランドによって規定されている。 I feel as if I have a close personal connection with this brand I most prefer. 私が最も好きなこのブランドとは、まるで個人的な関係を持っているかのように感じる。 I can identify with this brand in my life. 私の人生において、このブランドに共感できる。 There are links between this brand and how I view myself. このブランドと私自身との間には繋がりがある。 This brand is an important indication of who I am. このブランドは、私がどういう人間かを示す重要なものである。 このことから、研究者は、包括的かつ多面的な概念を、より具体的な複数の質問を使って様々な側面から捉えようとしていることが伺える。このような構成概念を用いた分析では、概念を構成する項目の合計値や算術平均を用いて、ひとつ合成変数を作成することでその後の分析に用いる場合や、共分散構造分析（Covariance-based Structural Equation Modeling: CB-SEM）という高度な手法を用いることがある。しかし、本書では共分散構造分析は扱わないため、別のテキスト等を参照してほしい。また、複数項目によって構成概念を捉える場合、それらの項目が十分に似ているか（共通の概念に寄与しているか）をチェックする必要がある。そのようなチェックにおいては、探索的因子分析や確認的因子分析が用いられる。探索的因子分析については、本書の後半で扱う。一方で、確認的因子分析は、通常共分散構造分析の一種として扱われることが多い手法なので本書では扱わない。 "],["質問におけるワーディング.html", "4.2 質問におけるワーディング", " 4.2 質問におけるワーディング 本節では、アンケート作成におけるより実務的な注意点を説明する。具体的には、ここではアンケート内で使用する言葉やフレーズについての注意点を説明する。これは、データ収集前に吟味し、修正する必要がある。研究者がアンケート内で用いる言葉や表現の複雑さは、調査のトピックや対象となる回答者特性に合わせて調整すべきである。研究者はよく犯すアンケートにおける言い回し（ワーディング）誤りとして、以下のものが挙げられる。 曖昧な質問 ダブルバレル質問 誘導的な表現 曖昧な質問は、質問文の解釈が一意に定まらないような文章による質問を指す。このような曖昧かつ多義的な質問をしてしまうと、回答者と調査者が質問に対して異なる意味を見出してしまうことで、不適切かつ予想外の回答を得る可能性が高まる。例えば、「あなたはいつケーキを買いますか？」という質問を考える。この質問で問われている「いつ」が曖昧であるといえる。回答者はこの質問に対して、一年間のうち特定の月やタイミングを答えるべきなのか、次にいつ（例えば、2週間後等）ケーキを買うと答えるべきなのかが曖昧である。この場合、仮に研究者の意図が後者であったとしても、適切な回答を得ることができないかもしれない。 ダブルバレル質問は一つの質問の中に2つの論点が含まれているような質問を指す。このような質問の場合、回答の含意が一意に定まらず、回答に対する適切な解釈が提示できない。例えば、ホテルにおいてリピート客に対してサービスを評価してもらうようなアンケートを考える。そこで、ホテルの調査担当者以下のような質問と回答選択肢を 「当ホテルの食事や接客サービスにおいて、以前利用した際と比べて何か改善は見られましたか？」 「(1) はい (2) いいえ (3) わからない」 この質問の問題点を明確にするために、ある回答者が「 (2) いいえ」と回答した場合を考える。この回答の含意には、以下の3つの可能性を見出すことができる。 食事とサービスどちらにも改善がない。 食事に改善があってもサービスにはない。 サービスに改善があっても食事にはない。 しかしながら、(2) いいえ、という回答が上記のどの理由によって提示されたのかは識別できない。また、このような質問は、回答者を混乱されることにもつながるため、回答にかかる精神的労力を高めるという点からも好ましくない。この場合、「食事に関する改善」と「接客サービスに関する改善」とを別々の質問として問うことが好ましい。 誘導的な質問は、回答者を特定の答えに誘導する傾向のある質問を指す。例えば、以下のような消費者による小売業態への評価や利用に関するアンケートを実施することを考える。 「ドン・キホーテのような安価なディスカウントストアをどの程度利用しますか？」 「(1) 週に1回か以上、(2) 2週間に1回程度、(3) 3週間に1回程度、(4) 1ヶ月に1回程度、(5) 利用しない」 上記の質問の場合、「安価」や「ディスカウント」という言葉を強調しており、回答者が頻繁に行くと答えにくいと感じてしまう可能性がある。また、特定の企業を想起させることで、特定の企業に対する評価を誘導してしまう可能性がある点にも注意が必要である。 誘導的な質問は質問文と回答選択肢の組み合わせによっても生じる可能性があるため注意が必要である。例えば、ある政策に対する評価を確認するため、以下のような質問と回答選択肢を考える。 あなたの〇〇政策を支持していますか？ 「(1) はい (2) いいえ (3) わからない」 このような質問および選択肢の問題点を理解するために、まず「(2) いいえ」という回答を得た場合を考える。「いいえ」という回答では、その回答者がこの政策に対して積極的に不支持なのか、それとも積極的に支持しているわけではないのかがわからない。つまり、「(1) はい」には、積極している支持している層しか観察されず、いいえに回答が集まりやすい設計になっていることがうかがえる。このような場合、例えば「賛成-反対」を両極とするSD法によって回答を得ることで、回答者が当該政策に対してどのような立場に立っているのかがわかりやすくなる。 暗黙の前提を含む質問は、研究者と回答者とで異なることを想定し、適切な回答を得ることができない可能性を高める。例えば、研究者が一般消費者の固定電話の利用頻度を知りたいと考えている状況を仮定し、そのために「あなたの電話の利用頻度について教えてください」と質問したとする。この質問における「電話」とは何を想定しているのだろうか。質問において聞かれているのは自宅等に置かれている固定電話、もしくは携帯電話の利用頻度なのかが不明確である。これは、「曖昧な質問」にも通じる問題も含まれるが、研究者が電話という言葉に対して暗黙的に固定電話を仮定していることが原因で生じた問題だと理解できる。このような問題を避けるために、例えば質問の前に固定電話についての説明やフィルター質問（事前質問）を提示することで、回答者が同じ対象（固定電話）を想定できるように調査プロセスを設計することも有効である。 本書は、実際にアンケートを実施する際にはまず先行研究を調べ参照することを強く勧める。マーケティングに関する多くの尺度はすでに対応する質問文が開発されている。それらのほとんどは信頼性や妥当性の分析をクリアした質問内容なので、それらを引用するのが一番確実である。平たく言えば、とにかくまずは先行研究を探すべきだと言える。先行研究については主に、国際的な査読誌（海外ジャーナル）と、国内の文献とに分けることができる。海外ジャーナルに掲載された論文は、Google scholarや図書館システムから検索しアクセスすることが可能である。こちらの方がそもそもの論文量も多く、また、競争率および査読の水準の高いジャーナルの審査を乗り越えたという点から質の高い論文も多い。ただし、これらで参照できる論文及び尺度は英語で書かれているため、日本語で尺度を引用する場合には、日本語への翻訳と、バックトランスレーションによる日本語訳の適切性チェックが必要になる。一方で国内文献はCiniiや図書館システムから検索及びアクセスすることが可能である。こちらの方が相対的に量は少ないが、すでに適切な翻訳プロセスや、信頼性・妥当性チェックを経た尺度を提示しているものもあり、そのような尺度の場合、追加的努力を節約する形で既存の尺度を引用できる。 "],["アンケートデザイン.html", "4.3 アンケートデザイン", " 4.3 アンケートデザイン 前節では個別の質問項目設計に関する注意点を紹介したが、ここでは複数の質問やアンケート全体の構成についての説明を行う。アンケートで用いる各質問項目を確定したら、それをどのような順番で構成するのかについて考えることが必要になる。質問の掲載順においては、できるだけ回答者の回答への心理的負担を下げるような工夫を考慮する必要がある。そのため、基本的には答えやすく単純かつ興味を引く質問を最初に尋ね、広範な質問から特定的な質問へ移行し、難しい質問やセンシティブないしプライベートな情報を問う質問は最後に聞く、というような工夫が求められる。特に、アンケートの最初の質問には興味深く簡潔で威圧的でないものを選ぶと良い。例えば、回答者の素朴な意見や感想を聞く質問が有効であり、仮に調査には不必要であってもこの手の質問を最初に聞くことが有効になる場合もある。また、特定のトピックや製品・ブランドに関する項目はまとめて質問したり、時系列に関する質問は時系列順に問うなどの論理的な順番を守る構成も回答者の認知・心理的負担を減らすことに貢献すると考えられる。 アンケートの構成の他にも、レイアウトや回答の回収方法についても決定する必要がある。アンケート全体のフォーマット、文字スペースや質問文の配置などの装丁は、回答者にとっての可読性および回答率に影響を与えると考えられる。基本的にはシンプルかつ読みやすいレイアウトを心がけてデザインすることが必要になる。また、アンケート自体をいくつかのパートに分けつつ、各質問に番号を割り振るなどの工夫をすることで、回答者がアンケート回答の進捗を把握でき、途中離脱を防ぐことができるかもしれない。 次に回収方法は、大きく分けてオフラインとオンラインによる回収に区別することができる。オフラインでの回答回収には、対面質問（家庭訪問や商業施設での接触設問）、電話、郵送といった方法が存在する。対面法では、回答者と調査員とのやり取りが可能になるため、回答回収率が高かったり、質問に関する理解を促すような即時的なコミュニケーションが可能になるという利点もあるが、一回答あたりの費用が高いことや調査員の介在によるバイアスといった欠点もある。電話による調査は広範囲への調査と比較的高い回収率につながる方法として利用されたが、近年では用いられなくなっている。郵送法は、回答者に印刷された質問票と返信用封筒（切手添付または料金受取人払手続き済み）を送付し、回答後に調査者の住所に回答結果を返送してもらうという方法である。この方法であれば、回答者が好きなタイミングで回答でき、かつ広範囲への調査が可能になる。しかしこの方法には、回答状況を統制できない、回収率が相対的に低いといった欠点も存在する。 一方でオンラインでの回答回収は、eメールによってアンケートを送付する方法と、回答者にアンケートサイトにアクセスしてもらい、回答させる方法とに大別できるが、近年ではアンケートサイトにアクセスを促し回答を回収する方法が主流である。オンラインでの調査の場合、回答者はインターネット環境が整っていれば、いつでもどこでも回答することができる。また、調査者はページの見出し、セクション紹介と進捗バーを組み合わせることで回答者が回答をやめないように工夫する事ができる。加えて、調査者はオンラインであれば、画像、音声、映像やアニメーションなどの要素を含めることが可能である。一方で、調査者はアンケートに関わるプライバシーポリシーについて回答者に説明することが必要になる。ただし、オンラインでのアンケートでは、オンライン調査に参加するようなタイプの回答者からの回答しか収集できないという、回答者の傾向についてのバイアスについても理解する必要がある。 以上のような点に気をつけつつ、実際の調査項目やデザインを決定するのだが、アンケートを通じたデータ収集とそれを用いた分析においてよく観察される失敗とその対策について説明する。ここでは、研究者がアンケートにより得た情報に基づき、 ある二つの変数間の関係を捉える場合を考える。例えば、あなたが「顧客の知覚サービス品質と満足度との間には正の関係ある。」という理論に関心があったとする。ここで着目されている変数（概念）はサービス品質と満足度であり、財務データでは観察不可であるため、一般的に研究者はアンケートを通じた調査が用いることで情報を得る。この2変数間の関係を捉えるために研究者はアンケートをデザインする必要があるのだが、「一つの質問でまとめてこの関係を捉えようとする」、という誤ったアプローチを採用することがよく観察される。例えば、初めてアンケートを実施する学生は「あなたは、品質が高いサービスを経験すると満足しますか？」のような質問項目を設定しがちである4。 このような質問項目は、この項目に対する回答を基にどのように二変数間の関係を検証するのかが明確ではない（検証できない）という点で問題がある。複数の変数間の関係を捉えたい場合、各変数ごとの質問を別々に作成し、それぞれの質問への回答データを用いて二変数間の関係を統計的に分析するというプロセスを経る。この点については、財務データに基づく二変数間関係の分析と対比させるとわかりやすい。例えば、研究者が小売企業の店舗数と売上高の関係に関心があるとする。このとき研究者は企業レベルの店舗数と売上高についてそれぞれ別の変数として情報を集め、これらの変数間の関係を回帰分析などの手法で分析しようと試みるだろう。おそらく、多くの人が、店舗数と売上高の関係を分析するために、これら両方の情報を内包した一つの変数に関する情報を収集しようとは考えないはずである。アンケートによる調査・分析においても原則としては同様であり、特定の変数を排他的に捉えるような質問項目を作成し、それらに対して得た回答を基に、分析を行っていく必要がある。 変数間の関係を捉えるための調査設計について、もう少し具体的な説明を例とともに提示する。ここではあなたが「具体的な小売店舗の属性（価格）とその店舗へのロイヤルティとの関係」に関心があると仮定する。このとき、先述の悪い質問に該当する質問例は「あなたは、価格の低いお店を利用しますか？」や「あなたは、価格の低いお店をどの程度利用しますか？」である。これらの質問は、価格とロイヤルティの関係を検証できるデザインになっていない。では、具体的な店舗の特徴とその店舗への評価や利用状況をアンケートによって捉えるためにはどのようなデザインが考えられるのだろうか。本書では、以下の2つの対応例を紹介する。 回答者が利用している店舗を特定し、その店舗について回答してもらう。 研究者が準備したシナリオや実験刺激としての店舗情報などを提示して、その店舗について回答してもらう。 の方法の場合、アンケートにおいて回答者が想定する企業は回答者ごとに異質である。この方法では、回答者が頻繁に利用する店舗について特定化するような質問をしたあとに、その店舗についての評価や利用状況を尋ねるという階層的な質問構造を形成する。例えば、以下のような質問を構成することが考えられる。 「Q1 あなたは過去3か月間に次のどのスーパーの店舗に、最も良く食料品を買いに行きましたか。（回答は1つ）食料品を買うスーパーについて伺います。:リストを提示」 「Q2 あなたが食料品を買う際に最もよく利用する店舗【Q1スーパー名引用】でのお買い物の状況について、お答えください。週間の間にあなたがその店舗で食料品の買い物をする頻度をお答えください：1. １回未満, 2. 1回, 3. 2回,…」 「Q3 最もよく利用する店舗【Q1スーパー名引用】での食料品購入額が、ご家庭の食料品購入額全体の何％を占めているか、それぞれお答えください。: 選択肢を提示」 「Q4 最もよく利用する【Q1スーパー名引用】の店舗は従業員のサービスが手厚い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 「Q5 最もよく利用する【Q1スーパー名引用】は取扱製品の品質が高い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 一方で、2. のシナリオを提示する方法は、シナリオ実験アプローチと言われ、基本的な調査の構造は投薬の実験などと同様であり、ある刺激を受ける群と受けない群とでその後の結果に差があるかを捉える。このアプローチの場合、回答者が想定する店舗やその他の状況は特定化され、コントロールされている。この方法では、とある購買状況を想定してもらうための、全ての回答者に共通したシナリオを想定しつつ、検証したい施策のみ変化させた（施策あり vs. 施策なし）２種類のシナリオを準備する。そして研究者は、回答者を検証したい施策ありのシナリオを読むグループ（トリートメント群）と施策なしのシナリオを読むグループ（コントロール群）とにランダムでわけ、それぞれのグループ間で、回答者が異なる情報に直面するように調査を設計する。回答者はシナリオ読了後、成果変数に相当する質問に回答する。そして、成果変数に関するトリートメント群とコントロール群間での差を統計的に分析する、というアプローチを取る。ここでは、例として企業の活動に関するシナリオを読ませるという方法を紹介したが、この方法はシナリオに限らず、何かを実際に体験させたり、回答者にタスクを課すなど、様々な調査設計に応用する事が可能になる。 具体的なアンケートデザインが固まると、研究者は次に実際にデータを収集する段階に移る。ここでは第1に母集団を定め、次に、サンプルについて決定する必要がある。母集団は、研究者が求める情報を持つ要素や物の集合体と考えられ、研究課題に応じて研究者によって決定される。ここでは、研究者が求める情報を有しているのはどのような人たちかという母集団の要素（日本の一般消費者か、東京都内の国立大学の学部生か、等）や、適切な母集団の単位（個人、家計など）はなにかについて定義する必要がある。 母集団という集合体全体を捉えることは通常困難であるため、研究者は母集団に対応するアクセス可能な標本（サンプル）の情報を得る。サンプルに対するデータ収集プロセスでは、サンプリングフレーム、サンプリングテクニック、サンプルサイズを決定する必要がある。サンプリングフレームとは、対象となる母集団の要素を表現したものであり、対象となる母集団を特定するためのリストによって構成される。例えば、電話帳や調査会社から購入した個人や組織のリストがサンプリングフレームの例である。次に、サンプリングテクニックは、サンプルフレームから特定のサンプルをピックアップする方法である。サンプリングテクニックは非確率的サンプリングと確率的サンプリングに大別できる。比較率的サンプリングは、サンプルをランダムな選択により抽出しない方法であり、コンビニエンスサンプリングとスノーボールサンプリングがその典型例である。コンビニエンスサンプリングは、研究者にとって便利な要素のサンプルを集める方法であり、適切なタイミングで適切な場所にいたという理由で回答者が選ばれる。たとえば、学生やある組織の構成員を使った調査や、ショッピングモールでのインターセプトインタビューはその典型的な例である。次に、スノーボールサンプリングは、最初の回答者をランダムで選んだ後、その後の回答者は、最初に選ばれた人による紹介や情報提供によって選ぶ方法である。これらの方法は最もお金も時間もかからないという利点を持つが、サンプルセレクションバイアスに注意することが必要である。一方で確率的サンプリングは研究者による恣意性を排除した抽出方である。この代表例がランダムサンプルである。ランダムサンプルでは、サンプリングフレームからランダムな手順でサンプルを抽出する方法であり、すべての回答者は他の回答者とは独立して選択される。これにより、サンプルは、互いに独立で同一の確率分布に従う。また、システマティックサンプリングと呼ばれるサンプリングフレームからの開始点がランダムで決定され、そこから任意の i 番目の要素がピックアップされる方法も存在する。サンプルサイズは、抽出する標本の数を決定する。サンプルサイズの決定においては、予算の都合、回答者属性に基づく割付、慣習等の歴史的経緯、などの非統計的要因が影響する場合もある。一方で、統計的な要因としては、分析結果において想定される効果量を所与とし、確率的な計算に基づいて適切なサンプルサイズを決定する方法も存在するが、本書で詳しい内容は扱わない。基本的な方針としては、サンプルサイズが多いほど精度が高くなるという前提のもと、必要な精度を達成するために十分なサンプルサイズを抽出するというものになる（池尾等, 2010）。 テキストにこのように書かれていると、「自分はこんなことをしない」と思うだろうが、実際にアンケート設計をさせるとこのような誤りを犯す学生は存外多いので注意されたい↩︎ "],["参考文献-2.html", "4.4 参考文献", " 4.4 参考文献 池尾恭一・青木幸弘・南知惠子・井上哲浩（2010）「マーケティング」, 有斐閣. 田村正紀 (2006) 「リサーチ・デザイン: 経営知識創造の基本技術」, 白桃書房. 南知惠子・小野孔輔 (2010) 『日本版顧客満足度指数（JCSI）のモデル開発とその理論的な基礎』, 「マーケティングジャーナル」,30(1), 4-19. Malhotra, N. (2019) Marketing Research: An Applied Orientation, Pearson Education Limited. Sprott, D., Czellar, S., &amp; Spangenberg, E. (2009). The importance of a general measure of brand engagement on market behavior: Development and validation of a scale. Journal of Marketing Research, 46(1), 92–104. "],["データ処理と顧客関係管理crm.html", "Chapter 5 データ処理と顧客関係管理（CRM）", " Chapter 5 データ処理と顧客関係管理（CRM） これまでの本書の内容は、データを収集するまでの注意点や方法を説明した。しかしながら、収集したデータをただ眺めているだけでは、定量的な知見を得ることはできない。そのため、以降の節では主にデータ処理や分析手法について説明する。まず我々は、データセットの構築から学ぶ。例えばあなたがアンケートを実施したならば、そのアンケートからデータセットを構築する努力が必要になる。アンケート結果に基づくデータセット構築において研究者はコーディング、トランスクライビング、データクリーニングのプロセスを経る。 コーディングは、回答を分析可能なフォーマットへ変換する作業であり、通常回答に対して数値を当てはめる作業を伴う。例えば、回答者が男性ならば 1 を、女性ならば 0 をとるようなダミー変数を作成する作業がこれに当てはまる。コーディングは、不必要な情報を減らすことでデータ化プロセスを担う。トランスクライビングは、質問紙に記載された回答をデータ入力していく作業である。入力に関するヒューマンエラーは起こるものとして考える必要があるため、通常このプロセスは二人一組でダブルチェックをしながら行う。なお、オンラインアンケートの際はこのプロセスは自動で行われるため、不要になる。データクリーニングでは、研究者は不適切な回答のチェックを行う。例えば、回答可能範囲から外れた回答（例、7点尺度における8点回答）や、論理的に非整合的な回答（例、回答者が利用したことないと答えているサービスについて評価している場合） がないかをチェックする。また、欠損値という回答がない観測についてもチェックする必要がある。マーケティング分野においては欠損値のあるサンプルを削除するという方法も用いられるが、欠損値の扱いは奥が深く、いくつかの対応法がある。本書ではその詳細については扱わないが、欠損値に対応するためのデータ処理についての専門書も存在するため、関心のある読者はそれを参照してほしい（高橋・渡辺, 2017）。 データセットの構築が完了したあとは、本書では基本的にRを通じて様々な作業を行う。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、実際に我々分析者が各コマンドもシンプルになる。関数の利用においてはf(argument)のように関数名fのあとにカッコをつけて表記する。なお、argumentは日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。また、我々は通常、パッケージをインストール・起動することで他者が作った関数を利用することが多い。関数とパッケージについての説明や実行例は「Rの基本操作」節で紹介しているのでそちらを参照してほしい。 本章では、Rを用いてデータを整理・処理することで、複雑な統計分析を行わなくてもマーケティング的知見を得られることを、顧客関係管理を例に紹介する。ここで学ぶRでの作業は主に以下の通りである。なお、これらの作業は、統計的な分析を実行する前のデータ前処理にも使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。そのため、顧客関係管理に興味関心のない読者も、以下の手法について学習することを勧める。 データの読み込み（csv, excel, etc.） dplyrの利用とデータ整形 パイプ演算子を用いた複数処理の実行 Wide vs. Long data format (おまけ) "],["データの読み込み.html", "5.1 データの読み込み", " 5.1 データの読み込み 本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動すること。 install.packages(&quot;tidyverse&quot;, &quot;readr&quot;, &quot;readxl&quot;) library(tidyverse) library(readr) library(readxl) ここからは、データセットを用いた分析を行う。基本的な操作においては、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。具体的には、エクセルファイルが誰にでも開けるとは思ってはいけない。そのため、ソフト特性に依存しない汎用的な形式を使うことが好ましい。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない ’mktData.csv’というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。 df1 &lt;- readr::read_csv(&quot;mktData.csv&quot;) df2 &lt;- readr::read_csv(&quot;mktData.csv&quot;, col_name = FALSE) なお、デスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、「（補足）デスクトップ版の利用とプロジェクト機能」節で紹介している「プロジェクト機能」必ずを活用するようにほしい。 本講義では、大学の学務ポータル（manaba）を通じて教員が配布したデータを学生各自のコンピュータにダウンロードし、それを post.cloudに各自がアップロードするという手順によって分析用データを利用する。Manabaからのデータのダウンロードは各自で済ませてほしい。Posit.cloudはR studio 画面を表示する段階でプロジェクトが作成される。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する&lt;デスクトップ版を使用している場合も、Project を指定していれば、以下のコードで全く同じ結果を得ることができる。&gt; 具体的は、以下の通りdir.create() を使って新たに data というディレクトリ（フォルダ）を作成する。 dir.create(&quot;data&quot;) 新たなディレクトリを作成したら、そこに、ポータルよりダウンロードしたデータを入れてほしい。ここではまず “2022idpos.csv”というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに “idpos” というオブジェクト名を定義する。なお、ここで分析実行社はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) 問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によると、このidposデータは、3000行、5列のデータセットであることがわかる。なお、同様の情報は R studio 画面内の Environment タブから確認することできる。 head(idpos) ## # A tibble: 6 × 5 ## id date spent coupon datediff ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019-09-25 14326 1 7 ## 2 32 2019-09-10 10232 1 22 ## 3 30 2019-09-09 6881 1 23 ## 4 29 2019-09-04 6365 0 28 ## 5 46 2019-09-10 7595 1 22 ## 6 44 2019-09-14 7858 0 18 また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。 names(idpos) ## [1] &quot;id&quot; &quot;date&quot; &quot;spent&quot; &quot;coupon&quot; &quot;datediff&quot; glimpse(idpos) ## Rows: 3,000 ## Columns: 5 ## $ id &lt;dbl&gt; 12, 32, 30, 29, 46, 44, 44, 32, 3, 34, 36, 3, 42, 18, 38, 4, … ## $ date &lt;date&gt; 2019-09-25, 2019-09-10, 2019-09-09, 2019-09-04, 2019-09-10, … ## $ spent &lt;dbl&gt; 14326, 10232, 6881, 6365, 7595, 7858, 9405, 1821, 8375, 1828,… ## $ coupon &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1… ## $ datediff &lt;dbl&gt; 7, 22, 23, 28, 22, 18, 23, 12, 2, 25, 19, 7, 5, 3, 23, 22, 30… なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定し作成した、簡易的な人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。 "],["データの整形.html", "5.2 データの整形", " 5.2 データの整形 データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。しかしながら、tidyverseのインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。 summarize() mutate() filter() select() arrange() パイプ演算子 %&gt;% summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。 summarize(data, M = mean(var_name)) mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、dataというデータセットに対し、definition で定義した変数をnew_varとして追加するには、以下のコマンドを用いる（以下は見本コードである）。実際にdefinitionを定義する場合には、様々な関数や論理式を利用する必要がある。例えば、“new_var = var1/100” という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、“new_var = var1 – mean(var1)”という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。 mutate(data, new_var = definition) mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いるのが良い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には “==” （同値）, “&gt;=”（以上）, “&lt;=”（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。 mutate(data, new_var = ifelse(var1 == 2, 1, 0)) filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == “male”）のサンプル情報のみ抽出したい場合には以下のような指示になる。 filter(data, gender == &quot;male&quot;) なお、特定の条件以外のものを指定したいときは、 という論理式 “!=” (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。 filter(data, gender != &quot;male&quot;) select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。 data2&lt;- select(data, var1, var2, var3) 反対に、取り除きたい変数を指定するときには、以下のように “-” を使う。 data2&lt;- select(data, -var1) 列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。 data2&lt;- select(data, var1:var5) また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、“v” という文字から始まる変数を取り出すための指示は、いかのようになる。 data3&lt;- select(data, starts_with(&quot;v&quot;)) arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある。 data2 &lt;- arrange(data, var1) data2 &lt;- arrange(data, desc(var1)) また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。 data3 &lt;- rename(data2, var1 = sales) Tidyverse 内の dplyr を使うことでパイプ演算子（%&gt;%）が使える（ショートカット: Command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず \\(\\small 10-6\\) が計算され、その結果である “4” が sqrt() の引数として利用される（sqrt(4) は 2）。 (10-6) %&gt;% sqrt() ## [1] 2 パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。 男性の情報だけ抜き出す(filter) Var1の値について降順に並べ替える(arrange) 第一位から最下位までの順位を割り当てた ranking 変数を作る(mutate) var1 , var2, var3, var4, orderだけ残し(select) new_dataとして定義する 上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。 new_data &lt;- select( mutate( arrange( filter(data, gender == &quot;male&quot;), desc(var1)), ranking = 1:n()), var1, var2, var3, var4, order) パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。 new_data &lt;- data %&gt;% filter(gender == &quot;male&quot;)%&gt;% arrange(desc(var1)) %&gt;% mutate(ranking = 1:n()) %&gt;% select(var1, var2, var3, var4, order) パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に “.” （ドット）を使う。たとえば、\\(\\small 10-2\\)の計算結果を用いて2から8の偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。 (10-2) %&gt;% seq(from = 2, to = ., by = 2) ## [1] 2 4 6 8 データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、“#Rds” 以降のコードを用いる。 readr::write_csv(df, path = &quot;data/new_data.csv&quot;) #Rds readr::write_rds(df, path = &quot;data/new_data.Rds&quot;) "],["統計的検定を用いない顧客分析.html", "5.3 統計的検定を用いない顧客分析", " 5.3 統計的検定を用いない顧客分析 5.3.1 データの構造変化とソート 顧客の購買データを用いて、（統計的な分析を要さず）重要顧客や顧客層を発見することが、小売企業を中心に行われている。ここでは基本的に、ID-POSデータを用いたデータベースの正規化と集計2焦点を合わせる。特に、顧客個人に関する情報を用いながら企業や店舗にとって重要な顧客を特定し、その顧客との関係性を深めた場合を考える。店舗の運営効率から考えると、単に来店客数を増やすだけでなく、より頻繁に、より高額の買い物をする顧客を特定し、その人（達）の購買を促進することが効果的になる。言い換えると、企業や店舗は、ロイヤルカスタマーを特定し、その顧客との関係性を構築したいと考えるのである。そのためにはまず、ロイヤルカスタマーを特定する作業が必要になる。そこで本節では、データから企業にとって価値のある顧客を発見する方法について、データの前処理技術を応用する形で紹介する。 本節では簡単に、単純なデータハンドリングから顧客インサイトを得る方法を考える。特に、データ処理とソーティング（順番の入れ替え）を用いる方法を用いる。本節ではID-POSデータを用いた分析として、デシル分析とRFM分析を紹介する。デシル分析は、支出額をもとに上位から顧客を並べ替え、その順番に基づき顧客を10分割することで、上位の支出額を担うランクに属する顧客を特定する。なお、他の指標で同様の分析を実行することも可能だが、一般的には支出額を用いることが多い。例えば、月当たり5000人の顧客がいるとすると、500人ずつのグループに分け、購買額の大きい順にデシル1〜10 (10〜1の場合もある) とする形でランク分けする方法がこれにあたる。このとき、各顧客の情報がポイントカードやアプリで紐付いているのであれば、最も購買額の多いグループの特徴を整理することで、現在購入額の高い顧客がどんな特徴を持つのか理解できる。 一方、RFM（Recency, Frequency, Monetary）分析は、取引情報から、最近いつ買ったか、どれだけの頻度で買い物するか、どれだけ支出しているかといった情報を総合的に勘案し、どの顧客が最重要かを特定する方法である。これらの指標は、ロイヤルティや再購買確率が高い顧客を判別するのに役立つ3つの指標である。例えば、最終購買日から時間が経っている顧客は離反しているかもしれないし、購買頻度や購買額が高いと、ロイヤルティが高い可能性が高い。また、クーポンや割引利用の有無の情報と紐付けることができれば、当該顧客がチェリーピッカーか否かも判断することができる。 ID-POSデータは、各顧客の会員IDについての情報はありながらも取引ベースで情報が整理されている。このようなデータに対して以下の手順を用いてデータを集計・ソーティングする。 顧客IDごとに、各取引情報を集計する。 顧客ID情報についてまとめたデータベースにおける順番をソートし、重要顧客を識別する。 このようなデータの集計は、データ構造を取引ベースから顧客IDベースに変換することを可能にする。下図は、取引ベースのID-POSデータを、集計作業によって顧客IDベースのデータ構造に変換するイメージを示したものである。ID-POSデータは、顧客ID情報が含まれていながらも、データの行（観測）は各取引を示している。そのため、仮に同じIDの顧客がデータ収集期間に複数回取引を行っている場合、同じIDを含む観測がいくつも見られることになる。一方で下部の顧客IDベースのデータは、ID-POSデータを顧客ID情報によって集計したものであり、一定期間中に特定のIDを持つ顧客がどのような購買行動を示していたかを捉えたデータである。そのため、データの行は各顧客IDを示している。本節では、まずはじめにこのようなデータ構造の変換について説明する。 データ構造変化 ここでは、先程利用した idpos データを用いて作業を進める。改めて、当該データを以下のコマンドで確認する。 head(idpos) ## # A tibble: 6 × 5 ## id date spent coupon datediff ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019-09-25 14326 1 7 ## 2 32 2019-09-10 10232 1 22 ## 3 30 2019-09-09 6881 1 23 ## 4 29 2019-09-04 6365 0 28 ## 5 46 2019-09-10 7595 1 22 ## 6 44 2019-09-14 7858 0 18 ここで、date変数を用いて直近で何日前の来店かを示す変数を作成する。このデータは、2019年09月01日 から 2019年10月01日までの一ヶ月間、とある店舗で記録された取引データであると仮定し作成されている。そのため、データ収集終了最新時点（2019-10-2）と来店日時の差を表す変数を作成する (ここでの処理にエラーが出る場合は、idpos$date &lt;- as.Date(idpos$date) というコマンドを事前に試してから変数の定義を行ってほしい)。head関数により出力された結果によって新たな変数（datediff）が追加されたことがわかる。 idpos$datediff&lt;- round(as.numeric(difftime(&quot;2019-10-02&quot;,idpos$date,units=&quot;days&quot;))) head(idpos) ## # A tibble: 6 × 5 ## id date spent coupon datediff ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019-09-25 14326 1 7 ## 2 32 2019-09-10 10232 1 22 ## 3 30 2019-09-09 6881 1 23 ## 4 29 2019-09-04 6365 0 28 ## 5 46 2019-09-10 7595 1 22 ## 6 44 2019-09-14 7858 0 18 続いて、パイプ演算子を使ったデータ処理によって顧客IDベースの形へ集計する。ここでは特に、group_by() という関数を使い、顧客id (今回は性別情報も残したいので gender も加えている)をグループ化の基準と指定する形で集計を行う。また、CRM分析で使う変数のために、idレベルでの集計という形で以下の変数を作成する。そして、以下の変数を用いて集計した新たなデータセットを “idpos_cust” として定義する。 frequency：各idの出現頻度をn()でカウントする monetary：spentの合計をsum()で計算する cherry (picker)：クーポンの利用回数の合計をsum()で計算する recency：datediffの最小値をmin()で求め、直近でいつ来たかを判別 idpos_cust &lt;- idpos %&gt;% group_by(id) %&gt;% summarize(frequency = n(), monetary = sum(spent), cherry = sum(coupon), recency = min(datediff) ) head(idpos_cust) ## # A tibble: 6 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 ## 2 2 20 157976 13 2 ## 3 3 21 134673 5 1 ## 4 4 19 154416 10 4 ## 5 5 20 177156 11 3 ## 6 6 19 151853 11 3 上記の操作によって、元々のidposデータから、顧客ベースのデータ構造（idpos_cust）に変換できたはずである。しかしこれだけでは、まだ我々は誰が重要顧客か特定できない。そのため、次に我々はデータの並べかえを行う。具体的には、支出額が高い順に並び替えたあとに上位20人の顧客を表示する。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) ##Customers in the top 20 (Monetary) idpos_cust_m[1:20,] ## # A tibble: 20 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15 31 305665 13 1 ## 2 35 30 262181 13 1 ## 3 31 31 254804 19 3 ## 4 33 29 251352 14 2 ## 5 22 24 214150 9 4 ## 6 20 21 199197 11 2 ## 7 18 24 196686 13 3 ## 8 49 24 196143 12 4 ## 9 9 24 195788 14 1 ## 10 12 25 194860 15 1 ## 11 42 21 193582 5 1 ## 12 30 26 190687 17 2 ## 13 36 20 189937 13 6 ## 14 14 21 188600 14 1 ## 15 26 22 184657 10 2 ## 16 32 20 184276 12 1 ## 17 17 19 181947 13 1 ## 18 44 23 181397 12 1 ## 19 34 20 179533 8 2 ## 20 37 21 179169 11 2 5.3.2 データ結合 ここまでの結果からは購買額の高い顧客IDを特定することができた。しかしながら、これらの顧客がどのような特徴を持っているのかについては推察できない。そのため、別で管理されていた顧客情報を捉えたデータセットと結合することでこれらの顧客についての属性を把握する。 以下では、今回使用する顧客情報データセットを読み込み、その概要を示している。このデータセットには、3000人分の会員登録済み顧客情報が蓄積されており、以下の変数を含む： id: 顧客ID gender: 性別 age: 年齢 famsize: 世帯人数 id_data &lt;- readr::read_csv(&quot;data/id_data.csv&quot;, na = &quot;.&quot;) str(id_data) ## spc_tbl_ [3,000 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:3000] 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : chr [1:3000] &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;female&quot; ... ## $ age : num [1:3000] 51 38 41 24 48 46 36 30 26 57 ... ## $ famsize: num [1:3000] 2 2 1 1 5 1 1 1 3 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. famsize = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ここで、顧客データと購買データを left_join() を用いて、idpos_cust をメインとする形で id によって結合する。left_join() は左側に指定したデータフレームに存在知するキーの行を返す形でデータの結合を行う。言い換えると、左側のデータセットに存在する行（観測）はすべて残され、そこに新たな変数を加える形でデータフレーム間の結合を行う。 idpos_cust &lt;- left_join(idpos_cust,id_data, by = &quot;id&quot;) head(idpos_cust) ## # A tibble: 6 × 8 ## id frequency monetary cherry recency gender age famsize ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 female 51 2 ## 2 2 20 157976 13 2 male 38 2 ## 3 3 21 134673 5 1 female 41 1 ## 4 4 19 154416 10 4 female 24 1 ## 5 5 20 177156 11 3 female 48 5 ## 6 6 19 151853 11 3 female 46 1 上記の通り、顧客ベースの取引情報に、各顧客の属性情報が追加された事がわかる。これを利用し、以下のように上位20顧客の性別比率を以下のように確認する。これによって、主要顧客に占める性別比率が確認できる。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) #gender ratio in the top 20 table(idpos_cust_m[1:20,]$gender) ## ## female male ## 16 4 続いては先述のデシル分析を実行する。具体的には、idpos_custに対し、cut()関数を使うことで、monetaryの大きさに基づきサンプルを10等分し、新たに “decile_rank” という変数（列）をデータに追加し、その新たなデータセットを “idpos_cust_m”と定義する。なお、次節にてこのデータを改めて使うため、データをprojectのdataディレクトリ内に保存しておいてほしい。 idpos_cust_m$decile_rank &lt;- cut(idpos_cust_m$monetary, quantile(idpos_cust_m$monetary, (0:10)/10,na.rm=TRUE), label=FALSE,include.lowest=TRUE) head(idpos_cust_m) ## # A tibble: 6 × 9 ## id frequency monetary cherry recency gender age famsize decile_rank ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 15 31 305665 13 1 female 19 3 10 ## 2 35 30 262181 13 1 female 22 3 10 ## 3 31 31 254804 19 3 female 55 1 10 ## 4 33 29 251352 14 2 male 54 5 10 ## 5 22 24 214150 9 4 female 53 4 10 ## 6 20 21 199197 11 2 female 50 1 10 readr::write_csv(idpos_cust_m, &quot;data/idpos_customer.csv&quot;) head()関数によって新たな変数の追加を確認したあとは、各デシルの店舗売上への貢献度を確認する。ここでは、decile_rankをグループ化の基準として設定し、summarize() によって集計する方法を用いる。その後、各デシルの売上比率を計算し、高い順に並び替える。集計・分析の結果は、上位20%の顧客で、ID-POSに計上されている売上の57%を締めていることを示した。 decile &lt;- idpos_cust_m %&gt;% group_by(decile_rank) %&gt;% summarize(freq = n(), monetary = sum(monetary)) total &lt;- sum(decile$monetary) decile2 &lt;- decile %&gt;% mutate(percent = monetary/total*100) %&gt;% arrange(desc(decile_rank)) decile2 ## # A tibble: 10 × 4 ## decile_rank freq monetary percent ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 149 11141710 45.1 ## 2 9 149 3042467 12.3 ## 3 8 147 2220019 8.98 ## 4 7 150 1976807 8.00 ## 5 6 148 1687031 6.83 ## 6 5 149 1466609 5.93 ## 7 4 149 1213922 4.91 ## 8 3 148 939338 3.80 ## 9 2 149 670413 2.71 ## 10 1 149 358988 1.45 本節で示したように、高度な統計的分析を実行せずとも重要顧客を特定する事が可能になる。本節では特に、データの集計や処理技術を使った方法を紹介した。このような分析によって回答できる問いは「企業にとっての重要顧客は誰か」というものだろう。この問いは非常に興味深く実務的にも有意義なものであるが、以下の点に注意することが重要である。第一に、何をもって顧客の重要性を定義するかという問題である。本節では特にRFMなどの基準を用いて、観察可能な購買結果をもとに重要顧客を識別する方法を捉えた。しかしながら、例えば購買頻度と購買額では異なる側面を捉えており、どの指標を用いて分析するかによって（通常は）結果が異なる。そのため、研究者自身が「重要性」をどのように定義するのかを注意深く判断し、なおかつそれをレポートやプレゼンテーション内できちんと明示する必要がある。さらに、データでは捉えきれない側面は分析結果に反映されないという点についても注意が必要である。例えば日本には江戸時代から続く小売企業もいくつも存在する。仮に、そのような小売企業と、長期間代々取引を続けている顧客（一族や企業）がいたとして、さらにその顧客が分析による上位顧客に含まれなかったとする。その場合、この顧客を重要顧客でないと切り捨てて良いのでだろうか。災害、国家の統治体制の変化、戦争、などの激動を経てなお取引が続いている顧客は重要でないと言い切れるのだろうか。もちろん、このような顧客を重要でないと捉えることも、経営判断として間違ったものではない。しかしながら、少なくともデータを用いた分析結果を過信しすぎず、データによって何が捉えられており、何が捉えきれていないのかについて研究者・意思決定者のどちらも自覚的になることが必要になる。 第二に、分析を行うだけでマーケティング実務が完結するわけではないという点についても注意が必要である。「重要顧客を特定する」という研究課題の背後には、「CRMを実行して収益性を向上させる」という実務的課題が存在しているはずである。そのため、今回の発見物をもとに、マーケティング活動への示唆を与えていくことが重要になるのだが、誰が重要顧客か、という問いに答えるだけでは具体的な活動指針（アプリを通じた囲い込みや、訪問販売等）を与えるのは難しい。そのため、重要顧客のライフスタイルや価値観などの彼/彼女らの特徴に踏み込んだ調査を行うことも必要になるかもしれない。昨今のロイヤルティプログラム（ポイントシステム）では、モバイルアプリを通じて個人の様々な行動履歴が記録されたり、アンケートへの回答を促されたりすることがあるだろう。これらの情報とID-POSデータをうまく接合できれば、重要顧客を特定しつつ、それらの顧客に適したCRM方策を策定できるかもしれない。 "],["おまけ-wide型とlong型データセット.html", "5.4 (おまけ) Wide型とLong型データセット", " 5.4 (おまけ) Wide型とLong型データセット インターネットを通じて、とても都合の良いかつ信頼できるデータセットが入手できたとしても、それが分析のために望ましい形で保存されているとは限らない。特に、横長(wide)と縦長(long)データが存在し、それらのデータの型の違いには注意が必要である。我々人間がデータを眺め、解釈を与える場合にはwide型データのほうが扱いやすいのだが、コンピュータやソフトウェアがデータを分析する際には、long型のほうが好ましい。例えば、下図は4店舗のある年の6月から10月までの売上情報（単位：千円）を示したデータセットである。これは、複数サンプル-複数時点という構造のデータだが、各時点の観測値が横に並んでおり、wide型データだといえる。 Wide型データ 手元にあるWide型データをLong型に変換したい場合の対応策として、ここでは tidyverseに含まれているtidyrのgather()関数を用いる方法を紹介する。この関数の利用方法を実演するためにManabaにアップされている sales_wide.csv をダウンロードし、プロジェクト内のdataディレクトリに移して欲しい。データを読み込むと、以下のようにデータ構造を確認することができる。 sales_wide &lt;- readr::read_csv(&quot;data/sales_wide.csv&quot;, na = &quot;.&quot;) ## Rows: 4 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): store, sales06, sales07, sales08, sales09, sales10 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sales_wide ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 ここで用いる gather()関数は、以下の引数を指定する： - data: 変換元のデータ - key: 変数を１列にまとめたあと、元の列を区別するための列につける名前 - value: 変数を１列にまとめたあと、値が入る列につける名前 - どの範囲を一列にまとめるかの範囲指定 その上で、さきほどのsalesデータをLong型に変換するために、以下のようなコマンドを利用する。 sales_long&lt;- gather(data = sales_wide, key = &quot;month&quot;, value = &quot;sales&quot;, starts_with(&quot;sales&quot;)) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 sales06 1500 ## 2 2 sales06 900 ## 3 3 sales06 700 ## 4 4 sales06 1000 ## 5 1 sales07 2000 ## 6 2 sales07 1500 ## 7 3 sales07 900 ## 8 4 sales07 1300 ## 9 1 sales08 2300 ## 10 2 sales08 1600 ## 11 3 sales08 1000 ## 12 4 sales08 2000 ## 13 1 sales09 1700 ## 14 2 sales09 1600 ## 15 3 sales09 800 ## 16 4 sales09 1500 ## 17 1 sales10 1500 ## 18 2 sales10 1000 ## 19 3 sales10 650 ## 20 4 sales10 1400 編集された縦長データは上記の通り示されるが、その中身を見ると、monthの列にsales06などの情報が記載されており、好ましくない。この問題は、元のデータにおける変数名に該当する情報を新しいkey列の値に使うというgather関数の仕様に影響するものである。この問題に対しては、パイプ演算子を使ってgatherの実行前に、列名を年だけの形に変更することで対応可能である。以下が、修正版のコードであり、出力結果より、先述の問題点が解決されたことが確認できる。ただし、gather関数の実行において、下記コード内では範囲の引数の指定についても修正されていることに注意が必要である。 sales_long &lt;- sales_wide %&gt;% rename(`06` = sales06, `07` = sales07, `08` = sales08, `09` = sales09, `10` = sales10) %&gt;% gather(key = &quot;month&quot;,value = &quot;sales&quot;, `06`:`10`) %&gt;% arrange(store) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 06 1500 ## 2 1 07 2000 ## 3 1 08 2300 ## 4 1 09 1700 ## 5 1 10 1500 ## 6 2 06 900 ## 7 2 07 1500 ## 8 2 08 1600 ## 9 2 09 1600 ## 10 2 10 1000 ## 11 3 06 700 ## 12 3 07 900 ## 13 3 08 1000 ## 14 3 09 800 ## 15 3 10 650 ## 16 4 06 1000 ## 17 4 07 1300 ## 18 4 08 2000 ## 19 4 09 1500 ## 20 4 10 1400 一方で、Long型データからWide型データへ変換するためには、tidyrのspread()を用いることが多い。spread() では、主に以下の引数を用いる。 data: 変換元のデータ key: 変数を複数列にわけるとき、列を区別するための変数 value: 複数列に分ける値 例えば、以下のコードで先程の sales_long データをwide型に変換することができる。 wide_test &lt;- sales_long %&gt;% spread(key = &quot;month&quot;, value = &quot;sales&quot;) %&gt;% rename(sales06 = `06`, sales07 = `07`, sales08 = `08`, sales09 = `09`, sales10 = `10`) wide_test ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 "],["参考文献-3.html", "5.5 参考文献", " 5.5 参考文献 高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版. 松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社. "],["データの要約と可視化.html", "Chapter 6 データの要約と可視化", " Chapter 6 データの要約と可視化 本章では、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。なお、本章の作業においても tidyverse を用いるので、以下のように tidyverse を起動してほしい。 library(tidyverse) "],["記述統計.html", "6.1 記述統計", " 6.1 記述統計 記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。「データのタイプ」という節で確認したように、データには量的変数とカテゴリ（を示す質的）変数がある。量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。 その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値はおもにデータの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる5。 d &lt;- c(1, 3, 2, 5, 4) median(d) ## [1] 3 d2 &lt;- c(1, 3, 2, 5, 4, 6) median(d2) ## [1] 3.5 平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、\\(\\small x_1,x_2,...,x_n\\) に対して以下のように定義される。 \\[\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\] 観測値と平均値の差（\\(x_i - \\bar{x}\\)）は偏差と呼ばれ、偏差の和はゼロである（\\(\\sum_ix_i - \\bar{x}=0\\)）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、mean() 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。 数学: (3,3,5,5,5,5,5,7,7) 国語: (2,3,3,5,5,5,7,7,8) このときの平均値は以下のように求まる。 math &lt;- c(3,3,5,5,5,5,5,7,7) jpn &lt;- c(2,3,3,5,5,5,7,7,8) mean(math) ## [1] 5 mean(jpn) ## [1] 5 計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。 分散 (Variance, \\(S^2\\)で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。 \\[S^2 = \\frac{1}{n}\\sum_i^n (x_i-\\bar{x})^2\\] しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 (\\(\\sqrt{\\cdot}\\)) を取った値を標準偏差と呼び、この標準偏差を用いることも多い6。なお、Rでは var() と sd() によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では \\(s^2=\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2\\) で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これは、これらの指標のほうが統計的に好ましい性質を持っているためであるが、Rを用いた分散の計算値が、nで割った際の手計算値と異なることがあるのでその点には注意が必要である。 var(math) ## [1] 2 var(jpn) ## [1] 4.25 先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。 観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 \\(x_1,...,x_n\\) に対して、標準化されたスコア \\(z_1,..,z_n\\) は以下のように定義できる。 \\[ z_i=\\frac{(x_i-\\bar{x})}{\\sqrt{(S^2)}} \\] ただし、 \\(S^2\\) は変数 \\(x\\) の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。 一方でデータの観測数（ \\(n\\) ）が偶数である場合、\\(\\small n/2\\) 番目と、\\(\\small (n/2)+1\\) 番目が中央となるため、n個のデータの観測値を、\\(x_1,x_2,...,x_n\\) とすると、これらふたつの値の平均値（ \\(\\small \\frac{x_{\\frac{n}{2}}+x_{ \\frac{n}{2}+1}}{2}\\) ）が中央値となる。Rにおいてはmedian() 関数によって以下のように計算することができる。↩︎ 偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。↩︎ "],["カテゴリ変数の要約.html", "6.2 カテゴリ変数の要約", " 6.2 カテゴリ変数の要約 一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、table() 関数を用いる。また、with()関数を用いて同様の結果を得ることも可能である。ここでは、前節で保存した顧客ベースのidposデータを用いる。 idpos_cust &lt;- readr::read_csv(&quot;data/idpos_customer.csv&quot;) table(idpos_cust$gender) ## ## female male ## 985 502 with(idpos_cust, table(gender)) ## gender ## female male ## 985 502 また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける各デシルランクと性別の関係は以下のように示される。 with(idpos_cust, table(gender,decile_rank)) ## decile_rank ## gender 1 2 3 4 5 6 7 8 9 10 ## female 102 100 99 94 106 103 97 83 98 103 ## male 47 49 49 55 43 45 53 64 51 46 特定のカテゴリ（例、デシルランク）に着目して、カテゴリ変数（例、性別）についての集計を行うことも可能である。例えば、デシルランク10における男女差のみを調べたいときには、filter() 関数を用いれば良い。 idpos_cust %&gt;% filter(decile_rank == 10) %&gt;% with(table(gender)) ## gender ## female male ## 103 46 カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。また、そのための手法はすでに我々は学習済みである。具体的には、前節で利用した group_by() 関数を用いる。例えば、合計支出額と購買頻度の平均と標準偏差を男女ごとに確認することは、以下のような指示で可能になる。なお、tidyverseを起動していない場合には、必要に応じて library(tidyverse) を事前に指示してほしい。 idpos_cust %&gt;% group_by(gender) %&gt;% summarize(mon_m = mean(monetary), mon_sd = sd(monetary), freq_m = mean(frequency), freq_sd = sd(frequency)) ## # A tibble: 2 × 5 ## gender mon_m mon_sd freq_m freq_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 17511. 32770. 2.13 3.88 ## 2 male 14878. 23222. 1.80 2.77 "],["データの可視化.html", "6.3 データの可視化", " 6.3 データの可視化 本書でのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データは一般的に、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化される。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて library(tidyverse) によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。 head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。 ##For mac users theme_set(theme_gray(base_size = 10, base_family = &quot;HiraMinProN-W3&quot;)) 本書の可視化では、まず、ggplot2の ggplot() 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。 data: 可視化に用いるデータフレームの指定 mapping: データから抽出する変数と画面に表示される図との関係の指定 mapping内で、aes() 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。 ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定されている。続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) 用いてグラフィックの層(layer)を加えることで図を作成する。このプロセスでは、geom_point() による散布図や、geom_histogram() によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に labs() というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。 ggplot2を用いたデータ可視化の例として、まず本書はヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに geom_histogram() を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例である。価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。 p1 &lt;- ggplot(diamonds, mapping = aes(x = price)) p1 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム1: ダイアモンド価格&quot;) なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、“p1” というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。 p1 + geom_density(fill = &quot;black&quot;, alpha = 0.5) + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム2: ダイアモンド価格（geom_density）&quot;) 箱ひげ図は、四分位数と四分位範囲等を図示化したもの。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。 p2 &lt;- ggplot(diamonds, mapping = aes(x = cut, y = price)) p2 + geom_boxplot() + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;箱ひげ図1: ダイアモンド価格&quot;) 箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上小さい、ないしは、Q3よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。 バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、geom_violin() を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。 p2 + geom_violin() + geom_boxplot(fill = &quot;gray&quot;, width = 0.1) + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;バイオリンプロット: ダイアモンド価格&quot;) バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。 "],["二変数間の関係の要約.html", "6.4 二変数間の関係の要約", " 6.4 二変数間の関係の要約 ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（\\(S_{xy}\\)）は、以下のように定義される。なお、Rで共分散を求める際には cov() 関数を用いる。 \\(S_{xy}=\\frac{1}{n}\\sum_i^n (x_i-\\bar{x})(y_i-\\bar{y})\\) また、\\(S_x\\)と\\(S_y\\)をそれぞれxとyの分散とし、相関係数（\\(\\rho_{xy}\\)）は以下のように定義される。Rで相関係数を求める際には cor() 関数を用いる。 \\(\\rho_{xy}=\\frac{S_{xy}}{\\sqrt{S_x^2}\\cdot \\sqrt{S_y^2}}\\) 共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を、着目している二つの変数が持つことが知られている。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、線形関係の程度である。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。 例えば、以下のようなデータセットを考える。 ## # A tibble: 5 × 3 ## x1 y1 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 16 8 ## 2 -1 12 6 ## 3 0 10 5 ## 4 2 6 3 ## 5 5 0 0 このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（\\(y=-2x+10\\)）に並ぶことがわかる。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は \\(y=-x+5\\)である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。 cor(X$x1, X$y1) ## [1] -1 ggplot(data = X, mapping = aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = lm) また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。 ## # A tibble: 5 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 4 ## 2 -1 1 ## 3 0 0 ## 4 1 1 ## 5 2 4 cor(AB$A, AB$B) ## [1] 0 しかしながら、両者の関係を描画すると、\\(y = x^2\\) という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。 ggplot(data = AB, mapping = aes(x = A, y = B)) + geom_point() + geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE) 勘の良い読者であればすでに気づいているかも知れないが、二変数間の関係についての可視化もggplot2にて対応できる。具体的には、geom_point()という関数を用いるのだが、それだけではなく、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関を計算する。 cov(diamonds$carat,diamonds$price) ## [1] 1742.765 cor(diamonds$carat,diamonds$price) ## [1] 0.9215913 これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。 p3 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price)) p3 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図1: カラット：価格&quot;) 研究目的次第では、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。 Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。 facet_grid() や facet_wrap() を用いる。 まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。 p4 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut)) p4 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, color = &quot;カット&quot;, title = &quot;散布図 2: カット別、カラット：価格&quot;) このように、mapping = aes() 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、color =という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合には、この可視化の方法だと逆に見にくいかもしれない。そこで、以下の方法を紹介する。facet_wrap() を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。geom_point() で散布図作成の指示を与えたあとに、facet_wrap() のレイヤーを重ねる指示を与えれば、散布図 3が作成される。 p3 + geom_point() + facet_wrap(~cut) + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図 3: カット別、カラット：価格&quot;) 散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。 これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。 p5 &lt;- diamonds %&gt;% filter(carat &gt;= 1.0) %&gt;% ggplot(mapping = aes(x = price)) p5 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット以上&quot;) p6 &lt;- diamonds %&gt;% filter(carat &lt; 1.0) %&gt;% ggplot(mapping = aes(x = price)) p6 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット未満&quot;) Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。 ggsave(filename = &quot;plot1.pdf&quot;, plot = plot1, width = 10, height = 5, units = &quot;cm&quot;) 日本語を含む頭の場合、quartz() を用いた以下の手順を経て図を保存する。 1. quartz()で作図デバイスを起動する。 2. 作図デバイスを開いたまま、Rstudio内で図を表示する。 3. dev.off()という指示で作図デバイスを閉じることで図が保存される。 また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -&gt; Save as Image/ Save as PDF -&gt; Directory -&gt; File name）。 "],["参考文献-4.html", "6.5 参考文献", " 6.5 参考文献 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. Healy, Kieran (2018) Data Visualization: A Practical Introduction, Princeton University Press. "],["基礎統計学復習.html", "Chapter 7 基礎統計学復習", " Chapter 7 基礎統計学復習 Rを用いて、統計的な分析（区間推定や検定）を実行すること自体はさほど難しいことではない。基本的な分析に必要な関数は基本パッケージに搭載されており、コードの書き方（引数の設定など）もネット上で検索すれば容易に知ることができる。しかしながら、あなた自身もしくは他者実行した分析をきちんと理解するためには、基礎的な統計学の内容を理解している必要がある。本資料は、基礎統計学を履修済みの学生を想定して書かれているが、本章でいくつかの基礎的な統計学の知識を復習する。また、本資料ではあくまで簡易的に説明を加えるのみなので、統計学を未習の場合は、基礎統計学の図書や講義で学習することを強く推奨する。また、章末に統計学や計量経済学の学習に役立つ参考文献を提示しているので、各自の学習に役立ててほしい。 "],["確率モデル期待値と分散.html", "7.1 確率モデル、期待値と分散", " 7.1 確率モデル、期待値と分散 7.1.1 確率と確率変数 伝統的なデータ分析では、標本から得た情報に基づき母集団の性質について推測する。母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈することができる（倉田・星野、2011）。そのうえで確率とは、起こりうる事象の集合内において、各事象の起こりやすさの度合いを０以上１以下の実数で表したものである。より詳細な定義として、標本空間を \\(\\small \\Omega\\)、任意の事象 A に対して実数 P(A) が定まっていて、以下の三つを満たすとき、P(A)は事象 A の確率という： 確率は非負であり、以下を満たす： \\[0\\leq P(A)\\leq 1\\] 全事象を \\(\\Omega\\)、空事象を \\(\\emptyset\\) としするとこれらの確率は以下の様に示される： \\[P(\\Omega)=1,~~ P(\\emptyset)=0\\] 事象 \\(A_1,A_2,...\\) が互いに排反ならば、これらのうち少なくとも１つが起こる事象 \\(A_1\\cup A_2\\cup ...\\) の確率は以下となる： \\[ P(A_1\\cup A_2\\cup ...)=P(A_1)+P(A_2)+... \\] 確率変数とは、ある標本空間上で定義される取りうる各値に対してそれぞれ一定の確率と対応関係のあるような変数である。例えば、細工のないサイコロを投げるとき、出た目の値を \\(x\\) とすると、\\(x\\) は１から６までの整数を取りうる変数だと言い換えることができる。この場合標本空間は、取りうる出目に対応した6個の標本点からなる。またこれらの標本点には、それぞれ対応する確率が以下のように付与されている。 Table 7.1: サイコロの確率分布 x 1 2 3 4 5 6 確率 1/6 1/6 1/6 1/6 1/6 1/6 このように、確率変数の取る値に対応して確率が付与されるルール（ \\(x\\) の関数としての確率 \\(P(x)\\) ）を確率分布や確率分布関数という。確率変数は主に、離散確率変数と連続確率変数に分けることができる。離散確率変数は、サイコロのように、取りうる値が離散的な確率変数である。一方で、連続確率変数は、ある範囲の中で連続的にどんな値も取りうる確率変数である。離散確率変数では、サイコロの表で示されているように、取りうる特定の値に対応する確率を確率分布に基づき計算できる。 一方で連続確率変数の場合、取りうる値の数が無限に存在するため取りうるある特定の値に対応する確率は０になる。もし取りうる各値に確率が付与されていると、確率の合計が無限大になってしまうという問題に直面する。そのため、連続確率変数の場合、取りうる区間に対して確率が付与される。これを踏まえて連続確率変数を捉え直すと、連続確率変数は、その取りうる任意の区間に対して一定の確率が対応するような変数であるといえる。また、連続確率変数における取りうる区間の起こりやすさには「確率密度」が対応することで計算可能になる。言い換えると、確率変数 \\(x\\) の値に確率密度がどのように対応するのかという関係は、\\(f(x)\\) という確率密度関数（probability density function: PDF）として示される。PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) が区間 [a, b] を取る確率 \\(P(a\\leq x \\leq b)\\) は、以下の積分計算で求められる。 \\[ P(a\\leq x \\leq b)=\\int^b_a f(x) dx \\] 以下の図はPDFの例であり、図内の曲線はPDFを、灰色に塗られている面積はある区間の確率を示している。なお、上記の式で示されている関係から、PDFを特定（仮定）することで、ある確率に対応する区間 [a, b] を求めることも可能である。以降の節で紹介する統計的分析では、この関係を用いて分析することもあるが、詳しくは後述する。 連続確率変数例 連続確率変数を用いた具体的な確率計算例を紹介するために、ここでは一様分布（uniform distribution）を用いる。区間 [a, b] を持つ一様分布に従う確率変数 \\(x\\) のPDFは以下のように示される。 \\[ f(x) = \\begin{cases} \\frac{1}{b-a} &amp; a\\leq x \\leq b\\\\ 0 &amp; otherwise \\end{cases} \\] 具体的な計算を実行するために、ここで区間 [-1, 3] を持つ一様分布を考える。この一様分布のPDFは、\\(\\small f(x)=\\frac{1}{4}~~-1\\leq x\\leq 3\\)（そのたの区間の確率は0）となる。このとき、\\(x\\) が区間 [0, 2] を取る確率は、以下のように求められる。 \\[ P(0\\leq x \\leq 2) = \\int^2_0 \\frac{1}{4} dx=\\left[\\frac{x}{4}\\right]^2_0=\\frac{1}{2}-0=\\frac{1}{2} \\] 7.1.2 期待値と分散 期待値とは確率の考え方を含む理論的な平均値（\\(\\mu\\)）といえる。確率分布 \\(P(x)\\) を持つ離散確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\sum_x x \\cdot P(x)=\\mu \\] 一方、PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\int_{-\\infty}^\\infty x \\cdot f(x) dx=\\mu \\] この定義に基づき、先程の区間 [-1, 3] を持つ一様分布の期待値を以下のように求める。 \\[ P(-1\\leq x \\leq 3) = \\int^2_0 \\frac{x}{4} dx=\\left[\\frac{x^2}{8}\\right]_{-1}^3=\\frac{9-1}{8}=1 \\] 期待値 \\(E(x)\\) は一般的に、\\(a\\) を定数、\\(g(x)\\) と \\(h(x)\\) を \\(x\\) の関数とするとき、以下が成り立つ： \\(E(a) = a\\) \\(E[a\\cdot g(x)]=a\\cdot E[g(x)]\\) \\(E[g(x)+h(x)]=E[g(x)]+E[h(x)]\\) これらの性質により、確率変数 \\(x\\) の分散（\\(\\sigma^2\\)）は以下のように求まる。つまり、分散は２乗の期待値から期待値の２乗を引くことで計算できる。 \\[ \\sigma^2=E[{x-E(x)}^2]=E[(x-\\mu)^2]=E(x^2)-E(x)^2 \\] "],["統計的推測.html", "7.2 統計的推測", " 7.2 統計的推測 前節で述べた通り、我々研究の対象となる集団全体ではなく、その一部から情報を取得し分析を行うことを想定する。このとき、その集団全体を母集団、母集団から抽出した一部を標本と呼ぶ。統計的な分析においては、確率分布を用いた母集団のモデル化し、標本をその確率分布に従う確率変数とみなすことで母集団と標本の関係を捉える。そのため、データ分析は標本を対象とするものの、分析者の関心は、母集団の特徴である母数（parameter）についてであることが多い。母集団の平均（\\(\\mu\\)）や分散（\\(\\sigma^2\\)）は母数の代表例である。しかし、母数は通常未知であり直接知ることはできないため、標本の情報を用いて母集団の特徴について推測する。このプロセスを統計的推測と呼ぶ。統計的推測を行うためには、原則として母集団からの無作為標本抽出（random sampling）が必要になる。統計的推測では、互いに独立に同一の分布に従う（Identically Independently Distributed: IID）ような標本が好ましく、無作為標本は、IIDを満たすことが知られている。 統計的推測においては、「推定」、「統計量」、「推定量」、「推定値」などの似たような言葉が利用されるが、これらはそれぞれ異なる意味を持つ。推定とは、標本の情報に基づき母数について把握しようとする作業そのものを示す。一方で、標本として得られるデータに基づき計算できる値（計算式）を一般的に統計量というのだが、その中でも特に推定に用いる統計量を推定量という。そして推定値は、推定量についてデータから求めた実際の計算値を表す。また、推定には「点推定」と「区間推定」がある。点推定とは、未知の母数について１つの数値に基づいて推定する方法である。例えば、標本平均は母平均（\\(\\mu\\)）を点推定するための推定量である。一方で区間推定は、未知の母数を一定の確率で含む区間を推定する方法である。これはは、点推定では捉えきれない統計的誤差を考慮して区間を推定する方法であり、母平均の信頼区間の測定は区間推定の代表例である。 "],["点推定.html", "7.3 点推定", " 7.3 点推定 ###推定量の性質 点推定は特定の推定量によって母数を捉えようとするわけだが、どのような推定量を用いるべきなのだろうか。本節では、不偏性（unbiasedness）、一致性（Consistency）、効率性（efficiency）という統計的に重要な推定量の性質について説明する。なお、以下の説明では、未知パラメータ \\(\\small \\theta\\)（シータ）に対する推定量 \\(\\small \\hat{\\theta}\\)（シータハット）を考える。不偏性とは、推定量の「期待値」が未知パラメータの真の値に等しいという性質であり、以下のように示すことができる。 \\[ E(\\hat{\\theta})=\\theta \\] つまり、実際の推定量の実現値がどうかは置いておいて、期待値の元では推定量が未知パラメータを示していることを表すものであり、サンプルサイズ（標本数）に関係のない推定量の性質である。そして、不偏性を満たす推定量のことを不偏推定量（unbiased estimator）という。なお、上記の定義から、統計的なバイアス（B）は、以下のように定義できる。 \\[ B=E(\\hat{\\theta})-\\theta \\] 第二に一致性とは、サンプルサイズが十分に大きいとき、推定量が未知パラメータの真の値と等しくなる確率が1に近づくという性質である。この性質について詳しく論じるには、漸近理論を学ぶ必要があるため、本資料では詳細を省略するが、サンプルサイズを大きくすると未知パラメータの真の値に近づくような推定量を示した性質だと解釈できる。なお、任意の\\(\\small \\epsilon &gt;0\\)（\\(\\small \\epsilon\\): イプシロン）に対して以下のような性質を持つ推定量を一致推定量という。 \\[ \\lim_{n\\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|\\leq \\epsilon \\right)=1 \\] 第三に効率性は、推定量の分散が小ささを示している。分散の小さい推定量の方が、期待値から離れた値を取りにくく、好ましい推定量と考えられる。複数の不偏推定量や一致推定量がある場合、効率性を元に好ましい推定量を考える。 7.3.1 代表定期推定量としての標本平均 なお、代表的な推定量である標本平均は母集団期待値の推定量として好ましい性質（不偏性と一致性）も持っている。以下では、期待値（\\(\\mu\\)）、分散 \\(\\sigma^2\\) の確率分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\)を考える（つまり、\\(\\small E(X)=\\mu\\), \\(\\small Var(X)=\\sigma^2\\)）。このとき、標本平均（\\(\\small \\bar{X}\\)）の普遍性は以下のように示すことができる。 \\[ E(\\bar{X})= \\left[\\frac{1}{n}(X_1+X_2+...+X_n)\\right] = \\frac{1}{n}~\\left[E(X_1)+E(X_2)+...+E(X_n)\\right] = \\frac{1}{n}\\cdot n\\mu=\\mu. \\] また標本平均の分散については、以下となることが知られている（計算は省略）。 \\[ Var(\\bar{X})=\\frac{\\sigma^2}{n} \\] 上記と同様の無作為標本による標本平均の一致性については、任意の \\(\\small \\epsilon&gt;0\\) に対していかが成り立つことが知られている。 \\[ \\lim_{n\\rightarrow \\infty}P(|\\bar{X}-\\mu|\\leq \\epsilon)=1 \\] 言い換えると、サンプルサイズが増えることで標本平均 \\(\\small\\bar{X}\\) は母集団の真の平均 \\(\\small \\mu\\) と等しくなる確率が1に近づく。なお、標本平均がもつこの特性は「大数の法則（Law of Large Number）」として知られている。 また、標本平均はその分布の収束に関しても重要な特性を持っている。ここで、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) を持つ確率分布に従う母集団からのn個の無作為標本 \\(\\small X_1,.., X_n\\) を考える。サンプルサイズが十分に大きい場合、 \\(\\small \\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)（\\(\\small \\bar{X}\\) が平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布に従う）となることが知られている。この性質を「中心極限定理（Central Limit Theorem）」という（詳細な証明や定義は省略）。また、この定理を以下のような\\(\\small \\bar{X}\\) を標準化した確率変数に応用することも可能である。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}} \\] 中心極限定理より、サンプルサイズが十分に大きい場合、Z の分布関数は標準正規分布（N(0,1)）の分布関数に収束する。詳細については割愛するが、サンプルサイズが十分に大きい場合、「標本平均」や「標本平均を標準化した確率変数」の確率分布が正規分布や標準正規分布に近似できるという定理は、統計的な推定や検定において重要なものである。 また、母集団分散の推定量としては、不偏標本分散が使われる事が多い。上記と同じ無作為標本に対し、標本分散 \\(S^2\\) と、不偏標本分散 \\(s^2\\) は以下のように定義される。 \\[ S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X}) \\] \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}) \\] そして、それぞれの推定量の期待値は以下のようになることが知られている（計算省略）。そのため、母集団分散の推定量として、不偏標本分散（\\(s^2\\)）が用いられる。 \\[ E(S^2)=\\frac{n-1}{n}\\sigma^2 \\] \\[ E(s^2)=\\sigma^2 \\] "],["推定量もまた確率変数.html", "7.4 推定量もまた確率変数", " 7.4 推定量もまた確率変数 次に、推定値と母数との関係を標本平均（\\(\\small \\bar{X}\\)）を使って考える（標本平均の定義は、「記述統計」の節を参照）。ある母集団からランダムサンプルを収集し、標本平均を計算することを考える。ここで計算された数値は真の母平均を捉えた唯一の値なのだろうか？結論としては、点推定の推定値は必ずしも母数そのものではなく、ひとつのある実現値でしかないことに注意が必要である。この理由は、「確率変数から計算される推定量もまた確率変数である」という事実から理解することができる。 例えば、我々が一橋大学商学部生の一ヶ月あたりの平均収入（仕送りは除く）に関心があるとする。（実現可能性は置いておいて）商学部全体を母集団とする無作為標本を100件収集し、標本平均を計算した結果 \\(\\small \\bar{X}=\\) 0 だったとする。もしこのような極端な結果を得た場合、多くの人が「標本平均の実現値は必ずしも真の母平均そのものではない」という説明に納得がいくだろう。同様の調査（100件のランダムサンプリング）をもう一度行い平均収入を計算し直すと、おそらく0とは異なる推定値を得る可能性が高い。仮に、\\(\\small\\bar{X}=\\) 50,000 だった場合、その結果をどのように解釈するだろうか？仕送りを除く大学生の月当たり収入の平均が５万円だという結果はなんとも尤もらしい。しかしながら、たとえ尤もらしい結果を得たとしても、それはひとつの分析結果であり、真の母平均を示す唯一の値ではない。 確率変数から計算される推定量もまた確率変数であるという点を直感的に経験するために、細工のない６面サイコロを（バーチャルに）振ってもらう。出力結果を少し見やすくするために、knitr というパッケージを利用しているため、関心のある学生は以下のようにインストールしてほしい。 install.packages(&quot;knitr&quot;) 標本平均についての議論を行う前に、理論的な期待値を求める。６面サイコロの出目の期待値 \\(\\mu\\) は以下の通りである。 \\[ \\mu = 1\\cdot \\frac{1}{6}+2\\cdot \\frac{1}{6}+...+6\\cdot \\frac{1}{6}=3.5 \\] ここで、以下のようなコマンドを用いてR内でサイコロを振ってみてほしい（実際にサイコロを振ってもらっても構わない）。 set.seed(442)# 乱数の再現性確保のための指示。関数内の数字に特に意味はないため各自別の値を使っても良い。 die &lt;- 1:6 d &lt;- sample(die,size=1,replace = TRUE) d ## [1] 6 この講義ノート内では以上で示されている通り、６という出目を得た。上記のコマンドを実施した各自がそれぞれ異なる値を得ているだろう。ここで得た６という数字は、サイコロの出目という確率変数の実現値（\\(n=1\\)）である。そのため、本データの標本平均も６であり、真の期待値とは異なる。ただし、読者によっては１件の標本による標本平均という表現を直感的に理解しにくいかもしれない。そのため、以下のように サイコロを１０回振る試行を３回実施し、各サンプリング結果に基づき標本平均を以下のように計算する。 set.seed(352) d1 &lt;- sample(die,size=10,replace = TRUE) d2 &lt;- sample(die,size=10,replace = TRUE) d3 &lt;- sample(die,size=10,replace = TRUE) d_mean &lt;- matrix(c(mean(d1),mean(d2),mean(d3)),nrow = 1) colnames(d_mean) &lt;- c(&quot;d1の平均&quot;, &quot;d2の平均&quot;, &quot;d3の平均&quot;) knitr::kable(d_mean, caption = &quot;サイコロの標本平均比較&quot;, align = &quot;ccc&quot;) Table 7.2: サイコロの標本平均比較 d1の平均 d2の平均 d3の平均 3.2 3 2.4 上記の通り、d1, d2, d3 いずれの標本平均も互いに異なるものであり、また3.5とも異なる。このことからも、確率変数（サイコロの出目）を用いて計算された推定値（標本平均）もまた確率変数であり、推定値と未知パラメータとの間にはズレ（誤差）が生じうることがわかる。なお、中には3.5と等しい標本平均を偶然得た読者もいると考えられるが、それもあくまで一つの実現値である。 では、標本平均の推定値がサンプルサイズによってどれだけ真の期待値に近づくのかについて、サイコロの試行回数を10回、100回、1,000回と増やして確認する。以下の結果を見ると、サンプルサイズ（試行回数）が増えるごとに真の期待値に近づいていることが伺える。ただし、これらの結果もあくまで確率的な試行結果の実現値である。そのため、読者によっては異なる傾向を示すような結果を得る可能性があることに注意が必要である。 set.seed(541) d10 &lt;- sample(die,size=10,replace = TRUE) d100 &lt;- sample(die,size=100,replace = TRUE) d1000 &lt;- sample(die,size=1000,replace = TRUE) d_lln &lt;- matrix(c(mean(d10),mean(d100),mean(d1000)),nrow = 1) colnames(d_lln) &lt;- c(&quot;10回試行の平均&quot;, &quot;100回試行の平均&quot;, &quot;1,000回試行の平均&quot;) knitr::kable(d_lln, caption = &quot;サイコロの標本平均比較２&quot;, align = &quot;ccc&quot;) Table 7.3: サイコロの標本平均比較２ 10回試行の平均 100回試行の平均 1,000回試行の平均 3 3.36 3.516 "],["補足いくつかの確率分布の関係性.html", "7.5 補足（いくつかの確率分布の関係性）", " 7.5 補足（いくつかの確率分布の関係性） 統計的な分析の際によく用いられる確率分布として、正規分布、カイ二乗分布、t分布、F分布間の関係性について簡単に紹介する。なお、各分布の確率密度関数などは記載しないため、関心のある読者は参考文献を参照してほしい。 7.5.1 正規分布 正規分布は、様々な分布の基準として用いられる重要な分布である。期待値を中心に左右対称であり「ベルカーブ」と言われる形状を持つ。また、平均０、分散１の正規分布は特に標準正規分布と言われ、正規母集団からの無作為標本の標本平均等の分布を特定する際などに用いられる。 7.5.2 カイ二乗分布 標準正規分布からの無作為標本の二乗和はカイ二乗分布に従う。カイ二乗分布は、正規母集団からの無作為標本の不偏標本分散の分布を特定する際などに用いられる。 7.5.3 t分布 標準正規分布とカイ二乗分布の比はt分布に従う。t分布は、正規母集団からの無作為標本による標本平均と不偏標本分散の比の分布を特定する際などに用いられる。 7.5.4 F分布 カイ二乗分布の比はF分布に従う。F分布は、異なる正規母集団からの無作為標本の不偏標本分散の比の分布を特定する際などに用いられる。 "],["区間推定.html", "7.6 区間推定", " 7.6 区間推定 点推定の節で示した通り、推定値と未知パラメータの間には、ずれ（誤差）がある。標本平均の様に好ましい性質（不偏性や一致性）を持つ推定量であっても、計算の結果示された一つの推定値がどの程度信頼できるものなのかはわからない。そこで区間推定という、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を計算する方法を用いて、統計的な誤差を加味した母数への検討を試みる。区間推定においては、「信頼水準zz%で、xx以上、yy以下という区間は真の母数を含む」という区間[xx, yy]を調べる。このような区間は信頼区間（confidence interval）と呼ばれ、多くの統計分析において用いられている。 7.6.1 信頼区間概要 Rで信頼区間を求める事自体は難しくない。t.test()（詳細は後述）の分析結果を用いて、conf.int()によって信頼区間が計算できる。信頼区間の計算を実行するために、倉田・星野（2011, p.248）で提示されている以下の電球の製品寿命に関する例を考える。ある製品（電球）の寿命は平均1700（時間）である。企業は性能を改良するために新型の電球が開発したが、新型化に伴い製品寿命も変化したのかについては不明である。ただし、この製品の寿命は新型も旧型のものも正規分布に従い、その標準偏差は \\(\\sigma=\\) 180（時間）であると仮定する。 工場で生産された新型製品を16個無作為に選びその寿命を計測した所、以下の結果を得た。 1873 1685 2275 1760 1769 2176 1748 1760 1994 1473 1715 1771 1784 1684 2038 1850 このデータは、平均が \\(\\small \\mu\\)、分散が \\(\\small 180^2\\) である正規分布（\\(\\small N(\\mu, \\sigma^2=180^2)\\) と表記する）からの無作為標本 \\(X_1,..., X_{16}\\)の実現値とみなすことができる。なお、このデータの標本平均は1,835（時間）、不偏標本標準偏差は 200である。このデータに基づく、新型電球寿命の期待値に関する95% 信頼区間（95%の確率で真の母数を含む区間）はt.tes() 関数を用いると以下の様に求まる（ただし後述するが、この方法はこの例に対しては適切ではない）。 bulb &lt;- c(1873, 1685, 2275, 1760, 1769, 2176, 1748, 1760, 1994, 1473, 1715, 1771, 1784, 1684, 2038, 1850) bulb_ci &lt;- t.test(bulb) #t検定の実施と格納 bulb_ci$conf.int #信頼区間の出力（デフォルトで95%信頼水準） ## [1] 1728.235 1941.140 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 出力されている [1] 1728.235 1941.140 が信頼区間、## attr(,\"conf.level\") ## [1] 0.95 が今回計算に用いられた信頼水準（confidence level）（もしくは信頼係数（confidence coefficient）ともいう）である。 分析の結果、95%の確率で真の新製品寿命期待値が 1728.2 から 1941.1 の間に含まれることがわかった。したがって、どうやら新製品寿命は平均的に旧型製品（1,700）よりも長そうである。では、ここで示された区間がどのように計算され、どのようなことを意味するのだろうか。以下では、一度電球の例から離れ、もう少し一般的な形で信頼区間の導出や解釈を説明する。 7.6.2 確率と区間の計算 はじめに、標準正規分布に基づくある区間の確率の求め方を説明する。 \\(Z_1,Z_2,...,Z_n\\) は、N(0, 1) （標準正規分布）に従う母集団からの無作為標本とする。このとき、標準正規分布がある区間 [\\(-\\infty,~z_\\alpha\\)] をとる確率は、以下の式および図のように示すことができる7。なお、\\(\\small z_\\alpha\\) は、確率\\(\\small \\alpha\\) に対応する標準正規分布上の上側確率 \\(\\small \\alpha\\) 点とする。このとき、この分布における \\(\\small z_\\alpha\\) 以下（以上）の範囲を取る確率は 1 \\(\\small -\\alpha\\)（\\(\\small \\alpha\\)） である。 \\[ P(Z\\leq z_\\alpha) = \\int^{z_\\alpha}_{-\\infty}~f(z)~dz =\\int^{z_\\alpha}_{-\\infty}~\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)~ dz = 1-\\alpha \\] 標準正規分布と確率計算 同様に、以下のような関係も捉えることができる。この場合、斜線部で示されている範囲の確率は両側合わせて \\(\\small \\alpha\\) であり、その内側の確率は 1 \\(\\small - \\alpha\\) である。 \\[ P(-z_{\\alpha/2} \\leq Z\\leq z_{\\alpha/2}) = \\int^{z_{\\alpha/2}}_{-z_{\\alpha/2}}~f(z)~dz = 1-\\alpha \\] 標準正規分布と両側確率 7.6.3 信頼区間の導出と解釈 上述の関係を、区間推定に応用するために、あるデータの標本平均に関する議論を捉える。標準正規分布に従う確率変数は、正規分布に従う確率変数を標準化することで得ることができる。ここで、\\(\\small X_1,...,X_n\\) を、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) の正規分布に従う母集団からの無作為標本とする。これまで学んだ標準化および標本平均の特性から、以下の通り、標本平均を標準化したものは標準正規分布に従うことがわかる。 \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] このことから、先述の標準正規分布における確率計算の関係を応用し、以下を得る。 \\[ P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha \\] したがって、区間 [\\(\\small \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\)] は、確率 \\(\\small 1-\\alpha\\) で未知の母平均 \\(\\small \\mu\\) を含むと解釈できる。また、上記の関係から任意の確率 \\(\\alpha\\) を指定することで、区間の上限と下限（\\(\\pm z_{\\alpha/2}\\)）の具体的な値を（統計学テキスト巻末などに記載されている）標準正規分布表などから求めることができる。 そして、このような区間を「信頼区間」といい、信頼区間の計算にて仮定された確率 \\(\\small 1-\\alpha\\) を「信頼水準」もしくは「信頼係数」という。信頼係数は、信頼区間の計算のために研究者によって事前に選択される。慣習としては、90%, 95%や99% (\\(\\small \\alpha =\\) 0.10, 0.05, 0.01)を用いる事が多い。なお、信頼係数を大きくすると、信頼区間も広くなる。 先述の区間推定は母分散 \\(\\sigma^2\\) が既知である場合に計算可能であるが、多くの場合母分散は未知である。そのような場合には、自由度 n-1 の「t分布」を用いて、両端の確率 \\(\\small \\alpha\\) 点を \\(\\small t_{\\alpha/2}(n-1)\\) とする信頼区間を求める。\\(N(\\mu,\\sigma^2)\\) に従う母集団からの無作為標本を考えるが、今回は母分散が未知である場合を仮定する。このような場合は、母分散 \\(\\small \\sigma^2\\) のかわりに母分散の不偏推定量である不偏標本分散 \\(\\small s^2\\) を用いた以下の統計量 t をもとに信頼区間を計算する。このとき、統計量 t は自由度 n-1 の t 分布に従うことが知られている（t分布に関する詳細および証明は省略）。 \\[ t=\\frac{\\bar{X}-\\mu}{\\sqrt{s^2/n}}\\sim t(n-1) \\] 7.6.4 Rを用いた信頼区間の計算（標準正規分布） ここで、先述の標準正規分布に基づくある区間の確率計算と同様の計算を、自由度 \\(\\small n-1\\) のt分布に基づき実行すると、以下のような確率と区間の関係に書き換えることができる。 \\[ P\\left(-t_{\\alpha/2}(n-1)\\leq \\frac{\\bar{X}-\\mu}{s^2/n}\\leq t_{\\alpha/2}(n-1)\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\right)=1-\\alpha \\] 信頼区間を求めるための手順は標準正規分布の場合もt分布の場合も同様だが、標準正規分布のかわりに t 分布を用いた場合、特定の確率に対応する閾値が変わることが知られている（\\(\\small z_{\\alpha/2}\\neq t_{\\alpha/2}(n-1)\\)）。 t分布は、左右対称であり標準正規分布よりもテールが厚いという特徴を持つが、自由度が大きくなると正規分布に近づくことが知られている。標準正規分布と、自由度の異なる t 分布との関係は以下のように図示化できる。自由度（df）3の t 分布よりも自由度20の t 分布のほうが標準正規分布に近い形状であることが伺える。 標準正規分布と t 分布によってある確率に対応する閾値が異なるということは、特定の信頼水準に対応する信頼区間も仮定する分布によって異なるということである。そのため、先述の新型電球の製品寿命についての信頼区間を t 分布（t.test() とconf.int()）ではなく、標準正規分布による信頼区間の計算によって求める必要がある。 先述の通り、標準正規分布に基づく信頼区間は以下のように示すことができる。 \\[ \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n} \\] このとき、仮定より \\(\\small \\bar{X}=1835\\), \\(\\sigma=180\\) であることがわかっている。また、慣習より95%信頼水準を仮定すると、確率 \\(\\small \\alpha = 0.05\\)となる。そのため、区間推定の計算で必要な要素のうち現時点で不明なのは、 \\(z_{\\alpha/2}=z_{0.025}\\)の値である。この値は、任意の確率に対応する区間の閾値を表している。今回の場合、分布が左右対称の分布であり正負どちらか一方の値さえ分かればよいため、閾値（\\(z_{0.025}\\)）以上の区間を取る確率が2.5%になるような閾値に着目する。このような閾値は、Rによって以下のように求める（なお先述の通り、統計学教科書に掲載されている分布表を使っても同様の数値を求めることができる）。 qnorm(0.025, lower.tail=FALSE) ## [1] 1.959964 これにより、計算に必要な情報が揃ったため、以下の要領で信頼区間を出力できる。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 upper &lt;- xbar+z*(sigma/sqrt(n)) lower &lt;- xbar-z*(sigma/sqrt(n)) ci.bulb &lt;- matrix(c(lower,upper),nrow=1) colnames(ci.bulb) &lt;- c(&quot;ci.lower&quot;, &quot;ci.upper&quot;) knitr::kable(ci.bulb, caption = &quot;Bulb data CI（95%）&quot;, align = &quot;cc&quot;) (#tab:ci_bulb)Bulb data CI（95%） ci.lower ci.upper 1746.489 1922.886 分析の結果、新型製品の平均寿命に関する95%信頼区間は、標準正規分布と t 分布どちらの分布を仮定しても旧型の平均1700（時間）を含まず、それよりも大きい値を取るものであった。したがって、新型製品は製品寿命の面においても95%の確率で旧型製品よりも優れていると考えられる。 ただし、標準正規分布の確率密度関数 \\(\\small f(z)\\) は、\\(\\small 1/\\sqrt{2\\pi}\\exp\\left(-z^2/2\\right)\\) だと知られている。↩︎ "],["信頼区間の解釈.html", "7.7 信頼区間の解釈", " 7.7 信頼区間の解釈 ここまでは主に信頼区間の導出およびRでの分析実行法について学んだ。それでは、計算された95%信頼区間は、そもそもどのように解釈すべきだろうか。結論から述べると、95%信頼区間の直感的解釈については以下のように説明できる： “「母集団からサンプルを取り平均値の95%信頼区間を構築する」という手順を100回繰り返すと考える。95%という信頼水準（確率）が示していることは、計算された区間が100回に95回は母集団の平均値を含むということである。言い換えると、今回得た標本平均に基づき計算された信頼区間がはずれ（真の母数を含まない区間）である可能性が5%存在するということである。” 95%信頼区間の解釈として、「分析対象としている母数の値がこの区間の値をとる確率が95%である」という旨の説明を行う人がいるが、これは\\(\\color{red}{\\text{誤り}}\\)である。確率的な議論を母数に適応するのは適切ではない。確率的二変動するのはあくまで区間の両端である点を理解しなければならない。なぜならば、\\(\\small \\bar{X}\\) が確率変数であるため、そこから計算される区間の両端もまた確率変数となるためである8。ここで示されている信頼水準は、計算された区間が真の母数を含んでいる確率である。つまり、信頼水準は、サンプルを収集し、信頼区間を求めるという「手順そのものに対する信頼度」を表す指標であると解釈できる。 また、分析の結果、t.test によって出力された信頼区間よりも短い区間を得た。これは、t 分布のほうがテールが厚く、中心より遠い値を取る確率密度が高いことから、95%水準における臨界値が標準正規分布に比べて大きくなるためである。また、t 分布の自由度が大きくなるにつれ、正規分布との差が小さくなる。 詳しくは、岩田暁一「経済分析のための統計的方法」を参照。↩︎ "],["統計的仮説検定.html", "7.8 統計的仮説検定", " 7.8 統計的仮説検定 データ分析においては提示した仮説が支持できるか否かを判断する目的で、統計的仮説検定を利用する。本節ではまず検定の手順について説明したあと、分析結果の意味や解釈について説明する。統計的仮説検定は、基本的に以下の手順によって実施される。 仮説（帰無仮説・対立仮説）を設ける。 仮説を検定するための統計量を選ぶ。 統計量の値について、有意確率に基づく臨界値を設定する。 帰無仮説が正しいと仮定した上で統計量を計算し、その値が棄却域と採択域のどちらの領域に入るかを分析する。 7.8.1 検定における仮説 統計的仮説検定で重要になる仮説は、帰無仮説と対立仮説である。理解を容易にするために、「問題と分析をつなげる仮説の提示」という節で議論した仮説を「作業仮説」と呼ぶ。作業仮説は、リサーチクエスチョンに答えるための論理的予測である。例えば、「女性に比べ男性の方が新製品購買意図が高い。」のような予測が考える。このような仮説を検証する場合、男女（グループ）間で購買意図の平均値を比較することが現実的な分析方法として考えられる。 帰無仮説と対立仮説は、統計的仮説検定の基準になる母集団の統計的特徴に関する仮説であり、検定という手続き上ではこれらの仮説に着目する。特に、帰無仮説は、統計的仮説検定の考察、分析の基準となる仮説であり、この仮説を棄却（否定）できるか否かを調べることが基本的な統計的仮説検定の枠組みだと言える。帰無仮説は棄却しうる仮説であり、\\(\\small H_0\\) という記号で表される事が多い。また、多くの場合において「差がない」、「効果がない（0である）」や、「特定の値と等しい」といった仮説が設計される。一方で対立仮説は、帰無仮説とは排反な仮説であり、帰無仮説が棄却された際に採用される推測であり、\\(\\small H_1\\)や\\(\\small H_a\\)という記号で表される。データ分析を用いた研究においては対立仮説と作業仮説は論理的に整合的ないしは等しいことが好ましい。つまり、作業仮説という研究上重要な論理的推測を検証するために、その作業仮説とは排反な帰無仮説を設計し統計的仮説検定を実施する必要がある。それによってもしその帰無仮説が棄却されたならば、対立仮説ひいては作業仮説がデータ分析によって支持されたと解釈することが可能になる。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_m\\)、女性における購買意図の期待値を \\(\\small \\mu_f\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\(H_0:~\\mu_m=\\mu_f\\) \\(H_1:~\\mu_m\\neq\\mu_f\\) \\(\\small H_0\\) は、男性における購買意図の期待値と女性における期待値が等しいというものであり、 \\(\\small H_1\\)はそれらが等しくないということを示している。そのため上記の二つの仮説は、どちらも未知パラメータについての関係を捉えており、\\(\\small H_0\\) と \\(\\small H_1\\) は互いに排反であることがわかる。その上で、もし帰無仮説が棄却され、男性の平均値のほうが女性よりも高い場合には、作業仮説が支持されたと解釈することができる。つまり統計的な検定においては、作業仮説として提示している推測を直接検証するのではなく、作業仮説と排反な帰無仮説を設計し、それが棄却されるならば暫定的に作業仮説の主張を指示しようという立場で検証を行う。なお対立仮説として\\(\\small H_1:~\\mu_m&gt;\\mu_f\\) を設定することも可能である。このような仮説に基づく検定方法は片側検定と呼ばれ、その詳細については後述する。 ここで、マーケティングリサーチにおける仮説間の関係を考える上で重要な点を、以下の3つの点から改めて整理する。 分析上の基準である帰無仮説は何かをきちんと理解し定義する。 それが棄却された際にはどのような結論（対立仮説）が採用されるのかを理解する。 そしてその結論が自身の立てた作業仮説と帰無仮説・対立仮説の関係が整合的かを考える。 言い換えると、自身の立てた作業仮説を帰無仮説・対立仮説の対比という分析手続きで証明できるような調査・分析法を採用する必要がある。ただし、レポートや論文には、帰無仮説・対立仮説を記載せず、作業仮説のみを記載することがほとんどである。 7.8.2 統計的検定の実行（t検定） 先程の新型電球の例を再度使い母平均の検定を実施する。新型電球について我々が関心を持っていたのは、新型電球の製品寿命が旧型の寿命（1700時間）より長いか否かである。そのため、新型電球の製品寿命の期待値を \\(\\small \\mu\\) とすると、帰無仮説と対立仮説は以下のように設計できる。 \\(H_0:~\\mu=1700\\) \\(H_1:~\\mu\\neq1700\\) 改めて以下の通り、新型電球に関する16個の無作為標本から得た製品寿命の平均値を計算すると、\\(\\small \\bar{X}=1835\\) であった。では、この1835は 1700 から十分に離れていると言えるのだろうか？もし、十分に離れていると判断されれば帰無仮説を棄却するが、この差が十分でなければ帰無仮説を採択する。 mean(bulb) ## [1] 1834.688 Rを用いて統計的検定を実行すること自体は難しくない。母平均の検定は t.test() で実施することが可能である。母平均が特定の値を取るか否かについての検定では、mu= という引数を使って帰無仮説に対応する値を指定する。今回の分析に関するコマンドおよびその結果は以下のとおりである。 t.test(bulb, alternative = &quot;two.sided&quot;, mu = 1700) ## ## One Sample t-test ## ## data: bulb ## t = 2.6968, df = 15, p-value = 0.01657 ## alternative hypothesis: true mean is not equal to 1700 ## 95 percent confidence interval: ## 1728.235 1941.140 ## sample estimates: ## mean of x ## 1834.688 分析結果の t= と df = はそれぞれt値（検定統計量の推定値）と自由度を表している。p-valueはp値と呼ばれるある確率を表しており、この確率が小さい場合、帰無仮説を棄却する。また、t.test() は、信頼区間や標本平均も出力してくれるため、これらの結果に基づき解釈を行うことも可能である。帰無仮説の棄却に至るp値の基準は慣習的に、0.10（10%）、0.05（5%）、0.01（1%）が用いられる。今回の結果では、p値が0.016であり、5%水準で帰無仮説を棄却することができるため、新型電球の寿命は旧型（1700時間）よりも有意に高いと結論づけることができる。では、このp値とはどのような確率を示しているのだろうか？この点を理解するために統計的仮説検定についてもう少し深掘りしていく。 "],["検定統計量と臨界値.html", "7.9 検定統計量と臨界値", " 7.9 検定統計量と臨界値 検定における統計量や有意確率について理解するために、再度新型電球の例を用いる。上述の t.test() は母分散が未知である際に用いられる検定方法である。この点は、信頼区間において説明した内容と同様である。なお実際のデータ分析作業においては多くの場合母分散は未知であるため、t.test() を用いることが多い。しかしながら、電球の例では母集団の分散は180の二乗であることを仮定した。そのため、ここからは母分散が既知（\\(\\small \\sigma^2=180^2\\)）であることを仮定した標準正規分布に基づく母平均の検定を軸に説明していく。 「区間推定」節の信頼区間の説明でも述べた通り、今回のように正規分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\) の標本平均は以下の分布に従うことがわかっている9。 \\[ \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] また、これまでの議論の通り、\\(\\small \\bar{X}\\)を標準化した統計量Zは以下の分布に従うことが知られている。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] ただし、今回の例においては、 \\(\\small \\bar{X}=1835\\)、\\(\\small= \\sigma=180\\) であることがわかっている。統計的仮説検定においては、この標準化された統計量を検定統計量（検定に用いる統計量）として用いて計算を行うのだが、我々の関心の中心でもある \\(\\small \\mu\\) は未知であり、通常この統計量を計算することはできない。すなわち、未知であるパラメーターを何かしらの値で代替しなければ、上記の検定統計量は計算できない。そこで、統計的な仮説検定では、「帰無仮説が正しいと一旦仮定」した上で統計量を計算するというプロセスを経る。言い換えると、未知のパラメーターについて帰無仮説で示されている値を代入することで、検定統計量を計算可能にする。 電球の例においては、\\(H_0:~\\mu=1700\\)と設計していたため、検定統計量 Z は以下の通りに書き換えることができる。 \\[ Z=\\frac{1835-1700}{\\sqrt{180^2/16}} \\] そして、もし「帰無仮説が正しければ」Zは標準正規分布に従うはずであり、言い換えると Z の計算結果は0に近い値を取る可能性が高いはずである。そこで、この Z を計算し、\\(\\small |Z|\\) がある閾値 c よりも大きい（十分に0から離れている）場合には帰無仮説を棄却する。なお、ここで用いる閾値 c のことを一般的に臨界値と呼ぶ。つまり、検定統計量 Z の計算結果に対して、以下の方針で仮説検定を行うといえる。 \\[ \\begin{cases} |Z|&gt;c &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq c &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 臨界値 c の求め方は区間推定と同様、分析に対応する確率分布（今回であれば標準正規分布）に基づくある区間の確率計算で求まる。研究者はまず、任意の確率 \\(\\small \\alpha\\) を決める。この確率は「有意水準（significance level）」と呼ばれ、この有意水準と標準正規分布に基づく確率計算によって臨界値（下図内では \\(\\small \\pm z_{\\alpha/2}\\)）を求める。その上で、統計量の計算結果が臨界値より外側（下図における斜線部）にある場合には帰無仮説を棄却する。そのため、斜線部のような領域を棄却域、確率 \\(\\small 1-\\alpha\\) に対応する範囲を採択域と一般に呼ぶ。 臨界値と確率計算 また、ここまでの例では対立仮説を \\(H_1:~\\mu\\neq1700\\) とし、左右対称の分布の両端に棄却域を設定した。このような検定方法を一般的に両側検定と呼ぶ。しかし、現実的ないしは理論的な根拠をもとに、ある値よりも高い（もしくは低い）ことが事前に予測できる場合がある。その場合には、例えば \\(\\small \\mu&gt;1700\\) や \\(\\small \\mu&lt;1700\\)といった対立仮説を設定することも可能である。このような対立仮説を利用した検定方法を一般的に片側検定と呼ぶ。ここでは、仮に \\(\\small \\mu&gt;1700\\)という対立仮説を立てた場合を考えるが、\\(\\small \\mu&lt;1700\\)のような対立仮説を設計しても正負を入れ替えることで同様の議論ができる。なお、片側検定において帰無仮説が棄却された場合、直ちに帰無仮説の値よりも大きい値を取ると判断する。しかしながら、たとえ異なる対立仮説を提示しても、採用する検定統計量や帰無仮説に基づく分布の仮定などは同じである。 片側検定を利用した場合の特殊性はその棄却域に現れる。片側検定の場合の棄却域は以下の図のように片側のみとなる。なお、その場合分布の両端に棄却域を設ける必要がないため、正の方向に \\(\\small \\alpha\\) 分の棄却域を設定する。 片側検定（正の場合） 7.9.1 有意水準と検定における誤り ここで再び話を両側検定に戻し、統計的検定において用いられる有意水準について説明する。ここまでの内容をまとめると、帰無仮説を仮定して検定統計量を計算する場合、帰無仮説が正しければ、棄却域内の値を取る確率は \\(\\small 100\\times \\alpha\\)%であると言える。そして、検定統計量の計算結果が棄却域に含まれる場合、帰無仮説を棄却するという判断を下す。そのため、統計的に帰無仮説を棄却したからと言って、その結果が必ず正しいとは言い切れない。統計的検定には、根本的に第一種の誤り（Type 1 error）と第二種の誤り（Type 2 error）という二種類の誤りの可能性が内包されている。 第一種の誤りとは、帰無仮説が真であるにも関わらず、帰無仮説を棄却してしまう誤りである。例えば、本当は効き目のない薬を効くと判断してしまう誤りである。一方で第二種の誤りは、帰無仮説が真ではないのにも関わらず、帰無仮説を採択してしまう誤りである。例えば、本当は効き目のある薬を効かないと判断してしまう誤りである。 Table 7.4: 検定の誤り H0が真 H0が偽 H0を棄却 Type 1 error ✓ H0を採択 ✓ Type 2 error どちらの誤りも見過ごすことのできないものではあるが、第一種の誤りによる損失と、第二種の誤りによる損失を比較し、一般的な統計的検定においては、第一種の誤りの確率を下げることに注視する。なお、研究によっては下記にある検定力という指標に着目し、第二種の誤りに対応した議論を提示することもあるが、本書では割愛する。特に、第一種の誤りの確率を有意水準 \\(\\small \\alpha\\) と設定し、仮説検定を行う。有意水準は、先述の通り棄却域の特定に用いられる。つまり、統計的仮説検定とは、帰無仮説が正しいと仮定した上で有意水準 \\(\\small \\alpha\\) の分だけ第一種の誤りの確率を許容したうえで仮説が正しいか否かを確認する作業である。 上記の統計的検定に関わる誤りは、\\(\\small T_0\\) は統計量 T の観測値、Rは\\(\\small H_0\\) の棄却域、Aは\\(\\small H_0\\)の採択域とし、以下のように示される。 有意水準: \\(\\alpha\\) \\[ P(T_0\\in R|H_0~\\text{is True})=\\alpha \\] 第二種の誤りの確率: \\(\\beta\\) \\[ P(T_0\\in A|H_0~\\text{is False})=\\beta \\] 検定力: \\(1 - \\beta\\) \\[ P(T_0\\in R|H_0~\\text{is False})=1-\\beta \\] 7.9.2 Rを用いた統計的検定の実施（標準正規分布） ここでは、新型電球の例に対応した標準正規分布を用いた仮説検定の方法を紹介する。先程の新型電球の例に対し、仮説検定に関わる有意水準を \\(\\small \\alpha=0.05\\) と設定する。帰無仮説が正しいという条件のもとで、帰無仮説を棄却する確率であるため、有意水準は以下のように示すことができる。 \\[ \\alpha=P(|Z|&gt;c|\\mu=1700) =0.05 \\] しかしながら、このままだと確率計算が複雑になるため、上式を以下のように書き換える。 $$ 1-=P(|Z|c|) = ^c_{-c}f(Z)dZ~_{|}=0.95 $$ 再掲になるが、上式の関係を表した図が、以下のものになる。 このとき、検定統計量 Z は帰無仮説が正しければ標準正規分布に従うはずである。したがって、臨界値 \\(\\pm c\\) は、\\(\\pm z_{0.025}\\) として分布表などより導出が可能である。再掲になるが、Rにおいては以下のように計算できる。すなわち、検定統計量の計算結果が 1.96（-1.96）を上回る（下回る）場合には、帰無仮説を棄却するが、そこには第一種の誤りを犯す確率が5%残されていると解釈できる。 qnorm(0.025, lower.tail=FALSE) ## [1] 1.959964 ここまでの議論を踏まえ、新型電球に関する統計的検定を標準正規分布に基づき以下のように実施する。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 mu &lt;- 1700 #Test statistic Z &lt;- (xbar - mu)/(sigma/sqrt(n)) Z ## [1] 2.993056 分析の結果、検定統計量 Z の実現値が5%有意水準に基づく臨界値（1.96）よりも大きいことが示されたため、5%有意水準で帰無仮説が棄却された。つまり、5%の第一種の誤りの確率を残した上ではあるが、新型電球の製品寿命は旧型製品の寿命よりも長いと言える。このような結果は一般的に、「統計的に有意な結果」と表現される。 ここまでは、有意水準の意味を踏まえ、検定の手順及び結果の解釈について説明した。上述の例では、ある有意水準のもと帰無仮説を棄却できる「統計的に有意な」結果を得た。しかしながら、統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。具体的には、統計的に有意でないからと言って、帰無仮説が正しい（つまり \\(\\small \\mu =1700\\) である）と結論づけることはできない。ここまでの説明の通り、有意水準とは第一種の誤りを犯す確率であり、有意水準に基づく統計的検定では主にこの確率に対応した分析を行っている。そのため、第二種の誤りを犯す確率については何も対応していないことになり、本当は \\(\\small \\mu \\neq 1700\\) であるにも関わらず、\\(\\small \\mu =1700\\) と判断している可能性がある。これらの点から「有意でない」ということを理由に、帰無仮説が正しいと結論づけることは適切ではない。そのため、もし今回の仮説検定で帰無仮説を棄却できていなかったとしたら、その結論は「新型電球寿命の平均は1700時間ではないとは言えない」となる。なんとも歯切れの悪い結論だということは理解できるが、統計的検定の特性上、このような解釈を提示しないといけない。 なお、この電球の例のように母集団の分散が既知の場合、検定統計量は標準正規分布に従うと仮定できる。 しかし母集団の分散が未知の場合は、信頼区間での議論と同様、標準偏差の不偏推定量を用いて、自由度 n-1 の t 分布を仮定した分析を行う。そして、t分布に基づく母平均に関する検定を一般的に「t検定（t-test）」と呼ぶ。 母平均の検定に関する検定統計量について、日本語を用いて直感的に書くと以下のように示すことができる（ただし、帰無値は、帰無仮説が正しいときの母数の値を示す勝手な略称なので注意）。この構造を直感的にも理解しておくと、自身が実行したい分析に応じて適した統計量を想定することができる。 \\[ \\text{統計量}＝\\frac{推定量-帰無値}{標準偏差（or 誤差）} \\] 7.9.3 p値について t.test() を用いた分析例でも紹介したが、R (他のソフトウェアでも)で統計的検定を実行すると “p-value”（p値）という値を得る。p値については、もう少し詳しい説明が必要であり、解釈にも注意が必要である。 p値は、計算された検定統計量の実現値を臨界値とし、有意水準を計算していると解釈できるが、棄却域と有意水準の関係に基づと以下のように説明することができる（cf. 西山など, 2019）。有意水準を小さく取ると、棄却域は狭くなる。例えば、ある仮説検定において、5%有意水準では帰無仮説を棄却できるが、1%ではできない場合がある。計算された検定統計量の実現値に基づき、有意水準を変えながら検定を行っていくと、これ以上有意水準を小さくすると帰無仮説が棄却されなくなるという有意水準の限界を見つけることができる。この限界をp値と呼ぶ。そのため、p値によって示されている確率は有意水準と同様のものを捉えているのだが、その計算過程が異なるという点において注意が必要である。 7.9.4 母平均の検定小括 ここで改めて、標準正規分布を用いた母平均の検定に着目し、統計的仮説検定についてのより一般的な説明を提示する。\\(\\small X_1,..,X_n\\) を正規母集団 \\(N(\\mu, \\sigma^2)\\) からのサンプルサイズ n の無作為標本とする。ことき、帰無仮説の下でのパラメータの値を\\(\\mu_0\\)として、以下の帰無仮説と対立仮説を設計する。 \\[ H_0:~\\mu=\\mu_0,~~~H_1:~\\mu\\neq\\mu_0 \\] このとき、検定統計量 Z を以下のように定義する。 \\[ Z=\\frac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n}} \\] そのうえで、有意水準 \\(\\small \\alpha\\) に基づく両側臨界値 \\(\\pm z_{\\alpha/2}\\) を設定し、以下の方式で検定する。 \\[ \\begin{cases} |Z|&gt;z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布↩︎ "],["平均値に関するその他の検定.html", "7.10 平均値に関するその他の検定", " 7.10 平均値に関するその他の検定 これまでは、母平均が特定の値を取るか否かに着目し、統計的仮説検定の基礎について説明した。しかしながら本章の冒頭でも例に挙げた通り、平均値をあるグループ間で比較したいと考えることも多い。本節では、期待値の比較に着目し、平均の差の検定と、分散分析について説明する。これらの検定では、用いる検定統計量は先述のものと異なるが、統計的仮説検定そのものの手順や、肝となる考え方は共通である。 7.10.1 平均の差の検定 前節で考えた、「女性に比べ男性の方が新製品購買意図が高い。」という作業仮説を再度考える。このとき、我々が観察可能なのは男性グループの標本平均（\\(\\small \\bar{X}\\)）と女性グループの標本平均（\\(\\small \\bar{Y}\\)）であるが、検定においてはそれぞれの期待値（\\(\\small \\mu_x\\) と \\(\\small \\mu_y\\)）に着目し、帰無仮説を作成する。なお、\\(\\small X_1,...,X_n\\) は\\(\\small N(\\mu_x,\\sigma^2_x)\\)に従う母集団からの無作為標本であり、\\(\\small Y_1,...,Y_n\\) は\\(\\small N(\\mu_y,\\sigma^2_y)\\)に従う母集団からの無作為標本であるとする。また、\\(\\small X_1,...,X_n\\) と \\(\\small Y_1,...,Y_n\\) は互いに独立であり、母分散は未知であるとする。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_x\\)、女性における購買意図の期待値を \\(\\small \\mu_y\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\[H_0:~\\mu_x=\\mu_y,~~H_1:~\\mu_x\\neq\\mu_y\\] 統計的検定の手順と直感的な検定統計量の作り方は前節の内容と同じである。そのため、本検定における推定量と帰無仮説条件下での未知パラメータの値を特定したい。この検定ではグループ間の平均の差を捉えているため、その関係を \\(\\small \\bar{X}-\\bar{Y}\\) と示す。したがって、帰無仮説と対立仮説は以下のように書き直すことができる。 \\[H_0:~\\mu_x-\\mu_y=0,~~H_1:~\\mu_x-\\mu_y\\neq0\\] また、母分散が未知である場合にはt検定を行うということも先程と同様である。このことから、以下の検定統計量を用いる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})-(\\mu_x-\\mu_y)}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] ただし、\\(s^2\\) は標準誤差と呼ばれる母集団の標準偏差の推定量である。なお、\\(s^2\\)は母分散を捉えた推定量であるが、母分散が両群で等しい（等分散: \\(\\small \\sigma^2_x=\\sigma^2_y=\\sigma^2\\)）である場合には上記の検定統計量を自由度（\\(\\small m+n-1\\)）のt分布として分析可能である。一方で等分散出ない場合には、Welchのt検定と呼ばれる、自由度の計算を修正した分析方法を用いる。なお、Welchのt検定で用いられる自由度の式は複雑なのでここでは省略する。 このとき、帰無仮説が正しいという仮定のもとでは、\\(\\small \\mu_x-\\mu_y=0\\)である。そのため、上記の検定統計量は以下のように観察可能な情報のみで構成される形で書き換えることができる。また、帰無仮説が正しければ、この検定統計量は自由度（\\(\\small m+n-1\\)）のt分布に従うと考えられる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] そのため、データに基づき計算された検定統計量tの実現値を用いて、以下の方式で検定を行う。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(m+n-1) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(m+n-1)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] Rにおいて平均の差の検定を行うことはさほど難しくない。先程の等分散性についても、var.equal=TRUEまたはvar.equal=FALSEという引数で設定できる。var.equal= 引数についてはTRUEが等分散性を仮定するが、デフォルトでは、FALSEとなっている。 平均の差の検定では、 t.test(outcome ~ category) のように、はじめに着目する成果変数を、その後 ~（チルダ）のあとに着目するカテゴリ変数を指示することで、どの変数（outcome）の平均の差をどのカテゴリ変数（category）で検定するのかを特定化する。ここでは、以前の章で利用したidposデータを利用して、総支出額の平均が男女間で異なるか否かを以下のように分析する。 idpos_cust &lt;- readr::read_csv(&quot;data/idpos_customer.csv&quot;) t.test(monetary ~ gender, data = idpos_cust) ## ## Welch Two Sample t-test ## ## data: monetary by gender ## t = 1.7895, df = 1334.2, p-value = 0.07377 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## -253.4545 5518.7863 ## sample estimates: ## mean in group female mean in group male ## 17511.03 14878.36 t.test(monetary ~ gender, data = idpos_cust,var.equal=T) ## ## Two Sample t-test ## ## data: monetary by gender ## t = 1.6061, df = 1485, p-value = 0.1085 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## -582.7229 5848.0547 ## sample estimates: ## mean in group female mean in group male ## 17511.03 14878.36 分析の結果、等分散性を仮定するか否かで、計算結果は微妙に異なるが、どちらの検定結果においても10%有意水準で帰無仮説を棄却できなかった。そのため、本データにおいて総支出額に関する男女差があるとは言えない。 なおRにおいては、等分散性に関する検定もvar.test(outcome ~ category)実行可能である。先程のidposデータに関する男女差について、等分散性の分析を以下のように実施する。 var.test(monetary ~ gender, data = idpos_cust, ratio = 1) ## ## F test to compare two variances ## ## data: monetary by gender ## F = 1.9913, num df = 984, denom df = 501, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.706794 2.314410 ## sample estimates: ## ratio of variances ## 1.991318 等分散性の検定では、ひとつのグループの分散ともう一方のグループの分散の比が１（等分散）であるという帰無仮説を設計する。詳細は割愛するが、帰無仮説が正しい場合には両グループの不偏標本分散の比が自由度（\\(m-1\\), \\(n-1\\)）のF分布に従う。分析の結果、帰無仮説は棄却されたため、等分散とは言えないと結論づけることができる。そのため、平均の差の検定においては、Welchのt検定を利用した分析結果を採用して議論することが好ましい。 "],["分散分析.html", "7.11 分散分析", " 7.11 分散分析 ここまでの内容では、２グループ間の平均の差に関する分析を捉えた。しかしながら、三つ以上のグループ間の平均の差に関心があることもある。例えば、異なる地域における売上高の差を比較したい場合が挙げられる。このような目的を持つ場合によく用いられるのが分散分析（Analysis of Variance; ANOVA）である。本節では、ANOVAの実行方法を中心に説明を行う。なお、ANOVAに関するより詳細な説明は別添の補足資料を参照してほしい。 ANOVAの構造については、要因と水準という二つの要素に基づく説明が行われる。要因とは、観測値に影響を与えていると考えうるカテゴリ変数のことを指し、水準とは、要因を構成するいくつかの条件やグループを指す。例えばある小売企業における各店舗の一定期間内の売上高が出店エリア特性によって差があるのかという問いに関心があるとする。その際各店舗を、都市エリア、郊外エリア、農村エリアという三つのグループに分類し、それぞれのグループにおける標本平均を求めれば、エリアごとの差を分析できるだろう。この場合、「地域」が売上高に影響を与えうる要因であり、「都市、郊外、農村」という三つのグループが水準だと言える。 分析において取り上げる要因が一つである分散分析を一元配置分散分析と呼ぶ。二元配置分散分析については本資料では扱わないため、補足資料を参照してほしい。なお、分散分析をRで実行することは難しくない。ここでは、reshape2というパッケージに内包されてる tipping データを用いてANOVAを実行するため、以下のようにreshape2パッケージをインストールして欲しい。 install.packages(reshape2) パッケージをインストールしたら、以下のようにreshape2を起動し、今回使用する tips データを確認する。本データに含まれている変数は以下の通りである。 total_bill: 支払料金（ドル） tip: チップ額（ドル） sex: 支払い者の性別 smoker: グループ内に喫煙者がいるか day: 曜日 time: 時間帯 size: 人数 library(reshape2) str(tips) ## &#39;data.frame&#39;: 244 obs. of 7 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels &quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ time : Factor w/ 2 levels &quot;Dinner&quot;,&quot;Lunch&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... ここでは、以下の通りチップ額が曜日によって異なるか否かを分析する。ANOVAの実行においてはaov()関数によって分析するモデルとデータを指定し、anova() によって分析結果を出力する（summary() を用いることも可能である）。なお、上記のデータサマリーより、day という要因には四水準 (levels) 含まれていることがうかがえる。 s &lt;- aov(tip ~ day, data = tips) anova(s) ## Analysis of Variance Table ## ## Response: tip ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## day 3 9.53 3.1753 1.6724 0.1736 ## Residuals 240 455.69 1.8987 分析結果における Sum Sq は、水準間変動和（i.e.,平方和）と呼ばれ、特定要因の水準間によって説明される観測値の変動を表している。 Mean Sq は平均平方と呼ばれ、Sum SqをDF(自由度)で割ったものである。F value は F値という検定統計量の実現値であり、Pr(&gt;F) は本検定の p値を表している。また、Residuals の行で示されているのは、残差平方和と呼ばれ、郡内変動、つまり同グループ（水準）内での値のばらつきの程度を表している。 なお分析の結果、チップ額について曜日による統計的に有意な差は確認されなかった。それでは、ANOVAでは具体的にどのような帰無仮説を用いた検定を行っているのだろうか？結論を先に述べると、「すべての水準間で平均値は同じか」を検定しており、仮に帰無仮説が棄却された場合、「少なくとも一つの水準では値が異なる」と考える。そのため、ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。そのため、具体的にどの水準間に差があるのかはわからない。そこで、ANOVAを用いた研究では事後分析として、多重比較と呼ばれる分析を行うことが多い。しかし、この多重比較には統計的な問題が伴うと言われている（詳細は補足資料参照）。その問題に対応した手法として広く用いられているのが、Tukeyのpair-wise 比較である。Rではこの分析を、TukeyHSD() という関数で実行できる。具体的には、aov() でストアしたANOVAの分析結果を用いて以下のように実施する。また、TukeyHSD()を用いて、pair-wise比較の結果を95%信頼区間とともに図示化することもできる。 tukey_result &lt;- TukeyHSD(s) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tip ~ day, data = tips) ## ## $day ## diff lwr upr p adj ## Sat-Fri 0.25836661 -0.6443694 1.1611026 0.8806455 ## Sun-Fri 0.52039474 -0.3939763 1.4347658 0.4558054 ## Thur-Fri 0.03671477 -0.8980753 0.9715049 0.9996235 ## Sun-Sat 0.26202813 -0.2976929 0.8217492 0.6203822 ## Thur-Sat -0.22165184 -0.8141430 0.3708394 0.7678581 ## Thur-Sun -0.48367997 -1.0937520 0.1263921 0.1724212 plot(tukey_result) 分析結果における diff 列は平均値の差を表している。 lwer と upr は信頼区間の下限と上限を表しており、一番右の列はp値を示している。分析の結果、P-valueが10%水準よりも低い結果がないため、どのペアに関する検定でも有意な差は確認できなかった。したがって、分析結果は必ずしもチップ額が曜日によって変化するとは言えないことを示した。この結果は、アメリカにおけるチップ額が会計額に対する割合や提供されたサービス品質によって決まるという慣習から考えると妥当な結果である。しかしながら、前節で注意した通り、このような統計的に非有意な結果をもって「曜日はチップ額に影響を与えない」と結論づけるのは不適切である。 なお、今回の分析においてはANOVAもTukeyのpair-wise比較も有意な結論を得ることができなかったという点で、両者の結果に一貫性があった。しかしながら、ANOVAでは有意だが、Tukeyの分析ではどの組み合わせも有意ではないという一見整合でない結果を得ることもある。その場合には、慣習としてTukeyの多重比較結果を優先して解釈を提示することが多い。しかしながら、研究者が自身の実施した検定の「意味」を理解し、解釈や議論を提示することも重要である。例えば、ANOVAとTukeyでは用いている帰無仮説が異なるため、異なる比較対象を用いた検定を実施している。そのため、自身が実行した分析がどのような帰無仮説を採用しており、何と何の比較を行っているのかを正確に把握し、実施した検定の意味に適した解釈や議論を展開する事が重要になる。そのためにも単にソフトウェア上の分析方法を覚えるだけでなく、統計的分析に関する理論的側面についても学習しておく必要がある。 本章では、基礎的な統計学の復習として、主に区間推定と統計的仮説検定について説明した。区間推定では、主に信頼区間の計算に着目し、信頼区間の意味についてきちんと理解、解釈することの重要性を強調した。また、統計的仮説検定では、母平均の検定を起点とし統計的検定の基礎的な構造と考え方について説明した。検定においては、母集団の統計的特徴に関する予測である帰無仮説と対立仮説を設計することが重要である。また、グループ間の差異に着目した検定を行う場合には、関心のある未知パラメータについての差や比に着目し検定統計量を作成する事が多いが、基本的な統計的検定の考え方と手順は変わらないという点についても説明した。 "],["参考文献-5.html", "7.12 参考文献", " 7.12 参考文献 浅野正彦・矢内勇生（2018）「Rによる計量政治学」，オーム社. 岩田暁一（1996）「経済分析のための統計的方法 第２版」，東洋経済新報社. 倉田博史・星野崇宏（2011）「入門統計解析」，新世社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 宮川公男（2002）「基本統計学」，有斐閣. "],["回帰分析.html", "Chapter 8 回帰分析", " Chapter 8 回帰分析 鋭意制作中（講義スライドを参照してください）。ごめんなさい。 "],["回帰分析上の工夫紹介.html", "Chapter 9 回帰分析上の工夫紹介", " Chapter 9 回帰分析上の工夫紹介 鋭意制作中（講義スライドを参照してください）。ごめんなさい。 "],["セグメントとクラスター分析.html", "Chapter 10 セグメントとクラスター分析", " Chapter 10 セグメントとクラスター分析 鋭意制作中（講義スライドを参照してください）。ごめんなさい。 "],["ポジショニングと知覚マップ.html", "Chapter 11 ポジショニングと知覚マップ", " Chapter 11 ポジショニングと知覚マップ 鋭意制作中（講義スライドを参照してください）。ごめんなさい。 "],["価格感度測定.html", "Chapter 12 価格感度測定", " Chapter 12 価格感度測定 鋭意制作中（講義スライドを参照してください）。ごめんなさい。 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
